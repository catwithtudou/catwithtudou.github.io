[{"categories":["search engine"],"content":"简介 聚合是 Elasticsearch 除搜索以外，提供的针对 ES 数据进行统计分析功能。通过聚合，我们可以得到数据的概览，是分析和总结全套的数据，而不是寻找单个文档。 Elasticsearch 的主要优点就是： 实时性高，和 Hadoop 相比会更快 性能高，可直接通过提供的API就能得到分析结果 ","date":"2022-03-08","objectID":"/posts/search_engine/es/7.html/:1:0","tags":["search_engine","note","es"],"title":"ElasticSearch聚合简介","uri":"/posts/search_engine/es/7.html/"},{"categories":["search engine"],"content":"聚合的分类 集合的分类主要有以下： Bucket Aggregation：一些列满足特定条件的文档的集合 Metric Aggregation：一些数学运算，可以对文档字段进行统计分析 Pipeline Aggregation：对其他的聚合结果进行二次聚合 Matrix Aggregation：支持对多个字段的操作并提取一个结果矩阵 ","date":"2022-03-08","objectID":"/posts/search_engine/es/7.html/:2:0","tags":["search_engine","note","es"],"title":"ElasticSearch聚合简介","uri":"/posts/search_engine/es/7.html/"},{"categories":["search engine"],"content":"Bucket\u0026Metric 与SQL进行比较： SELECT COUNT(brand) =\u003e Metric：一些系列的统计方法 FROM cars GROUP by band =\u003e Bucket：一组满足条件的文档 #价格统计信息+天气信息 GET kibana_sample_data_flights/_search { \"size\": 0, \"aggs\":{ \"flight_dest\":{ \"terms\":{ \"field\":\"DestCountry\" }, \"aggs\":{ \"stats_price\":{ \"stats\":{ \"field\":\"AvgTicketPrice\" } }, \"wather\":{ \"terms\": { \"field\": \"DestWeather\", \"size\": 5 } } } } } } #返回如下： { \"took\" : 5, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 10000, \"relation\" : \"gte\" }, \"max_score\" : null, \"hits\" : [ ] }, \"aggregations\" : { \"flight_dest\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 3187, \"buckets\" : [ { \"key\" : \"IT\", \"doc_count\" : 2371, \"max_price\" : { \"value\" : 1195.3363037109375 }, \"min_price\" : { \"value\" : 100.57646942138672 }, \"avg_price\" : { \"value\" : 586.9627099618385 } }, { \"key\" : \"US\", \"doc_count\" : 1987, \"max_price\" : { \"value\" : 1199.72900390625 }, \"min_price\" : { \"value\" : 100.14596557617188 }, \"avg_price\" : { \"value\" : 595.7743908825026 } }, { \"key\" : \"CN\", \"doc_count\" : 1096, \"max_price\" : { \"value\" : 1198.4901123046875 }, \"min_price\" : { \"value\" : 102.90382385253906 }, \"avg_price\" : { \"value\" : 640.7101617033464 } }, { \"key\" : \"CA\", \"doc_count\" : 944, \"max_price\" : { \"value\" : 1198.8525390625 }, \"min_price\" : { \"value\" : 100.5572509765625 }, \"avg_price\" : { \"value\" : 648.7471090413757 } }, { \"key\" : \"JP\", \"doc_count\" : 774, \"max_price\" : { \"value\" : 1199.4913330078125 }, \"min_price\" : { \"value\" : 103.97209930419922 }, \"avg_price\" : { \"value\" : 650.9203447346847 } }, { \"key\" : \"RU\", \"doc_count\" : 739, \"max_price\" : { \"value\" : 1196.7423095703125 }, \"min_price\" : { \"value\" : 101.0040054321289 }, \"avg_price\" : { \"value\" : 662.9949632162009 } }, { \"key\" : \"CH\", \"doc_count\" : 691, \"max_price\" : { \"value\" : 1196.496826171875 }, \"min_price\" : { \"value\" : 101.3473129272461 }, \"avg_price\" : { \"value\" : 575.1067587028537 } }, { \"key\" : \"GB\", \"doc_count\" : 449, \"max_price\" : { \"value\" : 1197.78564453125 }, \"min_price\" : { \"value\" : 111.34574890136719 }, \"avg_price\" : { \"value\" : 650.5326856005696 } }, { \"key\" : \"AU\", \"doc_count\" : 416, \"max_price\" : { \"value\" : 1197.6326904296875 }, \"min_price\" : { \"value\" : 102.2943115234375 }, \"avg_price\" : { \"value\" : 669.5588319668403 } }, { \"key\" : \"PL\", \"doc_count\" : 405, \"max_price\" : { \"value\" : 1185.43701171875 }, \"min_price\" : { \"value\" : 104.28328704833984 }, \"avg_price\" : { \"value\" : 662.4497233072917 } } ] } } } Bucket Elasticsearch 提供了很多类型的 Bucket，可以多种方式划分文档: Term\u0026Range：时间/年龄区间/地理位置等 比如下面这些例子： 杭州属于浙江/一个演员属于男或女性 嵌套关系 - 杭州属于浙江属于中国属于亚洲 #按照目的地进行分桶统计 GET kibana_sample_data_flights/_search { \"size\": 0, \"aggs\":{ \"flight_dest\":{ \"terms\":{ \"field\":\"DestCountry\" } } } } #返回如下： { \"took\" : 33, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 10000, \"relation\" : \"gte\" }, \"max_score\" : null, \"hits\" : [ ] }, \"aggregations\" : { \"flight_dest\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 3187, \"buckets\" : [ { \"key\" : \"IT\", \"doc_count\" : 2371 }, { \"key\" : \"US\", \"doc_count\" : 1987 }, { \"key\" : \"CN\", \"doc_count\" : 1096 }, { \"key\" : \"CA\", \"doc_count\" : 944 }, { \"key\" : \"JP\", \"doc_count\" : 774 }, { \"key\" : \"RU\", \"doc_count\" : 739 }, { \"key\" : \"CH\", \"doc_count\" : 691 }, { \"key\" : \"GB\", \"doc_count\" : 449 }, { \"key\" : \"AU\", \"doc_count\" : 416 }, { \"key\" : \"PL\", \"doc_count\" : 405 } ] } } } Metric Metric 会基于数据集计算结果，除了支持在字段上进行计算，同样支持在脚本（painless script）产生的结果之上进行计算： 大部分 Metric 是数学计算，仅输出一个值 min/max/sum/avg/cardinality 部分 metric 支持输出多个数值 stats/percentiles/percentile_ranks #查看航班目的地的统计信息，增加平均，最高最低价格 GET kibana_sample_data_flights/_search { \"size\": 0, \"aggs\":{ \"flight_dest\":{ \"terms\":{ \"field\":\"DestCountry\" }, \"aggs\":{ \"avg_price\":{ \"avg\":{ \"field\":\"AvgTicketPrice\" } }, \"max_price\":{ \"max\":{ \"field\":\"AvgTicketPrice\" } }, \"min_price\":{ \"min\":{ \"field\":\"AvgTicketPrice\" } } } } } } #返回如下： { \"took\" : 5, \"timed_out\" : false, \"_shar","date":"2022-03-08","objectID":"/posts/search_engine/es/7.html/:2:1","tags":["search_engine","note","es"],"title":"ElasticSearch聚合简介","uri":"/posts/search_engine/es/7.html/"},{"categories":["search engine"],"content":"参考 https://time.geekbang.org/course/intro/100030501?tab=catalog ","date":"2022-03-08","objectID":"/posts/search_engine/es/7.html/:3:0","tags":["search_engine","note","es"],"title":"ElasticSearch聚合简介","uri":"/posts/search_engine/es/7.html/"},{"categories":["search engine"],"content":"Index Template Index Template 可以设定 Mappings 和 Settings，并按照一定的规则自动匹配到新创建的索引上。 这里需要注意的是： 模板仅在一个索引被新创建时才会生效 修改模板不会影响已创建的索引 可以设定多个索引模板，这些设置会被\"merge\"在一起 可指定order的值，控制\"merging\"的过程 Index Template 的工作方式通过一个新索引被创建时观察： 应用默认的 settings 和 mappings 应用 order 数值低的 Index Template 中的设定 应用 order 数值高的 Index Template 中的设定，之前的设定会被覆盖 应用创建索引时，用户所指定的 Settings 和 Mappings，并覆盖之前模板中的设定 #Create a default template PUT _template/template_default { \"index_patterns\": [\"*\"], \"order\" : 0, \"version\": 1, \"settings\": { \"number_of_shards\": 1, \"number_of_replicas\":1 } } PUT _template/template_test { \"index_patterns\" : [\"test*\"], \"order\" : 1, \"settings\" : { \"number_of_shards\": 1, \"number_of_replicas\" : 2 }, \"mappings\" : { \"date_detection\": false, \"numeric_detection\": true } } #查看template信息 GET /_template/template_default # 返回如下： { \"template_default\" : { \"order\" : 0, \"version\" : 1, \"index_patterns\" : [ \"*\" ], \"settings\" : { \"index\" : { \"number_of_shards\" : \"1\", \"number_of_replicas\" : \"1\" } }, \"mappings\" : { }, \"aliases\" : { } } } #使用通配符查看template信息 GET /_template/temp* { \"template_test\" : { \"order\" : 1, \"index_patterns\" : [ \"test*\" ], \"settings\" : { \"index\" : { \"number_of_shards\" : \"1\", \"number_of_replicas\" : \"2\" } }, \"mappings\" : { \"numeric_detection\" : true, \"date_detection\" : false }, \"aliases\" : { } }, \"template_default\" : { \"order\" : 0, \"version\" : 1, \"index_patterns\" : [ \"*\" ], \"settings\" : { \"index\" : { \"number_of_shards\" : \"1\", \"number_of_replicas\" : \"1\" } }, \"mappings\" : { }, \"aliases\" : { } } } #数字字符串被映射成text，日期字符串被映射成日期 PUT ttemplate/_doc/1 { \"someNumber\":\"1\", \"someDate\":\"2019/01/01\" } GET ttemplate/_mapping #返回如下： { \"ttemplate\" : { \"mappings\" : { \"properties\" : { \"someDate\" : { \"type\" : \"date\", \"format\" : \"yyyy/MM/dd HH:mm:ss||yyyy/MM/dd||epoch_millis\" }, \"someNumber\" : { \"type\" : \"text\", \"fields\" : { \"keyword\" : { \"type\" : \"keyword\", \"ignore_above\" : 256 } } } } } } } #写入新的数据，index以test开头 PUT testtemplate/_doc/1 { \"someNumber\":\"1\", \"someDate\":\"2019/01/01\" } GET testtemplate/_mapping #返回如下： { \"testtemplate\" : { \"mappings\" : { \"date_detection\" : false, \"numeric_detection\" : true, \"properties\" : { \"someDate\" : { #可以看到这里并没有命中字段类型日期匹配 \"type\" : \"text\", \"fields\" : { \"keyword\" : { \"type\" : \"keyword\", \"ignore_above\" : 256 } } }, \"someNumber\" : { \"type\" : \"long\" } } } } } get testtemplate/_settings #返回如下： { \"testtemplate\" : { \"settings\" : { \"index\" : { \"creation_date\" : \"1646703465756\", \"number_of_shards\" : \"1\", \"number_of_replicas\" : \"2\", \"uuid\" : \"wLIqiv5VToSVKctGSbT1Kg\", \"version\" : { \"created\" : \"7010099\" }, \"provided_name\" : \"testtemplate\" } } } } PUT testmy { \"settings\":{ \"number_of_replicas\":5 } } put testmy/_doc/1 { \"key\":\"value\" } get testmy/_settings #返回如下 { \"testmy\" : { \"settings\" : { \"index\" : { \"creation_date\" : \"1646703533380\", \"number_of_shards\" : \"1\", \"number_of_replicas\" : \"5\", #可以看到模板被覆盖 \"uuid\" : \"EAS7QZZtSTGcYSN-8MlEEA\", \"version\" : { \"created\" : \"7010099\" }, \"provided_name\" : \"testmy\" } } } } ","date":"2022-03-08","objectID":"/posts/search_engine/es/6.html/:1:0","tags":["search_engine","note","es"],"title":"ElasticSearchTemplate使用","uri":"/posts/search_engine/es/6.html/"},{"categories":["search engine"],"content":"Dynamic Template 根据 Elasticsearch 识别的数据类型，结合字段名称，来动态设定字段类型。 比如所有字符串类型都被设定为 Keyword，is 开头的字段都设置成 boolean。 这里需要注意的是： DynamicTemplate 是定义在某个索引的 mapping 中 Template 有一个名称 匹配规则是一个数组 为匹配到字段设置 Mapping PUT my_index/_doc/1 { \"firstName\":\"Ruan\", \"isVIP\":\"true\" } GET my_index/_mapping #返回如下 { \"my_index\" : { \"mappings\" : { \"properties\" : { \"firstName\" : { \"type\" : \"text\", \"fields\" : { \"keyword\" : { \"type\" : \"keyword\", \"ignore_above\" : 256 } } }, \"isVIP\" : { \"type\" : \"text\", \"fields\" : { \"keyword\" : { \"type\" : \"keyword\", \"ignore_above\" : 256 } } } } } } } DELETE my_index PUT my_index { \"mappings\": { \"dynamic_templates\": [ { \"strings_as_boolean\": { \"match_mapping_type\": \"string\", \"match\":\"is*\", \"mapping\": { \"type\": \"boolean\" } } }, { \"strings_as_keywords\": { \"match_mapping_type\": \"string\", \"mapping\": { \"type\": \"keyword\" } } } ] } } PUT my_index/_doc/1 { \"firstName\":\"Ruan\", \"isVIP\":\"true\" } # GET my_index/_mapping #返回如下： { \"my_index\" : { \"mappings\" : { \"dynamic_templates\" : [ { \"strings_as_boolean\" : { \"match\" : \"is*\", \"match_mapping_type\" : \"string\", \"mapping\" : { \"type\" : \"boolean\" } } }, { \"strings_as_keywords\" : { \"match_mapping_type\" : \"string\", \"mapping\" : { \"type\" : \"keyword\" } } } ], \"properties\" : { \"firstName\" : { \"type\" : \"keyword\" }, \"isVIP\" : { \"type\" : \"boolean\" } } } } } DELETE my_index #结合路径 PUT my_index { \"mappings\": { \"dynamic_templates\": [ { \"full_name\": { \"path_match\": \"name.*\", \"path_unmatch\": \"*.middle\", \"mapping\": { \"type\": \"text\", \"copy_to\": \"full_name\" } } } ] } } PUT my_index/_doc/1 { \"name\": { \"first\": \"John\", \"middle\": \"Winston\", \"last\": \"Lennon\" } } GET my_index/_search?q=full_name:John #返回如下： { \"took\" : 92, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 1, \"relation\" : \"eq\" }, \"max_score\" : 0.2876821, \"hits\" : [ { \"_index\" : \"my_index\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"_score\" : 0.2876821, \"_source\" : { \"name\" : { \"first\" : \"John\", \"middle\" : \"Winston\", \"last\" : \"Lennon\" } } } ] } } ","date":"2022-03-08","objectID":"/posts/search_engine/es/6.html/:2:0","tags":["search_engine","note","es"],"title":"ElasticSearchTemplate使用","uri":"/posts/search_engine/es/6.html/"},{"categories":["search engine"],"content":"参考 https://time.geekbang.org/course/intro/100030501?tab=catalog ","date":"2022-03-08","objectID":"/posts/search_engine/es/6.html/:3:0","tags":["search_engine","note","es"],"title":"ElasticSearchTemplate使用","uri":"/posts/search_engine/es/6.html/"},{"categories":["search engine"],"content":"多字段特性 以不同的特性索引字段来实现不同的需求，即多字段的特性。 PUT products { \"mappings\":{ \"properties\":{ \"company\":{ \"type\":\"text\", \"fields\":{ \"keyword\":{ \"type\":\"keyword\", \"ignore_above\":256, } } }, \"comment\":{ \"type\":\"text\", \"fields\":{ \"english_comment\":{ \"type\":\"text\", \"analyzer\":\"english\", \"search_anlyzer\":\"english\" } } } } } } ","date":"2022-03-08","objectID":"/posts/search_engine/es/5.html/:1:0","tags":["search_engine","note","es"],"title":"ElasticSearch多字段特性\u0026自定义Analyzer","uri":"/posts/search_engine/es/5.html/"},{"categories":["search engine"],"content":"ExactValues\u0026FullText Exact Value：包括数字/日期/具体一个字符串 keyword Full text：全文本，非结构化的文本数据 text 其中 Exact Value 不需要被分词，会为每一个字段创建一个倒排索引。 ","date":"2022-03-08","objectID":"/posts/search_engine/es/5.html/:1:1","tags":["search_engine","note","es"],"title":"ElasticSearch多字段特性\u0026自定义Analyzer","uri":"/posts/search_engine/es/5.html/"},{"categories":["search engine"],"content":"自定义分词 可以通过不同的组合实现自定义的分词器： Character Filter 在 Tokenizer 之前对文本进行处理，可配置多个，且会影响 Tokenizer 的 position 和 offset 信息。 自带的 Character Filter：HTML strip（去除 html 标签）、Mapping（字符串替换）、Pattern replace（正则替换）。 POST _analyze { \"tokenizer\":\"keyword\", \"char_filter\":[\"html_strip\"], \"text\": \"\u003cb\u003ehello world\u003c/b\u003e\" } #返回如下 { \"tokens\" : [ { \"token\" : \"hello world\", \"start_offset\" : 3, \"end_offset\" : 18, \"type\" : \"word\", \"position\" : 0 } ] } #使用char filter进行替换 POST _analyze { \"tokenizer\": \"standard\", \"char_filter\": [ { \"type\" : \"mapping\", \"mappings\" : [ \"- =\u003e _\"] } ], \"text\": \"123-456, I-test\" } #返回如下： { \"tokens\" : [ { \"token\" : \"123_456\", \"start_offset\" : 0, \"end_offset\" : 7, \"type\" : \"\u003cNUM\u003e\", \"position\" : 0 }, { \"token\" : \"I_test\", \"start_offset\" : 9, \"end_offset\" : 15, \"type\" : \"\u003cALPHANUM\u003e\", \"position\" : 1 } ] } //正则表达式 GET _analyze { \"tokenizer\": \"standard\", \"char_filter\": [ { \"type\" : \"pattern_replace\", \"pattern\" : \"http://(.*)\", \"replacement\" : \"$1\" } ], \"text\" : \"http://www.elastic.co\" } #返回如下： { \"tokens\" : [ { \"token\" : \"www.elastic.co\", \"start_offset\" : 0, \"end_offset\" : 21, \"type\" : \"\u003cALPHANUM\u003e\", \"position\" : 0 } ] } Tokenizer 将原始的文本按照一定的规则，切分为词（term or token）。 Elasticsearch 内置的 Tokenizer whitespace/standard/uax_url email/pattern/keyword/path hierarchy 可以用官方提供的库自己开发。 POST _analyze { \"tokenizer\":\"path_hierarchy\", \"text\":\"/user/e/a\" } #返回如下 { \"tokens\" : [ { \"token\" : \"/user\", \"start_offset\" : 0, \"end_offset\" : 5, \"type\" : \"word\", \"position\" : 0 }, { \"token\" : \"/user/e\", \"start_offset\" : 0, \"end_offset\" : 7, \"type\" : \"word\", \"position\" : 0 }, { \"token\" : \"/user/e/a\", \"start_offset\" : 0, \"end_offset\" : 9, \"type\" : \"word\", \"position\" : 0 } ] } TokenFilter 将 Tokenizer 输出的单词（term），进行增加、修改、删除。 Elasticsearch 内置的： Lowercase/stop/synonym（添加近义词） // white space and snowball GET _analyze { \"tokenizer\": \"whitespace\", \"filter\": [\"stop\",\"snowball\"], \"text\": [\"The gilrs in China are playing this game!\"] } #返回如下： { \"tokens\" : [ { \"token\" : \"The\", \"start_offset\" : 0, \"end_offset\" : 3, \"type\" : \"word\", \"position\" : 0 }, { \"token\" : \"gilr\", \"start_offset\" : 4, \"end_offset\" : 9, \"type\" : \"word\", \"position\" : 1 }, { \"token\" : \"China\", \"start_offset\" : 13, \"end_offset\" : 18, \"type\" : \"word\", \"position\" : 3 }, { \"token\" : \"play\", \"start_offset\" : 23, \"end_offset\" : 30, \"type\" : \"word\", \"position\" : 5 }, { \"token\" : \"game!\", \"start_offset\" : 36, \"end_offset\" : 41, \"type\" : \"word\", \"position\" : 7 } ] } //remove 加入lowercase后，The被当成 stopword删除 GET _analyze { \"tokenizer\": \"whitespace\", \"filter\": [\"lowercase\",\"stop\",\"snowball\"], \"text\": [\"The gilrs in China are playing this game!\"] } #返回如下： { \"tokens\" : [ { \"token\" : \"gilr\", \"start_offset\" : 4, \"end_offset\" : 9, \"type\" : \"word\", \"position\" : 1 }, { \"token\" : \"china\", \"start_offset\" : 13, \"end_offset\" : 18, \"type\" : \"word\", \"position\" : 3 }, { \"token\" : \"play\", \"start_offset\" : 23, \"end_offset\" : 30, \"type\" : \"word\", \"position\" : 5 }, { \"token\" : \"game!\", \"start_offset\" : 36, \"end_offset\" : 41, \"type\" : \"word\", \"position\" : 7 } ] } ","date":"2022-03-08","objectID":"/posts/search_engine/es/5.html/:2:0","tags":["search_engine","note","es"],"title":"ElasticSearch多字段特性\u0026自定义Analyzer","uri":"/posts/search_engine/es/5.html/"},{"categories":["search engine"],"content":"参考 https://time.geekbang.org/course/intro/100030501?tab=catalog ","date":"2022-03-08","objectID":"/posts/search_engine/es/5.html/:3:0","tags":["search_engine","note","es"],"title":"ElasticSearch多字段特性\u0026自定义Analyzer","uri":"/posts/search_engine/es/5.html/"},{"categories":["search engine"],"content":"字段的数据类型 字段的数据类型主要如下： 简单类型 Text/Keyword Date Integer/Floating Boolean IPv4\u0026IPv6 复杂类型-对象和嵌套对象 对象类型/嵌套类型 特殊类型 geo_point\u0026geo_shape/percolator ","date":"2022-03-07","objectID":"/posts/search_engine/es/4.html/:1:0","tags":["search_engine","note","es"],"title":"ElasticSearchMapping","uri":"/posts/search_engine/es/4.html/"},{"categories":["search engine"],"content":"Mapping Mapping 类似数据库中的 Schema 的定义，作用如下： 定义索引中的字段的名称 定义字段的数据类型 字段，倒排索引的相关，Analyzer等 Mapping 会把 JSON 文档映射成 Lucene 所需要的扁平格式 一个 Mapping 属于一个索引的 Type 每个文档都属于一个 Type 一个 Type 有一个 Mapping 定义 ","date":"2022-03-07","objectID":"/posts/search_engine/es/4.html/:2:0","tags":["search_engine","note","es"],"title":"ElasticSearchMapping","uri":"/posts/search_engine/es/4.html/"},{"categories":["search engine"],"content":"显示自定义 Mappings PUT movies { \"mappings\":{ //define your mappings here } } 在自定义 Mappings 我们需要注意的是： 可以参考 API 手册手写 为了减少工作量和出错概率，最好可以创建临时 index 来测试 控制当前字段是否被索引 index 控制当前字段是否被索引： 默认为true，若设置为false则该字段不可被搜索 PUT users { \"mappings\" : { \"properties\" : { \"firstName\" : { \"type\" : \"text\" }, \"lastName\" : { \"type\" : \"text\" }, \"mobile\" : { \"type\" : \"text\", \"index\": false } } } } PUT users/_doc/1 { \"firstName\":\"Ruan\", \"lastName\": \"Yiming\", \"mobile\": \"12345678\" } POST /users/_search { \"query\": { \"match\": { \"mobile\":\"12345678\" } } } #返回如下： { \"error\": { \"root_cause\": [ { \"type\": \"query_shard_exception\", \"reason\": \"failed to create query: {\\n \\\"match\\\" : {\\n \\\"mobile\\\" : {\\n \\\"query\\\" : \\\"12345678\\\",\\n \\\"operator\\\" : \\\"OR\\\",\\n \\\"prefix_length\\\" : 0,\\n \\\"max_expansions\\\" : 50,\\n \\\"fuzzy_transpositions\\\" : true,\\n \\\"lenient\\\" : false,\\n \\\"zero_terms_query\\\" : \\\"NONE\\\",\\n \\\"auto_generate_synonyms_phrase_query\\\" : true,\\n \\\"boost\\\" : 1.0\\n }\\n }\\n}\", \"index_uuid\": \"SxPqkWdSSmqvEN1Fs1_VSQ\", \"index\": \"users\" } ], \"type\": \"search_phase_execution_exception\", \"reason\": \"all shards failed\", \"phase\": \"query\", \"grouped\": true, \"failed_shards\": [ { \"shard\": 0, \"index\": \"users\", \"node\": \"Ko9MGMaiQsqw-So5TLyBIg\", \"reason\": { \"type\": \"query_shard_exception\", \"reason\": \"failed to create query: {\\n \\\"match\\\" : {\\n \\\"mobile\\\" : {\\n \\\"query\\\" : \\\"12345678\\\",\\n \\\"operator\\\" : \\\"OR\\\",\\n \\\"prefix_length\\\" : 0,\\n \\\"max_expansions\\\" : 50,\\n \\\"fuzzy_transpositions\\\" : true,\\n \\\"lenient\\\" : false,\\n \\\"zero_terms_query\\\" : \\\"NONE\\\",\\n \\\"auto_generate_synonyms_phrase_query\\\" : true,\\n \\\"boost\\\" : 1.0\\n }\\n }\\n}\", \"index_uuid\": \"SxPqkWdSSmqvEN1Fs1_VSQ\", \"index\": \"users\", \"caused_by\": { \"type\": \"illegal_argument_exception\", \"reason\": \"Cannot search on field [mobile] since it is not indexed.\" } } } ] }, \"status\": 400 } index Options 四种不同级别的 Index Options 配置，可以控制倒排索引记录的内容： docs：记录 doc id freqs：记录 doc id 和 term frequencies positions：记录 doc id/term frequencies/term position offsets：记录 doc id/term frequencies/term positions/character offects Text类型默认记录 positions，其他默认为 docs 记录内容越多，占用存储空间越大 null_value 需要对 Null 值实现搜索 只有 Keyword 类型支持设定 Null_Value #设定Null_value DELETE users PUT users { \"mappings\" : { \"properties\" : { \"firstName\" : { \"type\" : \"text\" }, \"lastName\" : { \"type\" : \"text\" }, \"mobile\" : { \"type\" : \"keyword\", \"null_value\": \"NULL\" } } } } PUT users/_doc/1 { \"firstName\":\"Ruan\", \"lastName\": \"Yiming\", \"mobile\": null } GET users/_search { \"query\": { \"match\": { \"mobile\":\"NULL\" } } } # 返回内容： { \"took\" : 6, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 1, \"relation\" : \"eq\" }, \"max_score\" : 0.2876821, \"hits\" : [ { \"_index\" : \"users\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"_score\" : 0.2876821, \"_source\" : { \"firstName\" : \"Ruan\", \"lastName\" : \"Yiming\", \"mobile\" : null } } ] } } copy_to 设置 满足一些特定的搜索需求 copy_to将字段的数值拷贝到目标字段 copy_to的目标字段不出现在_source中 #设置 Copy to DELETE users PUT users { \"mappings\": { \"properties\": { \"firstName\":{ \"type\": \"text\", \"copy_to\": \"fullName\" }, \"lastName\":{ \"type\": \"text\", \"copy_to\": \"fullName\" } } } } PUT users/_doc/1 { \"firstName\":\"Ruan\", \"lastName\": \"Yiming\" } GET users/_search?q=fullName:(Ruan Yiming) #以上等同 POST users/_search { \"query\": { \"match\": { \"fullName\":{ \"query\": \"Ruan Yiming\", \"operator\": \"and\" } } } } #返回如下： { \"took\" : 44, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 1, \"relation\" : \"eq\" }, \"max_score\" : 0.5753642, \"hits\" : [ { \"_index\" : \"users\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"_score\" : 0.5753642, \"_source\" : { \"firstName\" : \"Ruan\", \"lastName\" : \"Yiming\" } } ] } } 数组类型 Elasticsearch中不提供专门的数组类型，但任何字段都可以包含多个相同类型的数值。 #数组类型 PUT users/_doc/1 { \"name\":\"onebird\", \"interests\":\"reading\" } PUT users/_doc/1 { \"name\":\"twobirds\", \"interests\":[\"reading\",\"music\"] } POST users/_search { \"query\": { \"match_all\": {} } } #返回如下： { \"took\" : 466, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipp","date":"2022-03-07","objectID":"/posts/search_engine/es/4.html/:2:1","tags":["search_engine","note","es"],"title":"ElasticSearchMapping","uri":"/posts/search_engine/es/4.html/"},{"categories":["search engine"],"content":"Dynamic Mapping Dynamic Mapping 机制的作用在于： 无需手动定义 Mappings，自动根据文档信息推断出字段类型 推断结果可能会存在错误，严重则导致一些功能无法运行，如地理位置信息、Range 查询 类型的自动识别 JSON类型 Elasticsearch类型 字符串 1.匹配日期格式，设置成Date 2.配置数字设置为float或long，该选项默认关闭 3.设置为Text，并且增加keyword子字段 布尔值 boolean 浮点数 float 整数 long 对象 Object 数组 由第一个非空数值的类型决定 空值 忽略 #写入文档，查看 Mapping PUT mapping_test/_doc/1 { \"firstName\":\"Chan\", \"lastName\": \"Jackie\", \"loginDate\":\"2018-07-24T10:29:48.103Z\" } #查看 Mapping文件 GET mapping_test/_mapping #返回如下： { \"mapping_test\" : { \"mappings\" : { \"properties\" : { \"firstName\" : { \"type\" : \"text\", \"fields\" : { \"keyword\" : { \"type\" : \"keyword\", \"ignore_above\" : 256 } } }, \"lastName\" : { \"type\" : \"text\", \"fields\" : { \"keyword\" : { \"type\" : \"keyword\", \"ignore_above\" : 256 } } }, \"loginDate\" : { \"type\" : \"date\" } } } } } #Delete index DELETE mapping_test #dynamic mapping，推断字段的类型 PUT mapping_test/_doc/1 { \"uid\" : \"123\", \"isVip\" : false, \"isAdmin\": \"true\", \"age\":19, \"heigh\":180 } #查看 Dynamic GET mapping_test/_mapping #返回如下： { \"mapping_test\" : { \"mappings\" : { \"properties\" : { \"age\" : { \"type\" : \"long\" }, \"heigh\" : { \"type\" : \"long\" }, \"isAdmin\" : { \"type\" : \"text\", \"fields\" : { \"keyword\" : { \"type\" : \"keyword\", \"ignore_above\" : 256 } } }, \"isVip\" : { \"type\" : \"boolean\" }, \"uid\" : { \"type\" : \"text\", \"fields\" : { \"keyword\" : { \"type\" : \"keyword\", \"ignore_above\" : 256 } } } } } } } ","date":"2022-03-07","objectID":"/posts/search_engine/es/4.html/:2:2","tags":["search_engine","note","es"],"title":"ElasticSearchMapping","uri":"/posts/search_engine/es/4.html/"},{"categories":["search engine"],"content":"更改 Mapping 字段类型 新增加字段 Dynamic设为True时，有新增字段写入 Mapping 会被更新 Dynamic设为False时，Mapping 不会被更新，新增字段数据无法被索引，但会出现在_source中 Dynamic设为Strict时，文档写入失败 对已有字段 一旦已经有数据写入，不再支持修改字段定义 因 Lucene 实现的倒排索引，一旦生成后，就不允许被修改。 必须 Reindex API 重建索引的方式来改变字段类型 若修改了字段的数据类型，会导致已被缩影的属于无法被搜索，但如果新增字段就不会被影响。 ","date":"2022-03-07","objectID":"/posts/search_engine/es/4.html/:2:3","tags":["search_engine","note","es"],"title":"ElasticSearchMapping","uri":"/posts/search_engine/es/4.html/"},{"categories":["search engine"],"content":"控制 Dyanmic Mappings true flase stric 文档可索引 YES YES NO 字段可索引 YES NO NO Mappings被更新 YES NO NO 被设置为False时若新增字段写入，该数据可被索引，但新增字段会被丢弃 被设置为Stric时，数据写入直接报错 #默认Mapping支持dynamic，写入的文档中加入新的字段 PUT dynamic_mapping_test/_doc/1 { \"newField\":\"someValue\" } #该字段可以被搜索，数据也在_source中出现 POST dynamic_mapping_test/_search { \"query\":{ \"match\":{ \"newField\":\"someValue\" } } } #返回如下： { \"took\" : 25, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 1, \"relation\" : \"eq\" }, \"max_score\" : 0.2876821, \"hits\" : [ { \"_index\" : \"dynamic_mapping_test\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"_score\" : 0.2876821, \"_source\" : { \"newField\" : \"someValue\" } } ] } } #修改为dynamic false PUT dynamic_mapping_test/_mapping { \"dynamic\": false } #新增 anotherField PUT dynamic_mapping_test/_doc/10 { \"anotherField\":\"someValue\" } #返回如下： { \"_index\" : \"dynamic_mapping_test\", \"_type\" : \"_doc\", \"_id\" : \"10\", \"_version\" : 1, \"result\" : \"created\", \"_shards\" : { \"total\" : 2, \"successful\" : 2, \"failed\" : 0 }, \"_seq_no\" : 1, \"_primary_term\" : 1 } #该字段不可以被搜索，因为dynamic已经被设置为false POST dynamic_mapping_test/_search { \"query\":{ \"match\":{ \"anotherField\":\"someValue\" } } } #返回如下： { \"took\" : 766, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 0, \"relation\" : \"eq\" }, \"max_score\" : null, \"hits\" : [ ] } } # 但是文档可以被索引到 get dynamic_mapping_test/_doc/10 # 返回如下： { \"_index\" : \"dynamic_mapping_test\", \"_type\" : \"_doc\", \"_id\" : \"10\", \"_version\" : 1, \"_seq_no\" : 1, \"_primary_term\" : 1, \"found\" : true, \"_source\" : { \"anotherField\" : \"someValue\" } } #修改为strict PUT dynamic_mapping_test/_mapping { \"dynamic\": \"strict\" } #写入数据出错，HTTP Code 400 PUT dynamic_mapping_test/_doc/12 { \"lastField\":\"value\" } #返回如下： { \"error\": { \"root_cause\": [ { \"type\": \"strict_dynamic_mapping_exception\", \"reason\": \"mapping set to strict, dynamic introduction of [lastField] within [_doc] is not allowed\" } ], \"type\": \"strict_dynamic_mapping_exception\", \"reason\": \"mapping set to strict, dynamic introduction of [lastField] within [_doc] is not allowed\" }, \"status\": 400 } ","date":"2022-03-07","objectID":"/posts/search_engine/es/4.html/:2:4","tags":["search_engine","note","es"],"title":"ElasticSearchMapping","uri":"/posts/search_engine/es/4.html/"},{"categories":["search engine"],"content":"参考 https://time.geekbang.org/course/intro/100030501?tab=catalog ","date":"2022-03-07","objectID":"/posts/search_engine/es/4.html/:3:0","tags":["search_engine","note","es"],"title":"ElasticSearchMapping","uri":"/posts/search_engine/es/4.html/"},{"categories":["search engine"],"content":"Search API 概览 主要包含两部分： URL Search 在 URL 中使用查询参数 Request Body Search 使用 Elasticsearch 提供的，基于 JSON 格式的更加完备的QueryDomainSpecificLanguage(DSL) ","date":"2022-03-07","objectID":"/posts/search_engine/es/3.html/:1:0","tags":["search_engine","note","es"],"title":"ElasticSearchSearchAPI","uri":"/posts/search_engine/es/3.html/"},{"categories":["search engine"],"content":"指定查询的索引 /_search：集群上所有的索引 /index1/_search：index1 /index1,index2/_search：index1和index2 /index*/_search：以index开头的索引 ","date":"2022-03-07","objectID":"/posts/search_engine/es/3.html/:1:1","tags":["search_engine","note","es"],"title":"ElasticSearchSearchAPI","uri":"/posts/search_engine/es/3.html/"},{"categories":["search engine"],"content":"URL 查询 使用q，指定查询字符串 query strig syntax，KV键值对 比如： curl -XGET \"http://localhost:9200/${index}/_search?q=${search}\" ","date":"2022-03-07","objectID":"/posts/search_engine/es/3.html/:1:2","tags":["search_engine","note","es"],"title":"ElasticSearchSearchAPI","uri":"/posts/search_engine/es/3.html/"},{"categories":["search engine"],"content":"Request Body 查询 #支持POST和GET #'match_all'指返回所有的文档 curl -XGET \"http://localhost:9200/${index}/_search\" -H 'Content-Type:application/json` -d '{\"query:{\"match_all\":{}}}' ","date":"2022-03-07","objectID":"/posts/search_engine/es/3.html/:1:3","tags":["search_engine","note","es"],"title":"ElasticSearchSearchAPI","uri":"/posts/search_engine/es/3.html/"},{"categories":["search engine"],"content":"搜索相关性 Relevance 用户搜索关心的是搜索结果的相关性，如是否可用找到所有有关的内容、是否有不相关的内容、文档的打分是否合理、结果排名是否满足需求等。 如不同的搜索需求： Web 搜索 电商搜索 衡量相关性（Information Retrieval）主要有以下标准衡量： Precision：尽可能返回较少的无关文档 Recall：尽量返回较多的相关文档 Ranking：是否能够按照相关度进行排序 Precision 和 Recall 的计算类似机器学习中的模型评估 Precision = TP/TP+FP Recall = TP/TP+FN ","date":"2022-03-07","objectID":"/posts/search_engine/es/3.html/:1:4","tags":["search_engine","note","es"],"title":"ElasticSearchSearchAPI","uri":"/posts/search_engine/es/3.html/"},{"categories":["search engine"],"content":"URL Search q指定查询语句，使用 Qeury String Syntax df默认字段，不指定时会对所有字段进行查询 Sort排序 from和size可用于分页 profile可以查看查询是如何执行的 #基本查询 GET /movies/_search?q=2012\u0026df=title\u0026sort=year:desc\u0026from=0\u0026size=10\u0026timeout=1s #带profile GET /movies/_search?q=2012\u0026df=title { \"profile\":\"true\" } #泛查询，正对_all,所有字段 GET /movies/_search?q=2012 { \"profile\":\"true\" } #指定字段 GET /movies/_search?q=title:2012\u0026sort=year:desc\u0026from=0\u0026size=10\u0026timeout=1s { \"profile\":\"true\" } 指定字段和泛查询 q=tiltle:2012 q=2012 Term和Phrase A B等效于 A OR B \"A B\"等效于 A AND B 分组和引号 title:(A AND B) title=\"A B\" # 查找美丽心灵, Mind为泛查询 GET /movies/_search?q=title:Beautiful Mind { \"profile\":\"true\" } # 泛查询 GET /movies/_search?q=title:2012 { \"profile\":\"true\" } #使用引号，Phrase查询 GET /movies/_search?q=title:\"Beautiful Mind\" { \"profile\":\"true\" } #分组，Bool查询 GET /movies/_search?q=title:(Beautiful Mind) { \"profile\":\"true\" } 布尔操作 AND/OR/NOT或者\u0026\u0026/||/! 必须大写 title:(matrix NOT reloaded) 分组 +表示must -表示must_not `title:(+matrix -reloaded) #布尔操作符 # 查找美丽心灵 GET /movies/_search?q=title:(Beautiful AND Mind) { \"profile\":\"true\" } # 查找美丽心灵 GET /movies/_search?q=title:(Beautiful NOT Mind) { \"profile\":\"true\" } # 查找美丽心灵 GET /movies/_search?q=title:(Beautiful %2BMind) { \"profile\":\"true\" } 范围查询 区间表示：[]闭区间，{}开区间 year:{2019 TO 2018} year:[* TO 2018] 算数符号 year:\u003e2010 year:(\u003e2010 \u0026\u0026 \u003c=2018\u003e) year:(+\u003e2010 +\u003c=2018) #范围查询 ,区间写法 GET /movies/_search?q=title:beautiful AND year:[2002 TO 2018] { \"profile\":\"true\" } GET /movies/_search?q=title:year:\u003e1980 { \"profile\":\"true\" } 通配符查询（效率较低，占用内存大，不建议使用。特别是放在最前面） ?代表1个字符，*代表0或多个字符 title:mi?d title:be* 正则表达式 title:[bt]oy 模糊匹配与近似查询 title:beutiful~1 title:\"long rings\"~2 #通配符查询 GET /movies/_search?q=title:b* { \"profile\":\"true\" } //模糊匹配\u0026近似度匹配 GET /movies/_search?q=title:beautifl~1 { \"profile\":\"true\" } GET /movies/_search?q=title:\"Lord Rings\"~2 { \"profile\":\"true\" } ","date":"2022-03-07","objectID":"/posts/search_engine/es/3.html/:2:0","tags":["search_engine","note","es"],"title":"ElasticSearchSearchAPI","uri":"/posts/search_engine/es/3.html/"},{"categories":["search engine"],"content":"Request Body Search 将查询语句通过 HTTP Request Body 发送给 Elasticsesarch Query DSL #ignore_unavailable=true，可以忽略尝试访问不存在的索引“404_idx”导致的报错 #查询movies分页 POST /movies,404_idx/_search?ignore_unavailable=true { \"profile\": true, \"query\": { \"match_all\": {} } } 分页 From从0开始，默认返回10个结果 获取靠后的翻页成本较高 POST /kibana_sample_data_ecommerce/_search { \"from\":10, \"size\":20, \"query\":{ \"match_all\": {} } } 排序 最好在“数字型”与“日期型”字段上排序 因为对于多值类型或分析过的字段排序，系统会选一个值，无法得知该值 #对日期排序 POST kibana_sample_data_ecommerce/_search { \"sort\":[{\"order_date\":\"desc\"}], \"query\":{ \"match_all\": {} } } _source filtering 如果_SOURCE没有存储，那就只返回匹配的文档的元数据 _source支持使用通配符 `_souce[“name*,“desc*\"] #source filtering POST kibana_sample_data_ecommerce/_search { \"_source\":[\"order_date\"], \"query\":{ \"match_all\": {} } } 脚本字段 能通过表达式返回所需字段 #脚本字段 GET kibana_sample_data_ecommerce/_search { \"script_fields\": { \"new_field\": { \"script\": { \"lang\": \"painless\", \"source\": \"doc['order_date'].value+'hello'\" } } }, \"query\": { \"match_all\": {} } } 使用查询表达式-Match POST movies/_search { \"query\": { \"match\": { \"title\": \"last christmas\" #默认OR } } } POST movies/_search { \"query\": { \"match\": { \"title\": { \"query\": \"last christmas\", \"operator\": \"and\" } } } } 短语搜索-Match Phrase POST movies/_search { \"query\": { \"match_phrase\": { \"title\":{ \"query\": \"one love\" #默认AND } } } } POST movies/_search { \"query\": { \"match_phrase\": { \"title\":{ \"query\": \"one love\", \"slop\": 1 #指定数量的Term } } } } ","date":"2022-03-07","objectID":"/posts/search_engine/es/3.html/:3:0","tags":["search_engine","note","es"],"title":"ElasticSearchSearchAPI","uri":"/posts/search_engine/es/3.html/"},{"categories":["search engine"],"content":"Query String Query 类似URL Query POST users/_search { \"query\": { \"query_string\": { \"default_field\": \"name\", \"query\": \"Ruan AND Yiming\" # 与操作 } } } POST users/_search { \"query\": { \"query_string\": { \"fields\":[\"name\",\"about\"], \"query\": \"(Ruan AND Yiming) OR (Java AND Elasticsearch)\" } } } # 多fields GET /movies/_search { \"profile\": true, \"query\":{ \"query_string\":{ \"fields\":[ \"title\", \"year\" ], \"query\": \"2012\" } } } ","date":"2022-03-07","objectID":"/posts/search_engine/es/3.html/:4:0","tags":["search_engine","note","es"],"title":"ElasticSearchSearchAPI","uri":"/posts/search_engine/es/3.html/"},{"categories":["search engine"],"content":"Simple Query String Query 类似 Query String，但是会忽略错误的语法，同时只支持部分查询语法 不支持 AND OR NOT，会当作字符串处理 Term 之间默认的关系是 OR，可以指定 Operator 支持部分逻辑 +替代AND -替代OR -替代NOT #Simple Query 默认的operator是 Or POST users/_search { \"query\": { \"simple_query_string\": { \"query\": \"Ruan AND Yiming\", \"fields\": [\"name\"] } } } POST users/_search { \"query\": { \"simple_query_string\": { \"query\": \"Ruan Yiming\", \"fields\": [\"name\"], \"default_operator\": \"AND\" } } } GET /movies/_search { \"profile\":true, \"query\":{ \"simple_query_string\":{ \"query\":\"Beautiful +mind\", \"fields\":[\"title\"] } } } ","date":"2022-03-07","objectID":"/posts/search_engine/es/3.html/:5:0","tags":["search_engine","note","es"],"title":"ElasticSearchSearchAPI","uri":"/posts/search_engine/es/3.html/"},{"categories":["search engine"],"content":"参考 https://time.geekbang.org/course/intro/100030501?tab=catalog ","date":"2022-03-07","objectID":"/posts/search_engine/es/3.html/:6:0","tags":["search_engine","note","es"],"title":"ElasticSearchSearchAPI","uri":"/posts/search_engine/es/3.html/"},{"categories":["os"],"content":" 以下主要来自 LMOS 大佬的操作系统课程中记录的笔记 ","date":"2022-03-07","objectID":"/posts/os/common/2.html/:0:0","tags":["os","note"],"title":"os中的内核","uri":"/posts/os/common/2.html/"},{"categories":["os"],"content":"计算机中的资源管理——内核 计算机中资源主要分为： 硬件资源（如总线、CPU、内存、硬盘、网卡、显卡、I/O设备等） 软件资源（如各种文件、软件程序等） 内核作为硬件资源和软件资源的管理者，其内部组成在逻辑上大致如下： 内核除了下面这些必要组件，根据功能不同还有安全组件等，且为了管理和控制各种硬件编写的代码，即称为“驱动程序”。 管理CPU 由于CPU是执行程序的，而内核把运行时的程序抽象成进程，所以又称为进程管理。 管理内存 由于程序和数据都要占用内存，内核要非常小心地分配、释放内存。 管理硬盘 硬盘主要存放用户数据，而内核把用户数据抽象成文件，即管理文件，文件需要合理地组织，方便用户查找和读写，所以形成了文件系统。 管理显卡 负责显示信息，而现在操作系统都是支持 GUI（图形用户接口）的，管理显卡自然而然地就成了内核中的图形系统。 管理网卡 网卡主要完成网络通信，网络通信需要各种通信协议，最后在内核中就形成了网络协议栈，又称网络组件。 管理各种 I/O 设备 键盘、鼠标、打印机、显示器等统称为 I/O（输入输出）设备，在内核中抽象成 I/O 管理器。 ","date":"2022-03-07","objectID":"/posts/os/common/2.html/:1:0","tags":["os","note"],"title":"os中的内核","uri":"/posts/os/common/2.html/"},{"categories":["os"],"content":"经典内核结构 如何组织内核的组件，让系统稳定和高效运行，这就是内核结构需要考虑的事情。现有的经典内核结构有以下： 宏内核结构 微内核结构 混合内核结构 ","date":"2022-03-07","objectID":"/posts/os/common/2.html/:2:0","tags":["os","note"],"title":"os中的内核","uri":"/posts/os/common/2.html/"},{"categories":["os"],"content":"宏内核结构 宏内核就是把以上诸如管理进程的代码、管理内存的代码、管理各种 I/O 设备的代码、文件系统的代码、图形系统代码以及其它功能模块的代码，把这些所有的代码经过编译，最后链接在一起，形成一个大的可执行程序： 大程序里有实现支持这些功能的所有代码，向用户应用软件提供一些接口（系统 API 函数） 大程序会在处理器的特权模式下运行（宏内核模式） 宏内核主要的缺点： 没有模块化，扩展性，移动性，开发新的能需要重新编译、链接、安装内核 高度耦合，造成“牵一发而动全身” 而主要的有点就在于： 性能很好（组件之间互相直接调用，没有过多消耗） 比如宏内核提供内存分配功能的服务过程，具体如下： 调用API：应用程序首先调用内存分配的 API（应用程序接口）函数； 内核运行：处理器切换到特权模式，开始运行内核代码； 内核分配内存：内核里的内存管理代码按照特定的算法，分配一块内存； API返回：把分配的内存块的首地址，返回给内存分配的 API 函数； 获取内存地址：内存分配的 API 函数返回，处理器开始运行用户模式下的应用程序，应用程序就得到了一块内存的首地址，并且可以使用这块内存了。 ","date":"2022-03-07","objectID":"/posts/os/common/2.html/:2:1","tags":["os","note"],"title":"os中的内核","uri":"/posts/os/common/2.html/"},{"categories":["os"],"content":"微内核结构 微内核架构正好与宏内核架构相反，它提倡内核功能尽可能少：仅仅只有进程调度、处理中断、内存空间映射、进程间通信等功能。而实际完成功能（如进程管理、内存管理、设备管理、文件管理等服务功能）是由服务进程（特殊的“应用程序”）专门负责。 微内核定义了一种良好的进程间通信的机制——消息，服务进程的编程模型就是循环处理来自其它进程的消息，完成相关的服务功能： 应用程序请求服务，则向微内核发送消息，微内核将消息转发给服务进程，然后服务进程完成服务后返回消息 微内核结构主要的缺点在于： 系统性能较低，转发消息和服务进程切换开销较大 而主要的优点在于： 系统结构清晰，利于协作开发 良好的移植性，且代码量较少 良好的伸缩性和扩展性，服务进程成本较低 微内核的代表作有 MACH、MINIX、L4 系统，这些系统都是微内核，但是它们不是商业级的系统，商业级的系统不采用微内核主要还是因为性能差。 而比如微内核提供内存分配功能的服务过程，具体如下： 调用API发送消息：应用程序发送内存分配的消息，这个发送消息的函数是微内核提供的，相当于系统 API，微内核的 API（应用程序接口）相当少，极端情况下仅需要两个，一个接收消息的 API 和一个发送消息的 API； 内核运行：处理器切换到特权模式，开始运行内核代码； 内核转发消息给服务进程：微内核代码让当前进程停止运行，并根据消息包中的数据，确定消息发送给谁，分配内存的消息当然是发送给内存管理服务进程； 服务进程分配内存：内存管理服务进程收到消息，分配一块内存； 服务进程返回消息给内核：内存管理服务进程，也会通过消息的形式返回分配内存块的地址给内核，然后继续等待下一条消息； 内核返回消息给应用程序：微内核把包含内存块地址的消息返回给发送内存分配消息的应用程序； 应用程序得到内存：处理器开始运行用户模式下的应用程序，应用程序就得到了一块内存的首地址，并且可以使用这块内存了。 ","date":"2022-03-07","objectID":"/posts/os/common/2.html/:2:2","tags":["os","note"],"title":"os中的内核","uri":"/posts/os/common/2.html/"},{"categories":["os"],"content":"内核设计 ","date":"2022-03-07","objectID":"/posts/os/common/2.html/:3:0","tags":["os","note"],"title":"os中的内核","uri":"/posts/os/common/2.html/"},{"categories":["os"],"content":"分离硬件的相关性 分层的主要目的和好处在于屏蔽底层细节，使上层开发更加简单。 计算机领域的一个基本方法是增加一个抽象层，从而使得抽象层的上下两层独立地发展。 分离硬件的相关性，就是要把操作硬件和处理硬件功能差异的代码抽离出来，形成一个独立的软件抽象层，对外提供相应的接口，方便上层开发。 比如进程调度模块，主要有两个机制：进程调度和进程切换。 不管是在 ARM 还是 X86 硬件平台，选择进程调度的代码不容易发生改变，而需要改变的代码是进程切换的相关代码，因为不同的硬件平台上下文是不同的。 所以这里最好是将进程切换分离到独立的层实现，当操作系统要运行在不同的硬件平台上时只需要修改该层的相关代码，提高其移植性。 ","date":"2022-03-07","objectID":"/posts/os/common/2.html/:3:1","tags":["os","note"],"title":"os中的内核","uri":"/posts/os/common/2.html/"},{"categories":["os"],"content":"自主设计的选择 将内核分为三个大层分别是： 内核接口层：定义了一系列接口 内核功能层：主要完成各种实际功能，这些功能按照其类别分成各种模块 内核硬件层：主要包括一个具体硬件平台相关的代码 这么设计的考虑： 吸取微内核优势（内核小维护成本小，且扩展性强），内核实现的功能很少； 吸收宏内核优势（高耦合代来的性能提升），把文件系统、网络组件、其它功能组件作为虚拟设备交由设备管理； ","date":"2022-03-07","objectID":"/posts/os/common/2.html/:3:2","tags":["os","note"],"title":"os中的内核","uri":"/posts/os/common/2.html/"},{"categories":["os"],"content":"业界成熟内核架构 ","date":"2022-03-07","objectID":"/posts/os/common/2.html/:4:0","tags":["os","note"],"title":"os中的内核","uri":"/posts/os/common/2.html/"},{"categories":["os"],"content":"Linux 内核 基本思想：一切都是文件 支持类 UNIX、POSIX 标准接口，也支持多用户、多进程、多线程、多核运行 宏内核结构 原图地址 ","date":"2022-03-07","objectID":"/posts/os/common/2.html/:4:1","tags":["os","note"],"title":"os中的内核","uri":"/posts/os/common/2.html/"},{"categories":["os"],"content":"Darwin-XNU 内核 主要特点就是两个内核层即 Mach 层与 BSD 层。 微内核结构 在调用 Darwin 系统 API 时，会传入一个 API 号码，用这个号码去索引 Mach 陷入中断服务表中的函数。此时，API 号码如果小于 0，则表明请求的是 Mach 内核的服务，API 号码如果大于 0，则表明请求的是 BSD 内核的服务，它提供一整套标准的 POSIX 接口。 ","date":"2022-03-07","objectID":"/posts/os/common/2.html/:4:2","tags":["os","note"],"title":"os中的内核","uri":"/posts/os/common/2.html/"},{"categories":["os"],"content":"Windows NT 内核 高内聚，低耦合 硬件抽象层 HAL ，并在之上定义了小内核 混合内核结构 Windows NT从设计架构上看属于微内核结构，HAL层之上的内核层属于微内核的核心，之上的执行体属于内核级别的应用层。Windows NT从权限的角度看属于宏内核，内核模式之下功能完备，并不是微内核那样单薄。这种设计兼顾了结构清晰和性能良好两个优点。 ","date":"2022-03-07","objectID":"/posts/os/common/2.html/:4:3","tags":["os","note"],"title":"os中的内核","uri":"/posts/os/common/2.html/"},{"categories":["os"],"content":"参考 https://time.geekbang.org/column/article/372609 ","date":"2022-03-07","objectID":"/posts/os/common/2.html/:5:0","tags":["os","note"],"title":"os中的内核","uri":"/posts/os/common/2.html/"},{"categories":["search engine"],"content":"正排索引和倒排索引 书籍的目录也是生活中常见的正排索引。 ","date":"2022-03-05","objectID":"/posts/search_engine/es/1.html/:1:0","tags":["search_engine","note","es"],"title":"ElasticSearch倒排索引\u0026Analysis分词","uri":"/posts/search_engine/es/1.html/"},{"categories":["search engine"],"content":"倒排索引核心组成 其主要包含两个部分： 单词词典（Term Dictionary） 记录所有的单词，记录单词到倒排列表的关联关系。 单词词典一般较大，可通过插入查询性能较高 B+树或哈希拉链法实。 倒排列表（Posting List） 记录单词对应的文档组合，由倒排索引项组成。 其中倒排索引项（Posting）： 文档 ID 词频 TF：记录单词在文档出现的次数，用于相关性评分 位置（Position）：单词在文档中分词的位置，利于语句搜索 偏移（Offset）:单词在文档中开始结束位置，实现高亮显示 ","date":"2022-03-05","objectID":"/posts/search_engine/es/1.html/:1:1","tags":["search_engine","note","es"],"title":"ElasticSearch倒排索引\u0026Analysis分词","uri":"/posts/search_engine/es/1.html/"},{"categories":["search engine"],"content":"Analysis \u0026 Analyzer Analysis 即文本分析，是把全文转换一系列单词（term/token）的过程，也叫分词。而 Analysis 就是通过 Analyzer 实现的。 可使用 Elasticsearch 内置的分析器或者按需定制分析器。 除了在数据写入时转换词条，匹配 Query 语句时也需要用相同的分析器对查询语句进行分析。 ","date":"2022-03-05","objectID":"/posts/search_engine/es/1.html/:2:0","tags":["search_engine","note","es"],"title":"ElasticSearch倒排索引\u0026Analysis分词","uri":"/posts/search_engine/es/1.html/"},{"categories":["search engine"],"content":"Analyzer 的组成 分词器是专门处理分词的组件，Analyzer 由三部分组成： Character Filters：针对原始文本处理，如去除 html Tokenizer：按照规则，切分为单词 Token Filter：将切分的单词进行加工，小写，删除 stopwords，增加同义词 一般按照顺序 Character Filters -\u003e Tokenizer -\u003e Token Filter 对语句进行拆分。 ","date":"2022-03-05","objectID":"/posts/search_engine/es/1.html/:2:1","tags":["search_engine","note","es"],"title":"ElasticSearch倒排索引\u0026Analysis分词","uri":"/posts/search_engine/es/1.html/"},{"categories":["search engine"],"content":"使用 _analyzer API 可指定 Analyzer 进行测试 指定索引的字段进行测试 自定义分词进行测试 GET /_analyze { \"analyzer\":\"standard\", \"text\":\"xxx\" } POST index/_analyze { \"field\":\"xxx\", \"text\":\"xxx\" } POST /_analyze { \"tokenizer\":\"standard\", \"filter\":[\"lowercase\"], \"text\":\"xxx\" } ","date":"2022-03-05","objectID":"/posts/search_engine/es/1.html/:2:2","tags":["search_engine","note","es"],"title":"ElasticSearch倒排索引\u0026Analysis分词","uri":"/posts/search_engine/es/1.html/"},{"categories":["search engine"],"content":"Analyzer 类型 Standard Analyzer 默认分词器 按词切分 小写处理 组成如下： Tokenizer:Standard TokenFilters:Standard\u0026LowerCase\u0026Stop(默认关闭) #standard GET _analyze { \"analyzer\": \"standard\", \"text\": \"2 running Quick brown-foxes leap over lazy dogs in the summer evening.\" } { \"tokens\" : [ { \"token\" : \"2\", \"start_offset\" : 0, \"end_offset\" : 1, \"type\" : \"\u003cNUM\u003e\", \"position\" : 0 }, { \"token\" : \"running\", \"start_offset\" : 2, \"end_offset\" : 9, \"type\" : \"\u003cALPHANUM\u003e\", \"position\" : 1 }, { \"token\" : \"quick\", \"start_offset\" : 10, \"end_offset\" : 15, \"type\" : \"\u003cALPHANUM\u003e\", \"position\" : 2 }, { \"token\" : \"brown\", \"start_offset\" : 16, \"end_offset\" : 21, \"type\" : \"\u003cALPHANUM\u003e\", \"position\" : 3 }, { \"token\" : \"foxes\", \"start_offset\" : 22, \"end_offset\" : 27, \"type\" : \"\u003cALPHANUM\u003e\", \"position\" : 4 }, { \"token\" : \"leap\", \"start_offset\" : 28, \"end_offset\" : 32, \"type\" : \"\u003cALPHANUM\u003e\", \"position\" : 5 }, { \"token\" : \"over\", \"start_offset\" : 33, \"end_offset\" : 37, \"type\" : \"\u003cALPHANUM\u003e\", \"position\" : 6 }, { \"token\" : \"lazy\", \"start_offset\" : 38, \"end_offset\" : 42, \"type\" : \"\u003cALPHANUM\u003e\", \"position\" : 7 }, { \"token\" : \"dogs\", \"start_offset\" : 43, \"end_offset\" : 47, \"type\" : \"\u003cALPHANUM\u003e\", \"position\" : 8 }, { \"token\" : \"in\", \"start_offset\" : 48, \"end_offset\" : 50, \"type\" : \"\u003cALPHANUM\u003e\", \"position\" : 9 }, { \"token\" : \"the\", \"start_offset\" : 51, \"end_offset\" : 54, \"type\" : \"\u003cALPHANUM\u003e\", \"position\" : 10 }, { \"token\" : \"summer\", \"start_offset\" : 55, \"end_offset\" : 61, \"type\" : \"\u003cALPHANUM\u003e\", \"position\" : 11 }, { \"token\" : \"evening\", \"start_offset\" : 62, \"end_offset\" : 69, \"type\" : \"\u003cALPHANUM\u003e\", \"position\" : 12 } ] } Simple Analyzer 按照非字母切分，非字母的都被去除 小写处理 组成如下： Tokenizer：LowerCase GET _analyze { \"analyzer\": \"simple\", \"text\": \"2 running Quick brown-foxes leap over lazy dogs in the summer evening.\" } { \"tokens\" : [ { \"token\" : \"running\", \"start_offset\" : 2, \"end_offset\" : 9, \"type\" : \"word\", \"position\" : 0 }, { \"token\" : \"quick\", \"start_offset\" : 10, \"end_offset\" : 15, \"type\" : \"word\", \"position\" : 1 }, { \"token\" : \"brown\", \"start_offset\" : 16, \"end_offset\" : 21, \"type\" : \"word\", \"position\" : 2 }, { \"token\" : \"foxes\", \"start_offset\" : 22, \"end_offset\" : 27, \"type\" : \"word\", \"position\" : 3 }, { \"token\" : \"leap\", \"start_offset\" : 28, \"end_offset\" : 32, \"type\" : \"word\", \"position\" : 4 }, { \"token\" : \"over\", \"start_offset\" : 33, \"end_offset\" : 37, \"type\" : \"word\", \"position\" : 5 }, { \"token\" : \"lazy\", \"start_offset\" : 38, \"end_offset\" : 42, \"type\" : \"word\", \"position\" : 6 }, { \"token\" : \"dogs\", \"start_offset\" : 43, \"end_offset\" : 47, \"type\" : \"word\", \"position\" : 7 }, { \"token\" : \"in\", \"start_offset\" : 48, \"end_offset\" : 50, \"type\" : \"word\", \"position\" : 8 }, { \"token\" : \"the\", \"start_offset\" : 51, \"end_offset\" : 54, \"type\" : \"word\", \"position\" : 9 }, { \"token\" : \"summer\", \"start_offset\" : 55, \"end_offset\" : 61, \"type\" : \"word\", \"position\" : 10 }, { \"token\" : \"evening\", \"start_offset\" : 62, \"end_offset\" : 69, \"type\" : \"word\", \"position\" : 11 } ] } Whitespace Analyzer 按照空格切分 组成如下： Tokenizer:Whitespace Stop Analyzer 相比 Simple Analyzer 多了 stop filter 会把 the\\a\\is 等修饰性词语去除 组成如下： Tokenizer:Lowe Case TokenFilter:Stop #stop GET _analyze { \"analyzer\": \"whitespace\", \"text\": \"2 running Quick brown-foxes leap over lazy dogs in the summer evening.\" } { \"tokens\" : [ { \"token\" : \"2\", \"start_offset\" : 0, \"end_offset\" : 1, \"type\" : \"word\", \"position\" : 0 }, { \"token\" : \"running\", \"start_offset\" : 2, \"end_offset\" : 9, \"type\" : \"word\", \"position\" : 1 }, { \"token\" : \"Quick\", \"start_offset\" : 10, \"end_offset\" : 15, \"type\" : \"word\", \"position\" : 2 }, { \"token\" : \"brown-foxes\", \"start_offset\" : 16, \"end_offset\" : 27, \"type\" : \"word\", \"position\" : 3 }, { \"token\" : \"leap\", \"start_offset\" : 28, \"end_offset\" : 32, \"type\" : \"word\", \"position\" : 4 }, { \"token\" : \"over\", \"start_offset\" : 33, \"end_offset\" : 37, \"type\" : \"word\", \"position\" : 5 }, { \"token\" : \"lazy\", \"start_offset\" : 38, \"end_offset\" : 42, \"type\" : \"word\", \"position\" : 6 }, { \"token\" : \"dogs\", \"start_offset\" : 43, \"end_offset\" : 47, \"type\" : \"word\", \"position\" : 7 },","date":"2022-03-05","objectID":"/posts/search_engine/es/1.html/:2:3","tags":["search_engine","note","es"],"title":"ElasticSearch倒排索引\u0026Analysis分词","uri":"/posts/search_engine/es/1.html/"},{"categories":["search engine"],"content":"参考 https://time.geekbang.org/course/intro/100030501?tab=catalog ","date":"2022-03-05","objectID":"/posts/search_engine/es/1.html/:3:0","tags":["search_engine","note","es"],"title":"ElasticSearch倒排索引\u0026Analysis分词","uri":"/posts/search_engine/es/1.html/"},{"categories":["os"],"content":" Hello OS 是基于硬件，一个最小的操作系统内核 Hello OS 的引导流程如下： BIOS 固件是固化在 PC 机主板上的 ROM 芯片中的，可掉电保存，它负责检测和初始化 CPU、内存及主板平台。 PC 机上电后的第一条指令就是 BIOS 固件中的，然后加载引导设备（大概率是硬盘）中的第一个扇区数据，到 0x7c00 地址开始的内存空间，再接着跳转到 0x7c00 处执行指令。 ","date":"2022-03-05","objectID":"/posts/os/common/1.html/:0:0","tags":["os","note","汇编"],"title":"HelloOs","uri":"/posts/os/common/1.html/"},{"categories":["os"],"content":"引导汇编部分 MBT_HDR_FLAGS EQU 0x00010003 MBT_HDR_MAGIC EQU 0x1BADB002 ;多引导协议头魔数 MBT_HDR2_MAGIC EQU 0xe85250d6 ;第二版多引导协议头魔数 global _start ;导出_start符号 extern main ;导入外部的main函数符号 [section .start.text] ;定义.start.text代码节 [bits 32] ;汇编成32位代码 _start: jmp _entry ALIGN 8 mbt_hdr: dd MBT_HDR_MAGIC dd MBT_HDR_FLAGS dd -(MBT_HDR_MAGIC+MBT_HDR_FLAGS) dd mbt_hdr dd _start dd 0 dd 0 dd _entry ;以上是GRUB所需要的头 ALIGN 8 mbt2_hdr: DD MBT_HDR2_MAGIC DD 0 DD mbt2_hdr_end - mbt2_hdr DD -(MBT_HDR2_MAGIC + 0 + (mbt2_hdr_end - mbt2_hdr)) DW 2, 0 DD 24 DD mbt2_hdr DD _start DD 0 DD 0 DW 3, 0 DD 12 DD _entry DD 0 DW 0, 0 DD 8 mbt2_hdr_end: ;以上是GRUB2所需要的头 ;包含两个头是为了同时兼容GRUB、GRUB2 上述汇编代码是用汇编定义的 GRUB 的多引导协议头，即一定格式的数据，通过实现该协议标准让 GRUB 能识别 Hello OS。 ALIGN 8 _entry: ;关中断 cli ;关不可屏蔽中断 in al, 0x70 or al, 0x80 out 0x70,al ;重新加载GDT lgdt [GDT_PTR] jmp dword 0x8 :_32bits_mode _32bits_mode: 上述汇编代码作用于关掉中断，设定 CPU 的工作模式。 ;下面初始化C语言可能会用到的寄存器 mov ax, 0x10 mov ds, ax mov ss, ax mov es, ax mov fs, ax mov gs, ax xor eax,eax xor ebx,ebx xor ecx,ecx xor edx,edx xor edi,edi xor esi,esi xor ebp,ebp xor esp,esp ;初始化栈，C语言需要栈才能工作 mov esp,0x9000 ;调用C语言函数main call main ;让CPU停止执行指令 halt_step: halt jmp halt_step 该部分汇编代码是初始化 CPU 的寄存器和 C 语言的运行环境。 GDT_START: knull_dsc: dq 0 kcode_dsc: dq 0x00cf9e000000ffff kdata_dsc: dq 0x00cf92000000ffff k16cd_dsc: dq 0x00009e000000ffff k16da_dsc: dq 0x000092000000ffff GDT_END: GDT_PTR: GDTLEN dw GDT_END-GDT_START-1 GDTBASE dd GDT_START 该部分汇编代码是服务 CPU 工作模式所需要的数据。 ","date":"2022-03-05","objectID":"/posts/os/common/1.html/:1:0","tags":["os","note","汇编"],"title":"HelloOs","uri":"/posts/os/common/1.html/"},{"categories":["os"],"content":"Hello OS 的主函数 上述汇编代码调用的 main 函数，是由下面 C 语言代码分别由 nasm 和 GCC 编译可链接模块，由 LD 连接器链接在一起，形成可执行程序文件： #include \"vgastr.h\"void main() { printf(\"Hello OS!\"); return; } ","date":"2022-03-05","objectID":"/posts/os/common/1.html/:2:0","tags":["os","note","汇编"],"title":"HelloOs","uri":"/posts/os/common/1.html/"},{"categories":["os"],"content":"控制计算机屏幕 屏幕上显示字符，就要编程操作显卡。 显卡都会支持 VESA 的标准，该标准下有两种工作模式（字符和图形），且为支持该标准还会提供 VGABIOS 的固件程序。 下图即显卡的字符模式工作细节，可以看到主要的特点是在于会每两个字节（ASCII码和字符颜色值）对应一个字符。 上述主函数中printf控制输出代码如下： void _strwrite(char* string) { char* p_strdst = (char*)(0xb8000);//指向显存的开始地址 while (*string) { *p_strdst = *string++; // 这里省略了字符颜色信息 p_strdst += 2; } return; } void printf(char* fmt, ...) { _strwrite(fmt); return; } ","date":"2022-03-05","objectID":"/posts/os/common/1.html/:3:0","tags":["os","note","汇编"],"title":"HelloOs","uri":"/posts/os/common/1.html/"},{"categories":["os"],"content":"make make 即工具程序，会读取makefile文件中的规则自动化构建软件。 makefile文件中的规则： 首先有一个或者多个构建目标称为target 目标后面紧跟着用于构建该目标所需要的文件 目标下面是构建该目标所需要的命令及参数 同时检查文件的依赖关系 demo如下： CC = gcc #定义一个宏CC 等于gcc CFLAGS = -c #定义一个宏 CFLAGS 等于-c OBJS_FILE = file.o file1.o file2.o file3.o file4.o #定义一个宏 .PHONY : all everything #定义两个伪目标all、everything all:everything #伪目标all依赖于伪目标everything everything :$(OBJS_FILE) #伪目标everything依赖于OBJS_FILE，而OBJS_FILE是宏会被 #替换成file.o file1.o file2.o file3.o file4.o %.o : %.c $(CC) $(CFLAGS) -o $@ $\u003c #后面为注释 :=或=可定义宏，引用宏时要用$(宏名)，宏最终会在宏出现的地方替换成相应的字符串 PHONY 表示定义伪目标（不代表一个真正的文件名），在执行 make 时可以指定这个目标来执行其所在规则定义的命令 伪目标可以依赖于另一个伪目标或者文件 %.o : %.c，其中%表示通配符，表示所有以“.o”结尾的文件依赖于所有以“.c”结尾的文件 这里补充下上面 GCC 编译的过程，我们可以手动控制该流程来得到相应的中间文件： gcc HelloWorld.c -E -o HelloWorld.i预处理：加入头文件，替换宏 gcc HelloWorld.c -S -c -o HelloWorld.s编译：包含预处理，将 C 程序转换成汇编程序 gcc HelloWorld.c -c -o HelloWorld.o汇编：包含预处理和编译，将汇编程序转换成可链接的二进制程序 gcc HelloWorld.c -o HelloWorld链接：包含以上所有操作，将可链接的二进制程序和其它别的库链接在一起，形成可执行的程序文件 ","date":"2022-03-05","objectID":"/posts/os/common/1.html/:4:0","tags":["os","note","汇编"],"title":"HelloOs","uri":"/posts/os/common/1.html/"},{"categories":["os"],"content":"编译 ","date":"2022-03-05","objectID":"/posts/os/common/1.html/:5:0","tags":["os","note","汇编"],"title":"HelloOs","uri":"/posts/os/common/1.html/"},{"categories":["os"],"content":"安装 当Hello OS.bin生成之后，需要让 GRUB 能够发现才能在启动时被加载，该过程称之为安装。 我们通过 GRUB 启动时的规整，即加载一个grub.cfg的文本文件，根据其中的内容执行相应的操作，其中一部分内容就是启动项。在启动时可选择不同的启动项后根据选择的启动项信息来加载 OS 文件到内存。 下面即 Hello OS 的启动项： menuentry 'HelloOS' { insmod part_msdos #GRUB加载分区模块识别分区 insmod ext2 #GRUB加载ext文件系统模块识别ext文件系统 set root='hd0,msdos4' #注意boot目录挂载的分区， multiboot2 /boot/HelloOS.bin #GRUB以multiboot2协议加载HelloOS.bin boot #GRUB启动HelloOS.bin } 上面Boot目录挂载分区可通过df /boot/命令查询，并且需要由特殊 GRUB 的命名方式。 eg:“hd0,msdos4”，代表sda4就是硬盘的第四个分区（硬件分区选择 MBR），hd0 表示第一块硬盘，结合起来就是第一块硬盘的第四个分区。 把上面启动项的代码追加到 Linux 机器上的/boot/grub/grub.cfg文件末尾，并且把上面生成的Hello OS.bin文件复制到/boot/目录下。 需要注意的是，Ubuntu或者其他操作系统进入 GRUB 引导界面可能也需要进行设置，比如 Ubuntu 默认就是关闭和隐藏的，需要修改/etc/default/grub配置才能够看到。 最后重启计算机，你就可以看到 Hello OS 的启动选项了。 ","date":"2022-03-05","objectID":"/posts/os/common/1.html/:6:0","tags":["os","note","汇编"],"title":"HelloOs","uri":"/posts/os/common/1.html/"},{"categories":["os"],"content":"参考 https://time.geekbang.org/column/intro/100078401?tab=catalog ","date":"2022-03-05","objectID":"/posts/os/common/1.html/:7:0","tags":["os","note","汇编"],"title":"HelloOs","uri":"/posts/os/common/1.html/"},{"categories":["search engine"],"content":"文档（Document） Elasticsearch 是面向文档的，文档是所有可搜索数据的最小单位 文档会被序列化成 JSON 格式，保存在 Elasticsearch 中 JSON 对象由字段构成 每个字段都有对应的字段类型（字符串/数值/布尔值/日期/二进制/范围类型） 每个文档都有一个 UniqueID（可指定也可自动生成） ","date":"2022-03-03","objectID":"/posts/search_engine/es/1.html/:1:0","tags":["search_engine","note","es"],"title":"ElasticSearch基本概念","uri":"/posts/search_engine/es/1.html/"},{"categories":["search engine"],"content":"JSON 文档 一篇文档包含了一系列字段。类似数据库表中一条记录 JSON 文档，格式灵活，不需要预先定义格式 字段的类型可以指定或者由 Elasticsearch自动推算得到 支持数组/支持嵌套 ","date":"2022-03-03","objectID":"/posts/search_engine/es/1.html/:1:1","tags":["search_engine","note","es"],"title":"ElasticSearch基本概念","uri":"/posts/search_engine/es/1.html/"},{"categories":["search engine"],"content":"文档的元数据 元数据，用于标注文档的相关信息： _index：文档所属的索引名 _type：文档所属的类型名 _id：文档唯一ID _source：文档原始的 JSON 数据 _version：文档的版本信息 _score：相关性打分（搜索查询时） 在7.0之前，一个 Index 可以设置多个 Type。 ","date":"2022-03-03","objectID":"/posts/search_engine/es/1.html/:1:2","tags":["search_engine","note","es"],"title":"ElasticSearch基本概念","uri":"/posts/search_engine/es/1.html/"},{"categories":["search engine"],"content":"索引 index（索引）是文档的容器，是一类文档的集合： index体现逻辑空间概念：每个索引都有自己的 Mapping 定义，用于定义包含文档的字段名和字段类型 Shard体现物理空间概念：索引中的数据分散在 Shard 上 索引中的 Mapping 和 Setting： 前者定义文档字段的类型 后者定义不同的数据分布 索引在不同场景语意可能不同，比如除上面提到的索引语义外，当保存一个文档到 Elasticsearch 的过程也可以叫索引（动词）。 ","date":"2022-03-03","objectID":"/posts/search_engine/es/1.html/:2:0","tags":["search_engine","note","es"],"title":"ElasticSearch基本概念","uri":"/posts/search_engine/es/1.html/"},{"categories":["search engine"],"content":"与 RDBMS 比较 RDBMS Elasticsearch Table Index(Type) Row Document Column Filed Schema Mapping SQL DSL 区别主要在于： RDBMS：事务性/Join Elasticsearch：Schemaless/相关性/高性能全文检索 ","date":"2022-03-03","objectID":"/posts/search_engine/es/1.html/:2:1","tags":["search_engine","note","es"],"title":"ElasticSearch基本概念","uri":"/posts/search_engine/es/1.html/"},{"categories":["search engine"],"content":"REST API 下面是一些常用指令： #查看索引相关信息 GET movies #查看索引的文档总数 GET movies/_count #查看前10条文档，了解文档格式 POST movies/_search { } #_cat indices API #查看indices GET /_cat/indices/kibana*?v\u0026s=index #查看状态为绿的索引 GET /_cat/indices?v\u0026health=green #按照文档个数排序 GET /_cat/indices?v\u0026s=docs.count:desc #查看具体的字段 GET /_cat/indices/kibana*?pri\u0026v\u0026h=health,index,pri,rep,docs.count,mt #How much memory is used per index? GET /_cat/indices?v\u0026h=i,tm\u0026s=tm:desc ","date":"2022-03-03","objectID":"/posts/search_engine/es/1.html/:3:0","tags":["search_engine","note","es"],"title":"ElasticSearch基本概念","uri":"/posts/search_engine/es/1.html/"},{"categories":["search engine"],"content":"分布式架构 常见的分布式架构一般会带来以下好处： 高可用性： 服务可用性：允许有节点停止服务 数据可用性：部分节点丢失，不会丢失数据 可扩展性： 请求量提升或数据的不断增长（将数据分布到所有节点上） Elasticsearch 的分布式架构可以做到： 存储的水平扩容 提高系统可用性，部分节点停止服务，整个集群的服务不受影响 涉及到 Elasticsearch 配置中： 不同的集群通过不同的名字区分，默认为elasticsearch 通过配置文件修改，或者在命令行中-E cluster.name=catwithtudou进行设定 一个集群可以有一个或多个节点 ","date":"2022-03-03","objectID":"/posts/search_engine/es/1.html/:4:0","tags":["search_engine","note","es"],"title":"ElasticSearch基本概念","uri":"/posts/search_engine/es/1.html/"},{"categories":["search engine"],"content":"节点 节点即 Elasticsearch 的一个实例： 本质上就是一个 JAVA 进程 一台机器上可以运行多个实例，但生产环境一般建议一台机器只运行一个 Elasticsearch 实例 涉及到相关配置中： 每个节点都有名字，可通过配置文件配置，或启动时-E node.name=node1指定 节点启动之后，会分配一个 UID，保存在 data 目录下 Master-eligible nodes\u0026Master Node 每个节点在启动后，默认为一个 Master eligible 节点 可以设置node.master:false禁止 Master-eligible 节点可以参加选主流程，成为 Master 节点 当第一个节点启动的时候，默认将自己选举成 Master 节点 每个节点上都保存了集群的状态，只有 Master 节点才能修改集群的状态信息，其中集群状态（Cluster State），维护集群中的必要信息： 所有的节点信息 所有的索引和其相关的 Mapping 与 Settings 信息 分片的路由信息 Data Node\u0026Coordinating Node Data Node：可以保存数据的节点。负责保存分片数据。在数据扩展上起到重要作用 Coordinating Node：负责接收 Client 的请求，将请求分发到合适的节点，最终把结果汇集到一起，每个节点默认都起到了该Node类型职责 Other Node Type Hot\u0026Warm Node：不同硬件配置的 Data Node，用来实现 Hot\u0026Warm 架构，降级集群部署的成本 Maching Learning Node：负责跑机器学习的 Job，用来做异常检测 Tribe Node：连接到不同的集群，并且支持将这些集群当成一个单独的集群处理 配置节点类型 开发环境中一个节点可以承担多种角色 生产环境中，应该设置单一的角色的节点（dedicated node） 节点类型 配置参数 默认值 master eligible node.master true data node.data true ingest node.ingest true coordinating only 无 每个节点默认是 machine learning node.ml true（需要 enable x-pack） ","date":"2022-03-03","objectID":"/posts/search_engine/es/1.html/:4:1","tags":["search_engine","note","es"],"title":"ElasticSearch基本概念","uri":"/posts/search_engine/es/1.html/"},{"categories":["search engine"],"content":"分片 分片主要分为主分片和副本： Primary Shard：用以解决数据水平扩展的问题。通过主分片，可以将数据分布到集群内的所有节点上 一个分片是一个运行的 Lucene 的实例 主分片数在索引创建时指定，后续不允许修改，除非Reindex Replica Shard：用以解决数据高可用的问题。分片是主分片的拷贝： 副本分片数，可以动态调整 增加副本数，还可以一定程度上提高服务的可用性 分片的设定 对于生产环境中分片的设定，需要提前规划好容量： 分片数设置过小： 导致后续无法增加节点实现水平扩展 单个分片的数据量太大，导致数据分配耗时高 分片数设置过大： 影响搜索结果的相关性打分，影响统计结果的准确性 单个节点上过多的分片，会导致资源浪费，同时也会消耗性能 7.0版本开始，默认主分片设置为 1，解决了 over-sharding 的问题。 ","date":"2022-03-03","objectID":"/posts/search_engine/es/1.html/:4:2","tags":["search_engine","note","es"],"title":"ElasticSearch基本概念","uri":"/posts/search_engine/es/1.html/"},{"categories":["search engine"],"content":"查看集群的健康状况 使用GET _cluster/health可查询集群的健康状况： Green：主分片与副本都分配正常 Yellow：主分片分配正常，副本分配不正常 Red：有主分片未分配（如当服务器的磁盘容量超过85%时，创建了一个新的索引） 一般有以下常用指令： GET _cat/nodes?v GET /_nodes/es7_01,es7_02 GET /_cat/nodes?v GET /_cat/nodes?v\u0026h=id,ip,port,v,m GET _cluster/health GET _cluster/health?level=shards GET /_cluster/health/kibana_sample_data_ecommerce GET /_cluster/health/kibana_sample_data_ecommerce?level=shards #### cluster state GET /_cluster/state #cluster get settings GET /_cluster/settings GET /_cluster/settings?include_defaults=true GET _cat/shards GET _cat/shards?h=index,shard,prirep,state,unassigned.reason ","date":"2022-03-03","objectID":"/posts/search_engine/es/1.html/:4:3","tags":["search_engine","note","es"],"title":"ElasticSearch基本概念","uri":"/posts/search_engine/es/1.html/"},{"categories":["search engine"],"content":"参考 https://time.geekbang.org/course/intro/100030501?tab=catalog ","date":"2022-03-03","objectID":"/posts/search_engine/es/1.html/:5:0","tags":["search_engine","note","es"],"title":"ElasticSearch基本概念","uri":"/posts/search_engine/es/1.html/"},{"categories":["search engine"],"content":"ElasticSearch 简介 开源分布式搜索分析引擎 近实时（Near Real Time） 分布式存储/搜索/分析引擎 Elastic Search 起源是基于 Lucene 。Lucene 是基于 Java 开发的搜索引擎类库，具有高性能、易扩展的优点，它的局限性主要在于以下方面： 只能基于 Java 语言开发 类库的学习曲线陡峭 原生并不支持水平扩展 ","date":"2022-02-28","objectID":"/posts/search_engine/es/0.html/:1:0","tags":["search_engine","note","es"],"title":"ElasticSearch初识","uri":"/posts/search_engine/es/0.html/"},{"categories":["search engine"],"content":"ElasticSearch 的分布式架构 集群规模可以从单个节点扩展至数百节点 高可用且支持水命扩展（服务和数据两个维度） 支持不同的节点类型（支持 Hot 和 Waram 架构） ","date":"2022-02-28","objectID":"/posts/search_engine/es/0.html/:1:1","tags":["search_engine","note","es"],"title":"ElasticSearch初识","uri":"/posts/search_engine/es/0.html/"},{"categories":["search engine"],"content":"支持多种方式接入 多种编程语言类库 RESTful API JDBC\u0026ODBC ","date":"2022-02-28","objectID":"/posts/search_engine/es/0.html/:1:2","tags":["search_engine","note","es"],"title":"ElasticSearch初识","uri":"/posts/search_engine/es/0.html/"},{"categories":["search engine"],"content":"主要功能 海量数据的分布式存储以及集群管理 服务和数据的高可用，水平扩展 近实时搜索，性能卓越 结构化/全文/地理位置/自动完成 海量数据的近实时分析 聚合功能 ","date":"2022-02-28","objectID":"/posts/search_engine/es/0.html/:1:3","tags":["search_engine","note","es"],"title":"ElasticSearch初识","uri":"/posts/search_engine/es/0.html/"},{"categories":["search engine"],"content":"ElasticStack 生态圈 Logstash 开源的服务器端数据处理管道，支持从不同来源采集数据，转换数据，并将数据发送到不同的存储库中。且有以下特性： 实时解析和转换数据（如从IP地址破译出地理坐标和PII数据匿名化等） 可扩展（200多个插件包含日志、数据库等） 可靠性安全性（持久化队列和数据传输加密等） 监控 Kibana 数据可视化工具。 BEATS 使用GO语言开发的轻量数据采集器。 X-Pack 商业化套件。可主要应用于： Machine Learning Alerting 图表功能 SQL 的 JDBC 和 ODBC 连接性 ","date":"2022-02-28","objectID":"/posts/search_engine/es/0.html/:2:0","tags":["search_engine","note","es"],"title":"ElasticSearch初识","uri":"/posts/search_engine/es/0.html/"},{"categories":["search engine"],"content":"ELK 应用场景 主要包括： 网站搜索/垂直搜索/代码搜索 日志管理与分析/安全指标监控/应用性能指标监控/WEB抓取舆情分 总结下来就是搜索和分析。 ","date":"2022-02-28","objectID":"/posts/search_engine/es/0.html/:3:0","tags":["search_engine","note","es"],"title":"ElasticSearch初识","uri":"/posts/search_engine/es/0.html/"},{"categories":["search engine"],"content":"常见架构接入场景 ","date":"2022-02-28","objectID":"/posts/search_engine/es/0.html/:4:0","tags":["search_engine","note","es"],"title":"ElasticSearch初识","uri":"/posts/search_engine/es/0.html/"},{"categories":["search engine"],"content":"ElasticSearch 与数据库的集成 单独使用 ElasticSearch 存储 以下情况可以考虑与数据库集成： 与现有系统的集成 需要考虑事务性 数据更新频繁 ","date":"2022-02-28","objectID":"/posts/search_engine/es/0.html/:4:1","tags":["search_engine","note","es"],"title":"ElasticSearch初识","uri":"/posts/search_engine/es/0.html/"},{"categories":["search engine"],"content":"指标分析/日志分析 ","date":"2022-02-28","objectID":"/posts/search_engine/es/0.html/:4:2","tags":["search_engine","note","es"],"title":"ElasticSearch初识","uri":"/posts/search_engine/es/0.html/"},{"categories":["search engine"],"content":"安装部署 ","date":"2022-02-28","objectID":"/posts/search_engine/es/0.html/:5:0","tags":["search_engine","note","es"],"title":"ElasticSearch初识","uri":"/posts/search_engine/es/0.html/"},{"categories":["search engine"],"content":"ElasticSearch 安装\u0026简单配置 ElasticSearch7.0版本及之后安装时已经内置 Java 环境，所以不需要额外准备 Java 编译环境。 这里以 windows 系统进行安装部署。 直接在官网上下载安装包 官网下载链接 安装及运行过程该页面都有相应提示： ElasticSearch 安装目录： Bin：脚本文件 config：集群配置文件，user，role based 相关配置 JDK：Java运行环境 data（path.data）：数据文件 lb：Java 类库 logs（path.log）：日志文件 modules：包含所有 ES 模块 plugins：包含所有已安装插件 配置建议 修改 JVM - config/jvm.options 7.1 下载的默认配置是1GB Xmx 和 Xms 设置成一样 Xmx 不要超过机器内存的50% 不要超过30GB 启动ElasticSearch 运行程序如下： 这里可以将 ElasticSearch 下的 bin 目录放入环境变量，方便后续使用。 这里需要注意的是，这里下载的是 8.0 的版本，第一次启动的时候默认是开启了x-pack安全鉴权和https等配置，所以在我们访问的是时候都是HTTPS进行访问，且需要输入账号密码进行登录，默认用户名为elastic，密码是第一次启动的时候自动生成的。 验证ElasticSearch是否启动成功 这里我们需要访问的是： https://localhost:9200 出现上述页面就说明启动成功了。 安装并查看ElasticSearch插件 我们可以通过elasticsearch-plugin来进行插件的安装，如下图所示： 安装成功后，可以通过https://localhost:9200/_cat/plugins查看： ","date":"2022-02-28","objectID":"/posts/search_engine/es/0.html/:5:1","tags":["search_engine","note","es"],"title":"ElasticSearch初识","uri":"/posts/search_engine/es/0.html/"},{"categories":["search engine"],"content":"第一章 搜索引擎及其技术架构 ","date":"2022-02-26","objectID":"/posts/search_engine/common/0.html/:1:0","tags":["search_engine","note"],"title":"（WIP）《这就是搜索引擎》阅读笔记","uri":"/posts/search_engine/common/0.html/"},{"categories":["search engine"],"content":"搜索引擎为何重要 搜索是目前解决信息过载的相对有效方式，在没有更有效的替代解决方式出来之前，搜索引擎作为互联网网站和应用的入口及处于行业制高点的重要地位只会逐步加强。 ","date":"2022-02-26","objectID":"/posts/search_engine/common/0.html/:1:1","tags":["search_engine","note"],"title":"（WIP）《这就是搜索引擎》阅读笔记","uri":"/posts/search_engine/common/0.html/"},{"categories":["search engine"],"content":"搜索引擎发展史 分类目录搜索引擎：人工分类 第一代文本检索搜索引擎：简单检索 第二代引入链接分析技术：改善搜索结果质量 第三代以用户为中心：尝试用户个性化需求 ","date":"2022-02-26","objectID":"/posts/search_engine/common/0.html/:1:2","tags":["search_engine","note"],"title":"（WIP）《这就是搜索引擎》阅读笔记","uri":"/posts/search_engine/common/0.html/"},{"categories":["search engine"],"content":"搜索引擎的3个目标 更全（如提高网络爬虫技术） 更快（如索引、缓存等相关技术） 更准（如排序技术、链接分析技术等） ","date":"2022-02-26","objectID":"/posts/search_engine/common/0.html/:1:3","tags":["search_engine","note"],"title":"（WIP）《这就是搜索引擎》阅读笔记","uri":"/posts/search_engine/common/0.html/"},{"categories":["search engine"],"content":"搜索引擎的3个核心问题 用户真正的需求是什么 哪些信息是和用户需求真正相关的 哪些信息是用户可以信赖的 ","date":"2022-02-26","objectID":"/posts/search_engine/common/0.html/:1:4","tags":["search_engine","note"],"title":"（WIP）《这就是搜索引擎》阅读笔记","uri":"/posts/search_engine/common/0.html/"},{"categories":["search engine"],"content":"搜索引擎的技术架构 ","date":"2022-02-26","objectID":"/posts/search_engine/common/0.html/:1:5","tags":["search_engine","note"],"title":"（WIP）《这就是搜索引擎》阅读笔记","uri":"/posts/search_engine/common/0.html/"},{"categories":["search engine"],"content":"第二章 网络爬虫 ","date":"2022-02-26","objectID":"/posts/search_engine/common/0.html/:2:0","tags":["search_engine","note"],"title":"（WIP）《这就是搜索引擎》阅读笔记","uri":"/posts/search_engine/common/0.html/"},{"categories":["search engine"],"content":"通用爬虫框架 ","date":"2022-02-26","objectID":"/posts/search_engine/common/0.html/:2:1","tags":["search_engine","note"],"title":"（WIP）《这就是搜索引擎》阅读笔记","uri":"/posts/search_engine/common/0.html/"},{"categories":["search engine"],"content":"优秀爬虫的特性 高性能 可扩展性 健壮性 友好性 ","date":"2022-02-26","objectID":"/posts/search_engine/common/0.html/:2:2","tags":["search_engine","note"],"title":"（WIP）《这就是搜索引擎》阅读笔记","uri":"/posts/search_engine/common/0.html/"},{"categories":["search engine"],"content":"爬虫质量的评价标准 抓取网页覆盖率 抓取网页时新性 抓取网页重要性 通盘考虑以上3个因素，可以将目前爬虫研发的目标简单描述如下：在资源有限的情况下，既然搜索引擎只能抓取互联网现存网页的一部分，那么就尽可能选择比较重要的那部分页面来索引；对于已经抓取到的网页，尽可能快地更新其内容，使得索引网页和互联网对应页面内容同步更新；在此基础上，尽可能扩大抓取范围，抓取到更多以前无法发现的网页。 ","date":"2022-02-26","objectID":"/posts/search_engine/common/0.html/:2:3","tags":["search_engine","note"],"title":"（WIP）《这就是搜索引擎》阅读笔记","uri":"/posts/search_engine/common/0.html/"},{"categories":["search engine"],"content":"抓取策略 爬虫的抓取策略有很多种，但不论方法如何，其基本目标一致：优先选择重要网页进行抓取。而其网页的重要性可以选择不同的方法进行评判（大部分按网页的流行性来定义）： 宽度优先遍历策略（Breath First） 非完全PageRank策略 OCIP策略 大战优先策略 宽度优先遍历策略 该策略的核心思想是将新下载网页包含的链接直接追加到待抓取URL队列末尾。 实验表明这种策略效果很好，虽然看似机械，但实际上的网页抓取顺序基本是按照网页的重要性排序的。原因在于实际上宽度优先遍历策略隐含了一些网页优先级假设. 非完全PageRank策略（Partial PageRank） PageRank是一种著名的链接分析算法，可以用来衡量网页的重要性。 该策略基本思路为对于已经下载的网页，加上待抓取URL队列中的URL一起，形成网页集合，在此集合内进行PageRank计算，计算完成后，将待抓取URL队列里的网页按照PageRank得分由高到低排序，形成的序列就是爬虫接下来应该依次抓取的URL列表。 考虑到非完全PageRank赋予这些新抽取出来但是又没有PageRank值的网页，可以通过一个临时PageRank值，将这个网页的所有入链传导的PageRank值汇总，作为临时PageRank值，若该值比目前队列中已计算的PageRank高的话那么优先下载这个URL。 OCIP策略（Online Page Importance Computation） OCIP的字面含义是“在线页面重要性计算”，可以将其看做是一种改进的PageRank算法。 该策略基本思路为假设为每个网页都给予相同的“现金”，每当下载了某个页面P后，P将自己拥有的“现金”平均分配给页面中包含的链接页面。而对于待抓取URL队列的网页中根据其手头拥有的现金金额多少排序，优先下载现金最充裕的网页。 与非完全PageRank策略思路基本一致，区别在于： 该策略不需要迭代过程，增加了计算速度，更利于实时计算使用 PageRank在计算时，存在向无链接关系网页的远程跳转过程，而OCIP没有这一计算因子； 实验结果表明，OCIP是种较好的重要性衡量策略，效果略优于宽度优先遍历策略。 大站优先策略（Larger Sites First） 该策略基本思路为以网站为单位来衡量网页重要性，对于待抓取URL队列中的网页，根据所属网站归类，如果哪个网站等待下载的页面最多，则优先下载这些链接。 即倾向于优先下载大型网站。 这个思路虽然简单，但是有一定依据。实验表明这个算法效果也要略优于宽度优先遍历策略。 ","date":"2022-02-26","objectID":"/posts/search_engine/common/0.html/:2:4","tags":["search_engine","note"],"title":"（WIP）《这就是搜索引擎》阅读笔记","uri":"/posts/search_engine/common/0.html/"},{"categories":["search engine"],"content":"网页更新策略 为了更优的用户体验，对于爬虫已经抓取过的页面，还需要负责保持其内容和互联网页面内容的同步，这就取决于爬虫所采用的网页更新策略。 该策略核心点就在于重新抓取之前已经下载过的网页的时机，且尽可能使得本地下载网页和互联网原始页面内容保持一致。一般策略如下： 历史参考策略 用户体验策略 聚类抽样策略 历史参考策略 该策略主要建立在以下前提： 过去频繁更新的网页，那么将来也会频繁更新。 所以预估某个网页何时进行更新可参考其历史更新情况。 这里预估就需要建模，用模型来预测发生变化的时机，如泊松过程。 用户体验策略 该策略判断一个网页更新时机取决于这个网页的内容变化所带来搜索质量的变化（往往采用搜索结果排名的变化来衡量），影响越大的网页，则应该越快更新。 用户体验策略保存网页的多个历史版本，并根据过去每次内容变化对搜索质量的影响，得出一个平均值，以此作为判断爬虫重抓该网页时机的参考依据，对于影响越厉害的网页，则越优先调度重新抓取。 聚类抽样策略 该策略就是为了解决严重依赖历史更新信息的局限性。认为网页具有一些属性，根据这些属性可以预测其更新周期，具有相似属性的网页其更新周期也是类似的。 根据这些属性将网页归类，同一类别内的网页具有相同的更新频率。为了计算某个类别的更新周期，只需对类别内网页进行采样，以这些被采样网页的更新周期作为类别内所有其他网页的更新周期。 相关实验表明，聚类抽样策略效果好于前述两种更新策略，但是对以亿计的网页进行聚类，其难度也是非常巨大的。 ","date":"2022-02-26","objectID":"/posts/search_engine/common/0.html/:2:5","tags":["search_engine","note"],"title":"（WIP）《这就是搜索引擎》阅读笔记","uri":"/posts/search_engine/common/0.html/"},{"categories":["search engine"],"content":"暗网抓取 所谓暗网，是指目前搜索引擎爬虫按照常规方式很难抓取到的互联网页面。为了能够对暗网数据进行索引，需要研发与常规爬虫机制不同的系统，这类爬虫被称做暗网爬虫。 暗网爬虫的目的是将暗网数据从数据库中挖掘出来，并将其加入搜索引擎的索引，增加其搜索时的信息覆盖程度。 其技术挑战点一般有两点： 查询组合问题。若组合太多既会对访问网站造成过大压力，也可能会有重复数据的问题。 查询文本框问题。即有些需要我们填写内容在此网页搜索才能得到内容。 查询组合问题 在查询组合问题中，Google对此提出了较为经典的富含信息查询模板（Informative Query Templates）技术。 该技术定义如下：对于某个固定的查询模板来说，如果给模板内每个属性都赋值，形成不同的查询组合，提交给垂直搜索引擎，观察所有返回页面的内容，如果相互之间内容差异较大，则这个查询模板就是富含信息查询模板。 为了进一步减少提交的查询数目，Google的技术方案使用了ISIT算法。该算法基本思路为从一维开始，若一维模板含有富含信息查询模板，若是的话则扩展到二维，如此类推逐步增加维数，直到再也无法找到富含信息查询模板为止。 Google的评测结果证明，这种方法和完全组合方式比，能够大幅度提升系统效率。在数据挖掘领域中此算法和数据挖掘里经典的Apriori规则挖掘算法其核心思想相同。 查询文本框问题 在此例中，我们可以通过人工观察网站进行定位，提供一个与网站内容相关的初始种子查询关键词表，以此作为爬虫能够继续工作的基础条件。 爬虫根据初始种子词表，向垂直搜索引擎提交查询，并下载返回的结果页面。之后从返回结果页面里自动挖掘出相关的关键词，并形成一个新的查询列表，依次将新挖掘出的查询提交给搜索引擎。如此往复，直到无法下载到新的内容为止。 通过这种人工启发结合递归迭代得到种子词表的方式，尽可能覆盖数据库里的记录。 ","date":"2022-02-26","objectID":"/posts/search_engine/common/0.html/:2:6","tags":["search_engine","note"],"title":"（WIP）《这就是搜索引擎》阅读笔记","uri":"/posts/search_engine/common/0.html/"},{"categories":["search engine"],"content":"分布式爬虫 采取分布式架构的目的在于提升海量数据抓取的效率。 大型分布式爬虫的3个层级：分布式数据中心、分布式抓取服务器及分布式爬虫程序。 对于同一数据中心的多台抓取服务器，不同机器之间的分工协同方式会有差异，常见的分布式架构有两种： 主从式分布爬虫 对等式分布爬虫 Google在早期即采用此种主从分布式爬虫，在这种架构中，因为URL服务器承担很多管理任务，同时待抓取URL队列数量巨大，所以URL服务器容易成为整个系统的瓶颈。 为了解决哈希取模的对等式分布爬虫存在的问题，UbiCrawler爬虫提出了改进方案，即放弃哈希取模方式，转而采用一致性哈希方法（Consisting Hash）来确定服务器的任务分工。 ","date":"2022-02-26","objectID":"/posts/search_engine/common/0.html/:2:7","tags":["search_engine","note"],"title":"（WIP）《这就是搜索引擎》阅读笔记","uri":"/posts/search_engine/common/0.html/"},{"categories":["search engine"],"content":"第三章 搜索引擎索引 ","date":"2022-02-26","objectID":"/posts/search_engine/common/0.html/:3:0","tags":["search_engine","note"],"title":"（WIP）《这就是搜索引擎》阅读笔记","uri":"/posts/search_engine/common/0.html/"},{"categories":["search engine"],"content":"3.1 索引基础 单词-文档矩阵 搜索引擎的索引其实就是实现单词—文档矩阵的具体数据结构。可以有不同的方式来实现上述概念模型，比如倒排索引、签名文件、后缀树等方式。其中倒排索引是文档映射关系的最佳实现方式。 倒排索引 构建倒排索引时的主要关注点： 分词系统 索引记载信息 ","date":"2022-02-26","objectID":"/posts/search_engine/common/0.html/:3:1","tags":["search_engine","note"],"title":"（WIP）《这就是搜索引擎》阅读笔记","uri":"/posts/search_engine/common/0.html/"},{"categories":["search engine"],"content":"3.2 单词词典 构造和查询单词词典所使用的数据结构一般有： 哈希加链表结构 树形词典结构 ","date":"2022-02-26","objectID":"/posts/search_engine/common/0.html/:3:2","tags":["search_engine","note"],"title":"（WIP）《这就是搜索引擎》阅读笔记","uri":"/posts/search_engine/common/0.html/"},{"categories":["search engine"],"content":"3.3 倒排列表 为了更好对数据进行压缩，在实际的搜索引擎系统中，并不存储倒排索引项中的实际文档编号，而是以文档编号差值（D-Gap）。 ","date":"2022-02-26","objectID":"/posts/search_engine/common/0.html/:3:3","tags":["search_engine","note"],"title":"（WIP）《这就是搜索引擎》阅读笔记","uri":"/posts/search_engine/common/0.html/"},{"categories":["search engine"],"content":"3.4 建立索引 两遍文档遍历法（2-Pass In-Memory Inversion） 此方法完全是在内存里完成索引的创建过程的。 第一遍扫描的主要目的是资源准备工作（如获得统计信息、以此分配内存等资源、建立内存中单词文档位置信息等）； 第二遍扫描真正建立每个单词的倒排列表信息，填充分配好的内存空间； 两遍扫描后将内存的倒排列表和词典信息写入磁盘； 缺点在于： 对内存有要求； 速度性能较差（主要因为两次遍历中磁盘读取消耗较大）； 排序法（Sort-based Inversion） 该方法在建立索引的过程中，始终在内存中分配固定大小的空间，用来存放词典信息和索引的中间结果，若内存空间不足则将中间结果排序处理后写入磁盘，同时清理内存空间。最终将磁盘写入的中间结果进行排序合并得到最终的索引。 归并法（Merge-based Inversion） 归并法是在排序法的基础上进行优化，改进点主要在将内存中维护的词典信息等所有中间结果也写入磁盘，利用后续内存空间使用。 虽然流程与排序法大致相同，但是实现方式差异较大： 存放在内存的中间结果，归并法是建议了完整的索引结构（完整倒排索引），而排序法则只是构建的并不完整； 中间结果写入磁盘临时文件，归并法是将完整内存倒排索引以词典项在前倒排列表在后的规律写入临时文件，直至内存清空，而排序法并没有将词典映射表写入； ","date":"2022-02-26","objectID":"/posts/search_engine/common/0.html/:3:4","tags":["search_engine","note"],"title":"（WIP）《这就是搜索引擎》阅读笔记","uri":"/posts/search_engine/common/0.html/"},{"categories":["search engine"],"content":"3.5 动态索引 这种动态索引中，有3个关键的索引结构： 倒排索引 临时索引 已删除文档列表 对于更新的处理可以认为是先删后增。 ","date":"2022-02-26","objectID":"/posts/search_engine/common/0.html/:3:5","tags":["search_engine","note"],"title":"（WIP）《这就是搜索引擎》阅读笔记","uri":"/posts/search_engine/common/0.html/"},{"categories":["search engine"],"content":"3.6 索引更新策略 动态索引通过在内存中维护临时索引，可以实现对动态文档和实时搜索的支持。但是服内存总是有限的，当内存被消耗完需要考虑将临时索引更新到磁盘索引中，释放内存空间，此时就涉及到了索引的更新策略。 常见的索引更新策略如下： 完全重建策略 再合并策略 原地更新策略 混合策略 完全重建策略（Complete Re-Build） 该策略的基本思路为直接将新老文档合并后重新建立新索引，完成后丢弃老索引。 比较适合小文档集合，因为完全重建索引的代价较高。 再合并策略（Re-Merge） 此策略就是把将写入磁盘的临时索引和老文档的倒排索引进行合并生成新的索引。 需要说明的是合并时老倒排文件和增量索引中的词典都已排序过，这样可以顺序读取文件内容，减少磁盘寻道时间，这是其高效的根本原因。而该策略缺点在于老索引中很多单词没有变化也需要重新进行写入构建新索引，造成不必要的磁盘消耗。 原地更新策略（In-Place） 该策略出发点就是解决再合并策略中的局限： 只对倒排列表变化的单词进行处理，而不需要对未发生变化的老索引进行处理； 即使老索引的倒排列表更新直接在末尾进行追加操作，而不需要读取原先的倒排列表并重写到磁盘； 所以该策略的主要优化在于： 在索引合并时直接在原先老的索引文件里进行追加操作，将增量索引里单词的倒排列表项追加到老索引相应位置的末尾； 为了支持上面的追加操作，在初始建立的索引时为每个单词的倒排列表末尾预留磁盘空间； 若预留空间不足，就需要进行扩容“迁移”即寻找到更大的磁盘空间后将老索引文件重新写入，然后进行追加操作。 但是实验数据证明其索引更新效率比再合并策略低，原因如下： 扩容“迁移”操作较为频繁，且需要对磁盘可用空间进行维护和管理的成本较高； “迁移”操作破坏了索引文件中的单词连续性 （降低读取速度） ，若在索引文件合并时顺序读取则需要维护单词及其倒排文件的映射表 （映射表维护管理成本较高）； 混合策略（Hybrid） 顾名思义即希望能够结合不同索引更新策略。 常见的做法如下： 出现较为频繁的单词倒排列表较长即长倒排列表单词，这里就采取原地更新策略，解决磁盘读写开销的瓶颈； 出现较少的单词倒排列表就较短即短倒排列表单词，这里采取再合并策略，解决顺序读写的瓶颈； ","date":"2022-02-26","objectID":"/posts/search_engine/common/0.html/:3:6","tags":["search_engine","note"],"title":"（WIP）《这就是搜索引擎》阅读笔记","uri":"/posts/search_engine/common/0.html/"},{"categories":["search engine"],"content":"3.7 查询处理 目前有两种常见的查询处理机制: 一次一文档方式; 一次一单词方式； 跳跃指针方式（优化）； 一次一文档（Doc at a Time） 以倒排列表中包含的文档为单位，每次将其中某个文档与查询的最终相似性得分计算完毕，然后开始计算另外一个文档的最终得分，直到所有文档的得分都计算完毕为止。 搜索系统的输出结果往往是限定个数的，在实际实现一次一文档方式时，不必保存所有文档的相关性得分，而只需要在内存中维护一个大小为K的优先级别队列（根堆数据结构）。 一次一单词（Term at a Time） 一次一文档可以直观理解为在单词—文档矩阵中，以文档为单位，纵向进行分数累计，之后移动到后续文档接着计算，即计算过程是“先纵向再横向”；而一次一单词则是采取“先横向再纵向”的方式。 跳跃指针（Skip Pointers） 如果用户输入的查询包含多个查询词，搜索引擎一般默认是采取与逻辑来判别文档是否满足要求。 对于多词查询，找到包含所有查询词的文档，等价于求查询词对应的倒排列表的交集。（一次一文档）但在求其交集时，若倒排列表的文档ID以文档编号差值（D-Gap）形式存储，且差值是以压缩后的方式进行编码的，则会变得复杂（先读入内存后进行压缩恢复到差值形式，然后恢复到有序列表，最后进行交集运算）。而跳跃指针就是为了优化该计算过程。 其基本思想是将一个倒排列表数据化整为零，切分为若干个固定大小的数据块，一个数据块作为一组，对于每个数据块，增加元信息来记录关于这个块的一些信息，这样即使是面对压缩后的倒排列表的优势也有： 只解压缩部分数据即可； 无须比较任意两个文档ID； 在实际应用中，如何设定数据块或者数据组的大小对于效率有影响，实践表明一个简单有效的启发规则是：假设倒排列表长度为L（即包含L个文档ID），使用L作为块大小，则效果较好。 ","date":"2022-02-26","objectID":"/posts/search_engine/common/0.html/:3:7","tags":["search_engine","note"],"title":"（WIP）《这就是搜索引擎》阅读笔记","uri":"/posts/search_engine/common/0.html/"},{"categories":["search engine"],"content":"3.8 多字段索引 区分不同字段对于搜索引擎的相关性评分也有很大帮助。 搜索引擎需要能够对多字段进行索引，去其实现多字段索引常见方式有以下： 多索引方式； 倒排列表方式； 扩展列表方式； 多索引方式 当用户没有指定特定字段时，搜索引擎会对所有字段都进行查找并合并多个字段的相关性得分，对于多索引方式来说，就需要对多个索引进行读取，所以这种方式的效率会比较低。 倒排列表方式 将字段信息存储在某个关键词对应的倒排列表内，实现在读到关键词时可以根据倒排列表中的字段信息直接进行过滤和并保留指定字段内出现过搜索词的文档作为搜索结果返回。 扩展列表方式 为每个字段建立一个列表，记载每个文档这个字段对应的出现位置信息。 ","date":"2022-02-26","objectID":"/posts/search_engine/common/0.html/:3:8","tags":["search_engine","note"],"title":"（WIP）《这就是搜索引擎》阅读笔记","uri":"/posts/search_engine/common/0.html/"},{"categories":["search engine"],"content":"3.9 短语查询 若单词的倒排列表只存储文档编号和单词频率信息，其保留的信息是不足以支持短语搜索的，因为单词之间的顺序关系没有保留。 搜索引擎支持短语查询，本质问题是如何在索引中维护单词之间的顺序关系或者位置信息。较常见的支持短语查询的技术方法包括： 位置信息索引 双词索引 短语索引 位置信息索引（Position Index） 使用索引记载单词位置信息来支持短语查询，但效率较低和成本较高。 双词索引（Nextword Index） 由于维度增加，双词索引使得索引和倒排列表指数级增长，所以一般对于计算和存储代价高使用 短语索引（Phrase Index） 短语索引就是在词典中直接加入多词短语并维护短语的倒排列表以此对短语进行支持。 局限性在于需要手动挖掘热门短语，对于其他短语查询采用普通方式处理。 混合索引 混合索引即结合上述索引的优缺点根据具体应用场景进行设计。 ","date":"2022-02-26","objectID":"/posts/search_engine/common/0.html/:3:9","tags":["search_engine","note"],"title":"（WIP）《这就是搜索引擎》阅读笔记","uri":"/posts/search_engine/common/0.html/"},{"categories":["search engine"],"content":"3.10 分布式索引 多台机器如何分工协作，目前常用的分布式索引方案包括两种： 按文档对索引划分 按单词对索引划分 按文档划分（Document Partitioning） 将整个文档集合切割成若干子集合，而每台机器负责对某个文档子集合建立索引，并响应查询请求。 主要过程就是查询分发服务器综合各个索引服务器的搜索结果后，合并搜索结果，将得分最高的文档作为最终搜索结果返回给用户。 按单词划分（Term Partitioning） 每个索引服务器负责词典中部分单词的倒排列表的建立和维护。 比较 以上两种分布式索引技术方案，按文档来对索引进行划分是比较常用的，而按单词进行索引划分只在比较特殊的应用场合才使用。相较于文档对索引进行划分，后者有以下局限性： 可扩展性（会影响到所有索引服务器） 负载均衡（如热点索引场景） 容错性（若索引服务器宕机则导致直接查询失败） 对查询方式处理（只支持一次一单词处理方式） ","date":"2022-02-26","objectID":"/posts/search_engine/common/0.html/:3:10","tags":["search_engine","note"],"title":"（WIP）《这就是搜索引擎》阅读笔记","uri":"/posts/search_engine/common/0.html/"},{"categories":["machine learning"],"content":"概述 通过读取大量的数据、学习数据的特征并从中找出数据的模式。 这样的任务也被称为机器学习或者模式识别。 机器学习中比较擅长的任务： 回归（regression）：从连续数据中学习趋势，如时间序列数据； 分类（classification）：数据携带标签进行分类； 聚类（clustering）：数据本身不带标签； 机器学习中监督学习区分： 使用有标签的数据进行的学习称为有监督学习（回归和分类）； 使用没有标签的数据进行的学习称为无监督学习（聚类）； ","date":"2022-02-26","objectID":"/posts/ai/machine_learning/0.html/:1:0","tags":["ai","machine_learning","note"],"title":"《白话机器学习的数学》阅读笔记","uri":"/posts/ai/machine_learning/0.html/"},{"categories":["machine learning"],"content":"回归问题 $y=\\theta_0+\\theta_1x$ $f_\\theta(x)=\\theta_0+\\theta_1x$ ","date":"2022-02-26","objectID":"/posts/ai/machine_learning/0.html/:2:0","tags":["ai","machine_learning","note"],"title":"《白话机器学习的数学》阅读笔记","uri":"/posts/ai/machine_learning/0.html/"},{"categories":["machine learning"],"content":"最小二乘法公式 $$ E(\\theta)=\\frac{1}{2}\\sum_{i=1}^{n}(y^i-f_\\theta(x^i))^2 $$ 使$y$与$f_\\theta(x)$误差尽可能减小即最优化问题，使用差值的平方是因为避免差值为负数同时利于微分，至于$\\frac{1}{2}$是为了让作为结果的表达式变得简单而随便加的常数（在最优化问题中不影响结果）。 ","date":"2022-02-26","objectID":"/posts/ai/machine_learning/0.html/:2:1","tags":["ai","machine_learning","note"],"title":"《白话机器学习的数学》阅读笔记","uri":"/posts/ai/machine_learning/0.html/"},{"categories":["machine learning"],"content":"最速下降法（或梯度下降法） $$ x:=x-\\eta\\frac{\\mathrm{d}}{\\mathrm{d}x}g(x) $$ 其中$\\eta$即为学习率，可以理解$\\eta$越大，则下降越快，更新次数越少，甚至可能远离最优值，这就是发散状态，而$\\eta$较小时移动量较小，更新次数增多，但是值会往收敛方向移动。 因为$f_\\theta(x)$拥有$\\theta_0$和$\\theta_1$两个参数，即目标函数是双变量函数，所以不能用普通的微分，而要用偏微分。 $$\\theta_0=\\theta_0-\\eta\\frac{\\partial{E}}{\\partial{\\theta_0}}$$ $$\\theta_1=\\theta_1-\\eta\\frac{\\partial{E}}{\\partial{\\theta_1}}$$ 假设$u=E(\\theta)$，$v=f_\\theta(x)$，利用阶梯性地进行微分： $$\\frac{\\partial{u}}{\\partial{\\theta_0}}=\\frac{\\partial{u}}{\\partial{v}}\\cdot\\frac{\\partial{v}}{\\partial{\\theta_0}}$$ 最后得到： $$\\theta_0=\\theta_0-\\eta\\sum_{i=1}^{n}(f_\\theta(x^i)-y^i)$$ $$\\theta_1=\\theta_1-\\eta\\sum_{i=1}^{n}(f_\\theta(x^i)-y^i)x^i$$ 前面我们假设$f_\\theta(x)=\\theta_0+\\theta_1x$，若这里为了更好拟合更新成$f_\\theta(x)=\\theta_0+\\theta_1x+\\theta_2x^2$，进行上面类似的梯度计算也可以得到： $$\\theta_0=\\theta_0-\\eta\\sum_{i=1}^{n}(f_\\theta(x^i)-y^i)$$ $$\\theta_1=\\theta_1-\\eta\\sum_{i=1}^{n}(f_\\theta(x^i)-y^i)x^i$$ $$\\theta_2=\\theta_2-\\eta\\sum_{i=1}^{n}(f_\\theta(x^i)-y^i)x^{i^2}$$ 像这样增加函数中多项式的次数，然后再使用函数的分析方法被称为多项式回归。 ","date":"2022-02-26","objectID":"/posts/ai/machine_learning/0.html/:2:2","tags":["ai","machine_learning","note"],"title":"《白话机器学习的数学》阅读笔记","uri":"/posts/ai/machine_learning/0.html/"},{"categories":["machine learning"],"content":"多重回归 $$f_\\theta(x_1,…,x_n)=\\theta_0+\\theta_1x_1+…+\\theta_nx_n$$ 将$\\theta$和$x$转换为列向量简化表达式： $$ \\mathbf{\\theta}=\\left[\\begin{matrix}\\theta_0\\\\\\theta_1\\\\\\vdots\\\\\\theta_n\\end{matrix} \\right] $$ $$ \\mathbf{x}=\\left[\\begin{matrix}x_0\\\\x_2\\\\\\vdots\\\\x_n\\end{matrix}\\right] $$ 得到：$\\mathbf{\\theta^T}\\mathbf{x}=\\theta_0x_0+\\theta_1x_1+…+\\theta_nx_n$ 即 $f_\\mathbf{\\theta}(\\mathbf{x})=\\mathbf{\\theta^T}\\mathbf{x}$ 然后我们代入梯度下降算法计算$\\theta$的公式： $$ \\theta_j=\\theta_j-\\eta\\sum_{i=1}^{n}(f_\\mathbf{\\theta}\\mathbf{x}^{(i)}-y^{(i)})x{_j}{^{(i)}} $$ ","date":"2022-02-26","objectID":"/posts/ai/machine_learning/0.html/:2:3","tags":["ai","machine_learning","note"],"title":"《白话机器学习的数学》阅读笔记","uri":"/posts/ai/machine_learning/0.html/"},{"categories":["machine learning"],"content":"随机梯度下降法 前面的最速下降法容易陷入局部最优解的情况： 而随机梯度下降法就是训练时随机选择训练数据，设随机选择$m$个训练数据额索引的集合为$K$，那么更新参数的公式为： $$ \\theta_j=\\theta_j-\\eta\\sum_{k\\in{K}}(f_\\mathbf{\\theta}\\mathbf{x}^{(k)}-y^{(k)})x{_j}{^{(k)}} $$ ","date":"2022-02-26","objectID":"/posts/ai/machine_learning/0.html/:2:4","tags":["ai","machine_learning","note"],"title":"《白话机器学习的数学》阅读笔记","uri":"/posts/ai/machine_learning/0.html/"},{"categories":["machine learning"],"content":"分类问题 使权重向量成为法线向量的直线，设权重向量为$\\mathbf{w}$，直线的表达式为（向量内积）： $$ \\mathbf{w}\\cdot\\mathbf{x} = 0 $$ 或者另外一个内积表达式：$\\mathbf{w}\\cdot\\mathbf{x} = |\\mathbf{w}|\\cdot|\\mathbf{x}|\\cdot\\cos{\\theta}$ 通过训练找到权重向量，然后才能得到与这个向量垂直的直线，最后根据这条直线就可以对数据进行分类了。 ","date":"2022-02-26","objectID":"/posts/ai/machine_learning/0.html/:3:0","tags":["ai","machine_learning","note"],"title":"《白话机器学习的数学》阅读笔记","uri":"/posts/ai/machine_learning/0.html/"},{"categories":["machine learning"],"content":"感知机 接受多个输入后将每个值与各自的权重相乘，最后输出总和的模型。 $$ f_\\mathbf{w}(\\mathbf{x})=\\left\\{\\begin{array}{ll}1\u0026(\\mathbf{w}\\cdot\\mathbf{x}\\geq0)\\\\-1\u0026(\\mathbf{w}\\cdot\\mathbf{x}\u003c0)\\end{array}\\right. $$ 权重向量的更新表达式： $$ \\mathbf{w}=\\left\\{\\begin{array}{ll}\\mathbf{w}+y^{i}\\mathbf{x}^{(i)}\u0026\u0026(f_\\mathbf{w}(\\mathbf{x}^{(i)})\\neq y^{(i)})\\\\\\mathbf{w}\u0026\u0026(f_\\mathbf{w}(\\mathbf{x}^{(i)})= ^{(i)})\\end{array}\\right. $$ 感知机的局限在于只能解决线性可分的问题。 ","date":"2022-02-26","objectID":"/posts/ai/machine_learning/0.html/:3:1","tags":["ai","machine_learning","note"],"title":"《白话机器学习的数学》阅读笔记","uri":"/posts/ai/machine_learning/0.html/"},{"categories":["machine learning"],"content":"sigmoid函数 $$ f_\\theta(\\mathbf{x})=\\frac{1}{1+e^{-\\mathbf{\\theta}^{T}\\mathbf{x}}} $$ 其sigmoid函数取值范围为$0\u003cf_\\theta(\\mathbf{x})\u003c1$。 其微分结果如下： $\\frac{\\mathrm{d}\\sigma{x}}{\\mathrm{d}x}=\\sigma{(x)}(1-\\sigma{(x)})$ ","date":"2022-02-26","objectID":"/posts/ai/machine_learning/0.html/:3:2","tags":["ai","machine_learning","note"],"title":"《白话机器学习的数学》阅读笔记","uri":"/posts/ai/machine_learning/0.html/"},{"categories":["machine learning"],"content":"似然函数 目标函数即为通过$P(y=1|\\mathbf{x})$和$P(y=0|\\mathbf{x})$的联合概率得到： $$ L(\\mathbf{\\theta})=\\prod_{i=1}^nP(y^{(i)}=1|\\mathbf{x}^{(i)})^{y^{(i)}}P(y^{(i)}=0|\\mathbf{x}^{(i)})^{1-y^{(i)}} $$ ","date":"2022-02-26","objectID":"/posts/ai/machine_learning/0.html/:3:3","tags":["ai","machine_learning","note"],"title":"《白话机器学习的数学》阅读笔记","uri":"/posts/ai/machine_learning/0.html/"},{"categories":["machine learning"],"content":"对数似然函数 将似然函数进行对数化便于运算，公式如下： $$ \\log{L(\\mathbf{\\theta})}=\\sum_{i=1}^n(y^{(i)}\\log{f_\\mathbf{\\theta}(\\mathbf{x}^{(i)})}+(1-y^{(i)})\\log{(1-f_\\mathbf{\\theta}(\\mathbf{x}^{(i)}}))) $$ 逻辑回归将这个对数似然函数用作目标函数，然后对目标函数求微分得到： $$ \\frac{\\partial{u}}{\\partial{\\theta_j}}=\\frac{\\partial{u}}{\\partial{v}}\\cdot\\frac{\\partial{v}}{\\partial{\\theta_j}}=\\sum_{i=1}^{n}(y^{(i)}-f_\\mathbf{\\theta}(\\mathbf{x}^{(i)}))x_j^{(i)} $$ 最后导出参数更新表达式（梯度下降），这里求的是最大化与之前最小化不同，所以其方向相反： $$ \\theta_j:=\\theta_j+\\eta\\sum_{i=1}^{n}(y^{(i)}-f_\\mathbf{\\theta}(\\mathbf{x}^{(i)}))x_j^{(i)} $$ 若与前面回归保持一致即： $$ \\theta_j:=\\theta_j-\\eta\\sum_{i=1}^{n}(f_\\mathbf{\\theta}(\\mathbf{x}^{(i)})-y^{(i)})x_j^{(i)} $$ ","date":"2022-02-26","objectID":"/posts/ai/machine_learning/0.html/:3:4","tags":["ai","machine_learning","note"],"title":"《白话机器学习的数学》阅读笔记","uri":"/posts/ai/machine_learning/0.html/"},{"categories":["machine learning"],"content":"线性不可分 类似多项回归一样，增加次数得到非线性曲线。 ","date":"2022-02-26","objectID":"/posts/ai/machine_learning/0.html/:3:5","tags":["ai","machine_learning","note"],"title":"《白话机器学习的数学》阅读笔记","uri":"/posts/ai/machine_learning/0.html/"},{"categories":["machine learning"],"content":"模型评估 ","date":"2022-02-26","objectID":"/posts/ai/machine_learning/0.html/:4:0","tags":["ai","machine_learning","note"],"title":"《白话机器学习的数学》阅读笔记","uri":"/posts/ai/machine_learning/0.html/"},{"categories":["machine learning"],"content":"交叉验证 将获取的全部训练数据分成两份，一份用于测试，一份用于训练。然后用前者来评估模型。（交叉验证） 回归 对于回归的情况，只要在训练好的模型上计算测试数据的误差的平方，再取其平均值就可以了： $$ \\frac{1}{n}\\sum_{i=1}^n(y^{(i)}-f_\\mathbf{\\theta}(\\mathbf{x}^{(i)}))^2 $$ 上面得到的值被称为均方误差或者MSE（Mean Square Error）。这个误差越小，精度就越高，模型也就越好。 分类 对于分类的情况，由于回归是连续值，所以可以从误差入手，但是分类中必须考虑分类的类别是否正确。 对于二分类的结果可以用这张图表示： 通过表中的4个记号来表示精度（Accuracy）,表达式如下，即值越高精度越高，也就意味着模型越好： $$ Accuracy = \\frac{TP+TN}{TP+FP+FN+TN} $$ 在大多情况下，除了精度还需要引入其他指标进行评估： 精确率（$Precision=\\frac{TP}{TP+FP}=\\frac{TN}{TN+FN}$，值越高分类错误越少) 召回率（$Recall=\\frac{TP}{TP+FN}=\\frac{TN}{TN+FP}$，值越高即被正确分类的数据越多） 当数据不平衡时，使用数量少的那个会更好。 一般来说，精确率和召回率会一个高一个低，需要取舍。基于此就出现了评定综合性能的指标$F1$值，表达式如下： $F1$值指标在数学上是精确率和召回率的调和平均值。 $$ Fmeasure=\\frac{2}{\\frac{1}{Precision}+\\frac{1}{Recall}} $$ 除$F1$值指标外，还有较为通用的带权重的$F$值指标，表达式如下： $$ WeightedFmeasure = \\frac{(1+\\beta^2)\\cdot Precision\\cdot Recall}{\\beta^2\\cdot Precision+Recall} $$ 交叉验证方法 在交叉验证的方法中，较为经典的是K折交叉验证: 把全部训练数据分为K份； 将K-1份数据用作训练数据，剩下的1份用作测试数据； 每次更换训练数据和测试数据，重复进行K次交叉验证； 最后计算K个精度的平均值，把它作为最终的精度； 不切实际地增加K值会非常耗费时间，所以必须要确定一个合适的K值。 ","date":"2022-02-26","objectID":"/posts/ai/machine_learning/0.html/:4:1","tags":["ai","machine_learning","note"],"title":"《白话机器学习的数学》阅读笔记","uri":"/posts/ai/machine_learning/0.html/"},{"categories":["machine learning"],"content":"正则化 过拟合 若模型只能拟合训练数据的状态被称为过拟合（overfitting）。 避免过拟合的方法一般有： 增加全部训练数据的数量 使用简单的模型 正则化 正则化方法 可以在前面回归得到的目标函数（最小二乘法）加上正则化项得到一个新的目标函数，如下： $$ E(\\mathbf{\\theta})=\\frac{1}{2}\\sum_{i=1}^{n}{(y^i-f_\\mathbf{\\theta}(\\mathbf{x}^i))^2}+R(\\mathbf{\\theta})=\\frac{1}{2}\\sum_{i=1}^{n}{(y^i-f_\\mathbf{\\theta}(\\mathbf{x}^i))^2}+\\frac{\\lambda}{2}\\sum_{j=1}^{m}\\theta_j^2 $$ 要对这个新的目标函数进行最小化，这种方法即称为正则化。 其中正则化项中： $m$：参数的个数; $\\theta_0$称为偏置项，一般不对它进行正则化； $\\lambda$：决定正则化项影响程度的正的常数； 正则化的效果可以防止参数变得过大，有助于参数接近较小的值。参数的值变小，意味着该参数的影响也会相应地变小。 分类的正则化 在前面分类提到的目标函数（对数似然函数）增加正则化项，表达式如下： $$ \\log{L(\\mathbf{\\theta})}=-\\sum_{i=1}^n(y^{(i)}\\log{f_\\mathbf{\\theta}(\\mathbf{x}^{(i)})}+(1-y^{(i)})\\log{(1-f_\\mathbf{\\theta}(\\mathbf{x}^{(i)}})))+\\frac{\\lambda}{2}\\sum_{j=1}^{m}\\theta_j^2 $$ 为方便与回归处理相似，将目标函数最大化的目标转化为最小化问题，在原目标函数增加负号，加上正则化项。且这里反转了符号之后，在更新参数时也需要将符号反方向移动。 包含正则化项的表达式的微分 其中回归和分类的目标函数的微分我们在前面已经求过了，这里主要是求正则化项的微分表达式: $$ \\frac{\\partial{R(\\mathbf{\\theta})}}{\\partial{\\theta_j}}=\\lambda\\theta_j $$ 最后这里代入到回归场景下的参数更新表达式中： $$ \\theta_j:=\\theta_j-\\eta\\big (\\sum_{i=1}^{n}(f_\\mathbf{\\theta}(\\mathbf{x}^{(i)})-y^{(i)})x_j^{(i)}+\\lambda\\theta_j\\big ) $$ 这里一般不对$\\theta_0$应用正则化，所以这里区分两种情况： $j=0$ $$ \\theta_0:=\\theta_0-\\eta\\big (\\sum_{i=1}^{n}(f_\\mathbf{\\theta}(\\mathbf{x}^{(i)})-y^{(i)})x_j^{(i)}\\big ) $$ $j\u003e0$ $$ \\theta_j:=\\theta_j-\\eta\\big (\\sum_{i=1}^{n}(f_\\mathbf{\\theta}(\\mathbf{x}^{(i)})-y^{(i)})x_j^{(i)}+\\lambda\\theta_j\\big ) $$ 至此提到的正则被称为L2正则化，除此之外还有L1正则化，它的正则化项如下： $$ R(\\mathbf{\\theta})= \\lambda\\sum_{i=1}^{m}|\\theta_i| $$ L1正则化的特征是被判定为不需要的参数会变为0，从而减少变量个数即直接去除不要的变量。而L2正则化不会把参数变为0，通过会抑制参数，使变量的影响不会过大。 ","date":"2022-02-26","objectID":"/posts/ai/machine_learning/0.html/:4:2","tags":["ai","machine_learning","note"],"title":"《白话机器学习的数学》阅读笔记","uri":"/posts/ai/machine_learning/0.html/"},{"categories":["machine learning"],"content":"学习曲线 欠拟合 欠拟合（underfitting）与过拟合相反的状态即没有拟合训练数据。在这种情况下模型的性能也会变差。 区分过拟合与欠拟合 将两份数据的精度用图来展示后，如果是这种形状，就说明出现了欠拟合的状态，也可以称为高偏差。 而过拟合或称为高方差，如下图所示： 像这样展示了数据数量和精度的图称为学习曲线。 ","date":"2022-02-26","objectID":"/posts/ai/machine_learning/0.html/:4:3","tags":["ai","machine_learning","note"],"title":"《白话机器学习的数学》阅读笔记","uri":"/posts/ai/machine_learning/0.html/"},{"categories":["分布式架构"],"content":"APM组件选型 探针的性能 重点在agent对服务的吞吐量、CPU和内存的影响。 微服务的规模和动态性使数据收集的成本提高。 collector的可扩展性 水平扩展以便支持更大规模服务器集群。 全面的调用链路数据分析 提供代码级别的可见性，定位失败点和瓶颈。 对于开发透明，容易开关 无需修改代码添加新功能，容易启用或者禁用。 完整的调用链应用拓扑 自动检测应用拓扑，应用架构。 ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/8.html/:0:0","tags":["分布式架构","apm"],"title":"apm组件选型","uri":"/posts/distributed_structure/structure/8.html/"},{"categories":["分布式架构"],"content":"参考 https://juejin.im/post/5a7a9e0af265da4e914b46f1 ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/8.html/:1:0","tags":["分布式架构","apm"],"title":"apm组件选型","uri":"/posts/distributed_structure/structure/8.html/"},{"categories":["通用技术"],"content":"Git中的工作流 ","date":"2022-01-29","objectID":"/posts/common_tech/git/0.html/:0:0","tags":["通用技术","Git"],"title":"Git中的工作流","uri":"/posts/common_tech/git/0.html/"},{"categories":["通用技术"],"content":"中心式协同工作流 一般过程如下： 从服务器上git pull origin master同步代码； 修改完后git commit到本地仓库中； 然后git push origin master到远程仓库中； 若三步push失败则出现了已经提交的版本与你本地版本不一致，则需要先把远程仓库的代码pull下来，为了避免有merge操作即可以使用git pull -rebase将远程仓库上的提交直接合并到本地仓库中。总结就是以下步骤： 先把本地提交的代码放在一边； 然后把远程仓库上的改动下载下来； 然后在本地把之前的改动全部分别commit直至全部成功。 若有冲突则解决冲突，使用git rebase –continue，直至解决所有冲突。 ","date":"2022-01-29","objectID":"/posts/common_tech/git/0.html/:1:0","tags":["通用技术","Git"],"title":"Git中的工作流","uri":"/posts/common_tech/git/0.html/"},{"categories":["通用技术"],"content":"功能分支协同工作流 一般过程如下： 首先使用git checkout -b new-feather创建新分支； 然后共同开发这个功能在此分支上进行开发工作； 然后通过git push -u origin new-feather把分支push到远程仓库上； 其他就可以通过git pull -rebase拿到这个分支的最新代码； 最后通过Pull Request做完CodeReveiw后合并到Master分支上。 ","date":"2022-01-29","objectID":"/posts/common_tech/git/0.html/:2:0","tags":["通用技术","Git"],"title":"Git中的工作流","uri":"/posts/common_tech/git/0.html/"},{"categories":["通用技术"],"content":"GitFlow协同工作流 重点在于维护且不影响当前可以发布的代码。 Master 分支。也就是主干分支，用作发布环境，上面的每一次提交都是可以发布的。 Feature 分支。也就是功能分支，用于开发功能，其对应的是开发环境。 Developer 分支。是开发分支，一旦功能开发完成，就向 Developer 分支合并，合并完成后，删除功能分支。这个分支对应的是集成测试环境。 Release 分支。当 Developer 分支测试达到可以发布状态时，开出一个 Release 分支来，然后做发布前的准备工作。这个分支对应的是预发环境。之所以需要这个 Release 分支，是我们的开发可以继续向前，不会因为要发布而被 block 住而不能提交。 一旦 Release 分支上的代码达到可以上线的状态，那么需要把 Release 分支向 Master 分支和 Developer 分支同时合并，以保证代码的一致性。然后再把 Release 分支删除掉。 Hotfix 分支。是用于处理生产线上代码的 Bug-fix，每个线上代码的 Bug-fix 都需要开一个 Hotfix 分支，完成后，向 Developer 分支和 Master 分支上合并。合并完成后，删除 Hotfix 分支。 ","date":"2022-01-29","objectID":"/posts/common_tech/git/0.html/:3:0","tags":["通用技术","Git"],"title":"Git中的工作流","uri":"/posts/common_tech/git/0.html/"},{"categories":["通用技术"],"content":"GitHub/Gitlab协同工作流 GitFlow会因分支太多造成git log混乱的局面，且需要同时维护Master和Developer分支。 ","date":"2022-01-29","objectID":"/posts/common_tech/git/0.html/:4:0","tags":["通用技术","Git"],"title":"Git中的工作流","uri":"/posts/common_tech/git/0.html/"},{"categories":["通用技术"],"content":"GitFlow 一般过程如下： 将官方库fork到自己的代码仓库； 开发人员在自己的代码仓库进行开发； 自己的代码仓库中需要配置两个远程仓库； 本地建立功能分支，在此分支上做代码开发； 这个功能分支push到自己的代码仓库中； 然后向官方库发起pull request，并做code review； 一旦通过则向官方库进行合并。 ","date":"2022-01-29","objectID":"/posts/common_tech/git/0.html/:4:1","tags":["通用技术","Git"],"title":"Git中的工作流","uri":"/posts/common_tech/git/0.html/"},{"categories":["通用技术"],"content":"GillabFlow 重点在于引入不同的环境分支，且包含了预发布和生产分支。 这样也就解决了两个问题： 环境和代码分支对应的问题； 版本和代码分支对应的问题。 ","date":"2022-01-29","objectID":"/posts/common_tech/git/0.html/:4:2","tags":["通用技术","Git"],"title":"Git中的工作流","uri":"/posts/common_tech/git/0.html/"},{"categories":["通用技术"],"content":"本质 软件开发的趋势一般如下： 以微服务或者SOA架构的方式。 使协同工作流变得简单，将各个服务拆解成若干个代码仓库，对于GitFlow过于重，更适合使用GitHubFlow方式。 以DevOps为主的开发流程。 通过CI/CD，不需要维护多个版本，也不需要关注不同的运行环境，GitHubFlow或者功能分支这种方式更适应这种开发流程。 协同工作流的本质，并不是怎么玩好代码仓库的分支策略，而是玩好我们的软件架构和软件开发流程。 与其花时间在 Git 协同工作流上，还不如把时间花在调整软件架构和自动化软件生产和运维流程上来，这才是真正简化协同工作流程的根本。 ","date":"2022-01-29","objectID":"/posts/common_tech/git/0.html/:5:0","tags":["通用技术","Git"],"title":"Git中的工作流","uri":"/posts/common_tech/git/0.html/"},{"categories":["分布式架构"],"content":"MIT6824LEC1笔记:Map Reduce 课程地址 代码实现repo地址 即课程Lab1。 ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/3.html/:1:0","tags":["分布式架构","MIT6824","note"],"title":"MIT6824LEC1笔记:Map Reduce","uri":"/posts/distributed_structure/structure/3.html/"},{"categories":["分布式架构"],"content":"GFS 课程地址 ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/5.html/:0:0","tags":["分布式架构","MIT6824","note"],"title":"MIT6824LEC3笔记:GFS","uri":"/posts/distributed_structure/structure/5.html/"},{"categories":["分布式架构"],"content":"Before gfs ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/5.html/:1:0","tags":["分布式架构","MIT6824","note"],"title":"MIT6824LEC3笔记:GFS","uri":"/posts/distributed_structure/structure/5.html/"},{"categories":["分布式架构"],"content":"why is distributed storage hard High performance -\u003e shard data over many servers Many servers -\u003e constant faults Fault tolerance -\u003e replication replication -\u003e potential inconsistencies Better consistency -\u003e low performance it can sum up the all thinking points like: performance fault tolerance replication consistency these points have connection and influence each other. ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/5.html/:1:1","tags":["分布式架构","MIT6824","note"],"title":"MIT6824LEC3笔记:GFS","uri":"/posts/distributed_structure/structure/5.html/"},{"categories":["分布式架构"],"content":"what would we like for consistency? There are two conditions we can think: single server C1: wx1 C2: wx2 C3: rx? C4: rx? C3 or C4 can see either 1 or 2, but both have to see the same value. It’s a strong consistency model in the single server, but it’s has poor fault-tolerance. So we can think the replication for the fault-tolerance, but it will make strong consistency tricky. Replication servers:S1 S2 C1: write S1 or S2 C2: write S1 or S2 C3: read S1 C4: read S2 C3 or C4 will see the different results. It can’t promise the same result or special result, because there is no standard protocol. So that’s not strong consistency. For consistency, we need to use the special protocol to fix these problems. The gfs is one case about the distributed system’s protocol. ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/5.html/:1:2","tags":["分布式架构","MIT6824","note"],"title":"MIT6824LEC3笔记:GFS","uri":"/posts/distributed_structure/structure/5.html/"},{"categories":["分布式架构"],"content":"Gfs ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/5.html/:2:0","tags":["分布式架构","MIT6824","note"],"title":"MIT6824LEC3笔记:GFS","uri":"/posts/distributed_structure/structure/5.html/"},{"categories":["分布式架构"],"content":"Context Many Google services needed a big fast unified storage system, like Mapreduce, crawler/indexer, log storage/analysis, Youtube… Big: large data set Fast: automatic sharding For parallel performance To increase space available Global(over a single data center) any client can read any file Allows sharing of data among applications Fault-tolerance Automatic recovery from failures Just one data center per deployment Just Google applications/users Aimed at sequential access to huge files; read or append not a low-latency DB for small items What was new about this in 2003? How did they get an SOSP paper accepted? Not the basic ideas of distribution, sharding, fault-tolerance. Huge scale. Used in industry, real-world experience. Successful use of weak consistency. Successful use of single master. ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/5.html/:2:1","tags":["分布式架构","MIT6824","note"],"title":"MIT6824LEC3笔记:GFS","uri":"/posts/distributed_structure/structure/5.html/"},{"categories":["分布式架构"],"content":"Design clients (library, RPC – but not visible as a UNIX FS) each file split into independent 64 MB chunks chunk servers, each chunk replicated on 3 every file’s chunks are spread over the chunk servers for parallel read/write (e.g. MapReduce), and to allow huge files single master, and master replicas division of work: master deals w/ naming, chunkservers w/ data ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/5.html/:2:2","tags":["分布式架构","MIT6824","note"],"title":"MIT6824LEC3笔记:GFS","uri":"/posts/distributed_structure/structure/5.html/"},{"categories":["分布式架构"],"content":"Master state in RAM (for speed, must be smallish) file name / index -\u003e array of chunk handles Chunk handle(stable) -\u003e version(stable) -\u003e list of chunk servers(volatile) -\u003e primary, secondaries(volatile) -\u003e lease time(volatile) on disk: stable storage log -\u003e master will first write the log. when master fails or crashes, the log can reconsturct or fix the master for the backup. Checkpoint -\u003e when master fails or crashes, it can replay the last part or state basically then can quickly reconsturct master from the log. what state does need to end up in stable storage for the mass detection function coreectly? ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/5.html/:2:3","tags":["分布式架构","MIT6824","note"],"title":"MIT6824LEC3笔记:GFS","uri":"/posts/distributed_structure/structure/5.html/"},{"categories":["分布式架构"],"content":"steps when read a file C sends filename and offset to master M (if not cached) M finds chunk handle for that offset M replies with list of chunkservers only those with latest version C caches handle + chunkserver list -\u003e caches is to reduce the load on this single machine C sends request to nearest chunkserver chunk handle, offset -\u003e nearest chunkserver is to minimize network traffific or maximize the throughput about the joint setup form the clients to the servers chunk server check the reversion then reads from chunk file on disk, returns -\u003e check the version is to avoid reading the stale data ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/5.html/:2:4","tags":["分布式架构","MIT6824","note"],"title":"MIT6824LEC3笔记:GFS","uri":"/posts/distributed_structure/structure/5.html/"},{"categories":["分布式架构"],"content":"steps when write a file: append C asks M about file’s last chunk if M sees chunk has no primary (or lease expired): 2a. if no chunkservers w/ latest version # error 2b. pick primary P and secondaries from those w/ latest version # 2c. increment version #, write to log on disk 2d. tell P and secondaries who they are, and new version # 2e. replicas write new version # to disk 3. M tells C the primary and secondaries + version 4. C sends data to all (just temporary…), waits 5. C tells P to append 6. P checks that lease hasn’t expired, and chunk has space 7. P picks an offset (at end of chunk) 8. P writes chunk file (a Linux file) 9. P tells each secondary the offset, tells to append to chunk file 10. P waits for all secondaries to reply, or timeout secondary can reply “error” e.g. out of disk space 11. P tells C “ok” or “error” 12. C retries from start if error What consistency guarantees does GFS provide to clients? Needs to be in a form that tells applications how to use GFS. Here’s a possibility: If the primary tells a client that a record append succeeded, then any reader that subsequently opens the file and scans it will see the appended record somewhere. ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/5.html/:2:5","tags":["分布式架构","MIT6824","note"],"title":"MIT6824LEC3笔记:GFS","uri":"/posts/distributed_structure/structure/5.html/"},{"categories":["分布式架构"],"content":"question What if an appending client fails at an awkward moment? Is there an awkward moment? What if the appending client has cached a stale (wrong) primary? What if the reading client has cached a stale secondary list? Could a master crash+reboot cause it to forget about the file? Or forget what chunkservers hold the relevant chunk? Two clients do record append at exactly the same time. Will they overwrite each others' records? Suppose one secondary never hears the append command from the primary. What if reading client reads from that secondary? What if the primary crashes before sending append to all secondaries? Could a secondary that didn’t see the append be chosen as the new primary? Chunkserver S4 with an old stale copy of chunk is offline. Primary and all live secondaries crash. S4 comes back to life (before primary and secondaries). Will master choose S4 (with stale chunk) as primary? Better to have primary with stale data, or no replicas at all? What should a primary do if a secondary always fails writes? e.g. dead, or out of disk space, or disk has broken. Should the primary drop secondary from set of secondaries? And then return success to client appends? Or should the primary keep sending ops, and having them fail, and thus fail every client write request? What if primary S1 is alive and serving client requests, but network between master and S1 fails? “network partition” Will the master pick a new primary? Will there now be two primaries? So that the append goes to one primary, and the read to the other? Thus breaking the consistency guarantee? “split brain” If there’s a partitioned primary serving client appends, and its lease expires, and the master picks a new primary, will the new primary have the latest data as updated by partitioned primary? What if the master fails altogether. Will the replacement know everything the dead master knew? E.g. each chunk’s version number? primary? lease expiry time? Who/what decides the master is dead, and must be replaced? Could the master replicas ping the master, take over if no response? What happens if the entire building suffers a power failure? And then power is restored, and all servers reboot. Suppose the master wants to create a new chunk replica. Maybe because too few replicas. Suppose it’s the last chunk in the file, and being appended to. How does the new replica ensure it doesn’t miss any appends? After all it is not yet one of the secondaries. Is there any circumstance in which GFS will break the guarantee? i.e. append succeeds, but subsequent readers don’t see the record. All master replicas permanently lose state (permanent disk failure). Could be worse: result will be “no answer”, not “incorrect data”. “fail-stop” All chunkservers holding the chunk permanently lose disk content. again, fail-stop; not the worse possible outcome CPU, RAM, network, or disk yields an incorrect value. checksum catches some cases, but not all Time is not properly synchronized, so leases don’t work out. So multiple primaries, maybe write goes to one, read to the other. What application-visible anomalous behavior does GFS allow? Will all clients see the same file content? Could one client see a record that another client doesn’t see at all? Will a client see the same content if it reads a file twice? Will all clients see successfully appended records in the same order? Will these anomalies cause trouble for applications? How about MapReduce? ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/5.html/:2:6","tags":["分布式架构","MIT6824","note"],"title":"MIT6824LEC3笔记:GFS","uri":"/posts/distributed_structure/structure/5.html/"},{"categories":["分布式架构"],"content":"strict consistency I.e. all clients see the same file content. Too hard to give a real answer, but here are some issues. Primary should detect duplicate client write requests. Or client should not issue them. All secondaries should complete each write, or none. Perhaps tentative writes until all promise to complete it? Don’t expose writes until all have agreed to perform them! If primary crashes, some replicas may be missing the last few ops. New primary must talk to all replicas to find all recent ops, and sync with secondaries. To avoid client reading from stale ex-secondary, either all client reads must go to primary, or secondaries must also have leases. Performance (Figure 3) large aggregate throughput for read (3 copies, striping) 94 MB/sec total for 16 chunkservers or 6 MB/second per chunkserver is that good? one disk sequential throughput was about 30 MB/s one NIC was about 10 MB/s Close to saturating network (inter-switch link) So: individual server performance is low but scalability is good which is more important? Table 3 reports 500 MB/sec for production GFS, which is a lot writes to different files lower than possible maximum authors blame their network stack (but no detail) concurrent appends to single file limited by the server that stores last chunk hard to interpret after 15 years, e.g. how fast were the disks? Random issues worth considering What would it take to support small files well? What would it take to support billions of files? Could GFS be used as wide-area file system? With replicas in different cities? All replicas in one datacenter is not very fault tolerant! How long does GFS take to recover from a failure? Of a primary/secondary? Of the master? How well does GFS cope with slow chunkservers? Retrospective interview with GFS engineer: http://queue.acm.org/detail.cfm?id=1594206 file count was the biggest problem eventual numbers grew to 1000x those in Table 2 ! hard to fit in master RAM master scanning of all files/chunks for GC is slow 1000s of clients too much CPU load on master applications had to be designed to cope with GFS semantics and limitations master fail-over initially entirely manual, 10s of minutes BigTable is one answer to many-small-files problem and Colossus apparently shards master data over many masters ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/5.html/:2:7","tags":["分布式架构","MIT6824","note"],"title":"MIT6824LEC3笔记:GFS","uri":"/posts/distributed_structure/structure/5.html/"},{"categories":["分布式架构"],"content":"Summary case study of performance, fault-tolerance, consistency specialized for MapReduce applications good ideas: global cluster file system as universal infrastructure separation of naming (master) from storage (chunkserver) sharding for parallel throughput huge files/chunks to reduce overheads primary to sequence writes leases to prevent split-brain chunkserver primaries not so great: single master performance:ran out of RAM and CPU chunkservers not very efficient for small files lack of automatic fail-over to master replica maybe consistency was too relaxed ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/5.html/:3:0","tags":["分布式架构","MIT6824","note"],"title":"MIT6824LEC3笔记:GFS","uri":"/posts/distributed_structure/structure/5.html/"},{"categories":["分布式架构"],"content":"Primary/Backup Replication 课程地址 the note from the lecture Primary/Backup Replication for Fault Tolerance Case study of VMware FT, an extreme version of the idea fault tolerance to provide availability despite server and network failures using replication What kinds of failures can replication deal with? “fail-stop” failure of a single replica fan stops working, CPU overheats and shuts itself down someone trips over replica’s power cord or network cable software notices it is out of disk space and stops Maybe not defects in h/w or bugs in s/w or human configuration errors Often not fail-stop May be correlated (i.e. cause all replicas to crash at the same time) But, sometimes can be detected (e.g. checksums) How about earthquake or city-wide power failure? Only if replicas are physically separated Is replication worth the Nx expense? Two main replication approaches: State transfer Primary replica executes the service Primary sends [new] state to backups State transfer is simpler ,But state may be large, slow to transfer over network Replicated state machine Clients send operations to primary, primary sequences and sends to backups All replicas execute all operations If same start state, same operations, same order, deterministic, then same end state. Replicated state machine often generates less network traffic Operations are often small compared to state But complex to get right VM-FT uses replicated state machine Big Questions: What state to replicate? Does primary have to wait for backup? When to cut over to backup? Are anomalies visible at cut-over? How to bring a replacement backup up to speed? At what level do we want replicas to be identical? Application state, e.g. a database’s tables? GFS works this way Can be efficient; primary only sends high-level operations to backup Application code (server) must understand fault tolerance, to e.g. forward op stream Machine level, e.g. registers and RAM content? might allow us to replicate any existing server w/o modification! requires forwarding of machine events (interrupts, DMA, \u0026c) requires “machine” modifications to send/recv event stream… paper (VMware FT) replicates machine-level state Transparent: can run any existing O/S and server software! Appears like a single server to clients Overview [diagram: app, O/S, VM-FT underneath, disk server, network, clients] words: hypervisor == monitor == VMM (virtual machine monitor) O/S+app is the “guest” running inside a virtual machine two machines, primary and backup primary sends all external events (client packets \u0026c) to backup over network “logging channel”, carrying log entries ordinarily, backup’s output is suppressed by FT if either stops being able to talk to the other over the network “goes live” and provides sole service if primary goes live, it stops sending log entries to the backup VMM emulates a local disk interface ,but actual storage is on a network server treated much like a client: usually only primary communicates with disk server (backup’s FT discards) if backup goes live, it talks to disk server external disk makes creating a new backup faster (don’t have to copy primary’s disk) When does the primary have to send information to the backup? Any time something happens that might cause their executions to diverge. Anything that’s not a deterministic consequence of executing instructions. What sources of divergence must FT handle? Most instructions execute identically on primary and backup. As long as memory+registers are identical, which we’re assuming by induction. Inputs from external world – just network packets. These appear as DMA’d data plus an interrupt. Timing of interrupts. Instructions that aren’t functions of state, such as reading current time. Not multi-core races, since uniprocessor only. Why would divergence be a disaster? b/c state on backup would differ from state on primary, and if primary then failed, clients would see inconsistency. Example: GFS lease expiration Imagine we’re replicating the GFS ma","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/1.html/:0:0","tags":["分布式架构","MIT6824","note"],"title":"MIT6824LEC4笔记:Primary-Backup Replication","uri":"/posts/distributed_structure/structure/1.html/"},{"categories":["分布式架构"],"content":"分布式熔断方案note ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/7.html/:0:0","tags":["分布式架构","熔断"],"title":"分布式熔断方案","uri":"/posts/distributed_structure/structure/7.html/"},{"categories":["分布式架构"],"content":"微服务容错机制 微服务架构中因各个服务之间的依赖和调用关系较为复杂，当下游的服务出现问题时就可能会造成上游的雪崩效应。 解决雪崩效应就需要建立有效的服务容错机制，主要有两个方向考虑： 服务冗余 熔断限流 其中服务冗余就需要建立集群，依托负载均衡和重试机制，保证服务可用性，当服务出错时可以设置以下不同的策略： FailOver 失败转移 FailBack 失败通知 FailSafe 失败安全 FailFast 快速失败 而除了服务冗余之外就是服务熔断和限流的保护机制，熔断是预防下游服务出现故障时阻断下游调用，限流是防止上游服务调用量过大导致当前服务或下游服务被压垮。 ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/7.html/:1:0","tags":["分布式架构","熔断"],"title":"分布式熔断方案","uri":"/posts/distributed_structure/structure/7.html/"},{"categories":["分布式架构"],"content":"熔断器设计方案 ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/7.html/:2:0","tags":["分布式架构","熔断"],"title":"分布式熔断方案","uri":"/posts/distributed_structure/structure/7.html/"},{"categories":["分布式架构"],"content":"核心思想 核心在于AOP进行请求拦截判断是否熔断，根据策略进行熔断状态转移。 ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/7.html/:2:1","tags":["分布式架构","熔断"],"title":"分布式熔断方案","uri":"/posts/distributed_structure/structure/7.html/"},{"categories":["分布式架构"],"content":"设计思想 根据服务发现和服务调用的不同方式，目前主要有三种方案： 直连模式 服务A直接访问服务B，在访问过程中加入熔断机制； 代理模式 通过集中代理方式，如利用网关层的代理进行熔断策略，不会侵入服务本身； 服务网格模式（Service Mesh） 也被称为边车模式（Side Car），类似于在服务本身所在机器节点层面加入代理，可以理解为将服务治理控制粒度细化； 直连模式 该模式下具体到服务访问流程中最为直接，即在代码层面加入熔断判断，基于AOP模式需要经过BeforeCall、Call、AfterCall的操作流程。 具体流程可以参考下图： 代理模式 该模式下一般引入网关进行统一熔断限流处理后再进行转发到达具体服务，可以为异步也可以为同步，具体根据实际业务场景。 在异步模式下需要注意熔断状态判断切换时的延时，优势在于对请求性能无影响。 在这里可以参考下图架构的方案，重点是利用Kafka进行异步日志收集： 服务网格模式 该模式适应于微服务架构下的服务治理，在服务节点中加入粒度更小的治理模块，本质就是将SDK代码部署为单独进程且与服务机器共存，并作为该机器的代理与其他节点进行交互。 具体实现方案和策略与上述模式没有太大区别，只是落地方案不同，因为服务网格模式下都处于类似于k8s等集群管理的系统下。 ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/7.html/:2:2","tags":["分布式架构","熔断"],"title":"分布式熔断方案","uri":"/posts/distributed_structure/structure/7.html/"},{"categories":["分布式架构"],"content":"参考 微服务高可用之熔断器实现原理与 Golang 实践 ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/7.html/:3:0","tags":["分布式架构","熔断"],"title":"分布式熔断方案","uri":"/posts/distributed_structure/structure/7.html/"},{"categories":["分布式架构"],"content":"分布式系统全局唯一ID生成器 代码实现示例repo地址 使用场景一般： 分布式系统设计时，数据分片场景下，通常需要一个全局唯一id； 在消息系统中需要消息唯一ID标识来防止消息重复； 多系统打通需要一个全局唯一标识 （如集团各业务线面对不同用户，需要一个全局用户id）； 一般有三种方式实现： UUID Universally Unique Identifier 是自由软件基金会组织制定的唯一辨识ID生成标准，大多数系统已实现，如微软的GUID实现。 生成格式如：3d422567-f034-4ab4-b98f-a34fd263d0de。 sequence 使用数据库维护一张映射表，使用主键自增生成唯一ID。 SnowFlake Twitter实现的算法，使用时间戳+机器分配标识+自增序列组成64位数字ID。 生成格式如：1292755860950487050 ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/4.html/:0:0","tags":["分布式架构","全局ID"],"title":"分布式系统全局唯一ID生成器","uri":"/posts/distributed_structure/structure/4.html/"},{"categories":["分布式架构"],"content":"UUID 实现可通过以下方式： google方案：https://github.com/google/uuid linux命令：/usr/bin/uuidgen 优点：性能高，本地生成，无依赖。 缺点：生成格式太长，不适合做数据库主键id，基于mac地址生成算法可能导致mac地址泄漏。 ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/4.html/:1:0","tags":["分布式架构","全局ID"],"title":"分布式系统全局唯一ID生成器","uri":"/posts/distributed_structure/structure/4.html/"},{"categories":["分布式架构"],"content":"sequece 实现的时候需要注意并发写读，使用事务进行解决。 在冗余服务部署时，可部署多个库表且设置不同step，让每个sequence产生不同id。 可提前进行分配id然后加载到业务中，可减少数据库IO操作，提高性能。 优点：ID单调递增，方便排序，且方案成熟部署简单。 缺点：依赖DB，若为单节点则有性能瓶颈，若为主从架构则需要注意主从一致性问题。 ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/4.html/:2:0","tags":["分布式架构","全局ID"],"title":"分布式系统全局唯一ID生成器","uri":"/posts/distributed_structure/structure/4.html/"},{"categories":["分布式架构"],"content":"SnowFlake 1位最高位：符号位不使用； 41位时间戳：2^41-1个数字代表69年，所以设置发号起始时间最好为发号器首次运行时间 ； 10位工作机器id：也会分为5位datacenterId和5位workerId ； 12位序列号：2^12-1个数字总共4095，同一毫秒同一机器节点可以并发产生4095个不同Id； 在部署上面，不同机器使用不同DatacenterId（数据中心集群id）和WorkerId （机器节点id），最多可部署1024个节点。 多节点部署时，可使用zookeeper做节点分布式协调进行一致性管理，当出现时钟回拨可由zk来同步时间或摘除节点。 优点：ID呈递增趋势，满足排序场景，不依赖于其他组件且易于维护。 缺点：依赖机器时钟，会因为时钟回拨问题会导致发号重复或不可用，代码实现时采用循环等待下一时钟的方式，可能会有性能问题。 ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/4.html/:3:0","tags":["分布式架构","全局ID"],"title":"分布式系统全局唯一ID生成器","uri":"/posts/distributed_structure/structure/4.html/"},{"categories":["分布式架构"],"content":"总结 UUID sequence snowflake 描述 集成在标准系统中可简单生成 使用DB自增id实现 根据时间+机器分配标识+自增序列生成 依赖 无 DB 无 优点 本地生成性能高 部署简单生成ID单调递增 部署简单生成ID单调递增 缺点 生成号码复杂，很多场景不利于使用 依赖DB有性能问题和重复发号问题号码存在规律会泄露信息 依赖系统时钟，时钟回拨会造成重复发号问题 针对sequence表方式、snowflake方式的缺点，美团leaf给出了更详细的优化方案可以参考，这里就不过多引用，直接查看参考文档。 ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/4.html/:4:0","tags":["分布式架构","全局ID"],"title":"分布式系统全局唯一ID生成器","uri":"/posts/distributed_structure/structure/4.html/"},{"categories":["分布式架构"],"content":"参考 https://mp.weixin.qq.com/s/XjF_6x7uRzX-gLCdnb5DQQ ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/4.html/:5:0","tags":["分布式架构","全局ID"],"title":"分布式系统全局唯一ID生成器","uri":"/posts/distributed_structure/structure/4.html/"},{"categories":["分布式架构"],"content":"分布式系统架构的本质note 以下为左耳听风 记录的笔记。 ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/0.html/:0:0","tags":["分布式架构","note","架构本质"],"title":"分布式系统架构的本质note","uri":"/posts/distributed_structure/structure/0.html/"},{"categories":["分布式架构"],"content":"为什么需要分布式系统 放弃传统的单体架构，选择分布式系统，主要有两方面原因： 增大系统容量。 当业务量越来越大，一台机器的性能无法满足，需要多台机器才能应对大规模应用场景，所以需要垂直或者水平拆分业务系统，使其变成分布式架构。 加强系统可用。 业务越来越关键，需要提高整个系统架构的可用性，意味着架构中不能存在单点故障，为避免一台机器故障导致整体不可用，所以需要通过分布式架构来冗余系统以消除单点故障。 还有其他优势： 模块化，系统模块重用度高； 软件服务模块被拆分，开发和发布速度可以并行而变得更快； 系统扩展性更高； 团队协作流程得到改善等。 ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/0.html/:1:0","tags":["分布式架构","note","架构本质"],"title":"分布式系统架构的本质note","uri":"/posts/distributed_structure/structure/0.html/"},{"categories":["分布式架构"],"content":"分布式系统的发展 开发、维护和使用 SOA 要遵循以下几条基本原则： 可重用，粒度合适，模块化，可组合，构件化以及有互操作性； 符合开放标准（通用的或行业的）； 服务的识别和分类，提供和发布，监控和跟踪。 SOA架构的演化图： 单体架构，软件模块高度耦合； 通过标准的协议或是中间件来联动其它相关联的服务来实现松耦合，利用IOC（控制反转）和DIP（依赖倒置原则）设计思想进行实践； 业务拆分成多个微服务，微服务独立完整运行，服务间的整合需要服务编排或是服务整合的引擎，可以使工作流引擎，也可以是网关，也需要辅助于像容器化调度的技术方式来进行管理。 微服务的出现使得开发速度变得更快，部署快，隔离性高，系统的扩展度也很好，但是在集成测试、运维和服务管理等方面难度提升，所以需要一套较好的微服务PaaS平台。 ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/0.html/:2:0","tags":["分布式架构","note","架构本质"],"title":"分布式系统架构的本质note","uri":"/posts/distributed_structure/structure/0.html/"},{"categories":["分布式架构"],"content":"分布式系统中需要注意的问题 ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/0.html/:3:0","tags":["分布式架构","note","架构本质"],"title":"分布式系统架构的本质note","uri":"/posts/distributed_structure/structure/0.html/"},{"categories":["分布式架构"],"content":"异构系统的不标准问题 主要表现在： 软件和应用不标准； 通讯协议不标准； 数据格式不标准； 开发和运维的过程和方法不标准； ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/0.html/:3:1","tags":["分布式架构","note","架构本质"],"title":"分布式系统架构的本质note","uri":"/posts/distributed_structure/structure/0.html/"},{"categories":["分布式架构"],"content":"系统架构中的服务依赖性问题 服务依赖会导致的问题主要有以下场景： 如果非关键业务被关键业务所依赖，会导致非关键业务变成一个关键业务。 服务依赖链中，出现“木桶短板效应”——整个 SLA 由最差的那个服务所决定。 这就涉及到服务治理的内容。 不但要拆分服务，还要为每个服务拆分相应的数据库。 ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/0.html/:3:2","tags":["分布式架构","note","架构本质"],"title":"分布式系统架构的本质note","uri":"/posts/distributed_structure/structure/0.html/"},{"categories":["分布式架构"],"content":"故障发生的概率更大 出现故障不可怕，故障恢复时间过长才可怕。 出现故障不可怕，故障影响面过大才可怕。 所谓 Design for Failure。在设计时就要考虑如何减轻故障。如果无法避免，也要使用自动化的方式恢复故障，减少故障影响面。 ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/0.html/:3:3","tags":["分布式架构","note","架构本质"],"title":"分布式系统架构的本质note","uri":"/posts/distributed_structure/structure/0.html/"},{"categories":["分布式架构"],"content":"多层架构的运维复杂度更大 通常来说可以把系统分成四层：基础层、平台层、应用层和接入层。 基础层：机器、网络和存储设备等。 平台层：中间件层，Tomcat、MySQL、Redis、Kafka 之类的软件。 应用层：业务软件，比如，各种功能的服务。 接入层：接入用户请求的网关、负载均衡或是 CDN、DNS 类似。 需要注意： 任何一层的问题都会导致整体的问题； 没有统一的视图和管理，导致运维被割裂开来，造成更大的复杂度。 ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/0.html/:3:4","tags":["分布式架构","note","架构本质"],"title":"分布式系统架构的本质note","uri":"/posts/distributed_structure/structure/0.html/"},{"categories":["分布式架构"],"content":"分布式系统的技术栈 构建分布式系统的目的是为了增加系统容量，提高系统的可用性，转换为技术方面，就是以下两件事情，即一是提高整体架构的吞吐量，服务更多的并发和流量，二是为了提高系统的稳定性，让系统的可用性更高。 大流量处理。 关键业务保护。 ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/0.html/:4:0","tags":["分布式架构","note","架构本质"],"title":"分布式系统架构的本质note","uri":"/posts/distributed_structure/structure/0.html/"},{"categories":["分布式架构"],"content":"提高架构的性能 缓存系统 加入缓存系统，可以有效地提高系统的访问能力。对于分布式系统下的缓存系统，需要的是一个缓存集群。这其中需要一个 Proxy 来做缓存的分片和路由。 负载均衡系统 水平扩展的关键技术，使用多台机器来共同分担一部分流量请求。 异步调用 主要通过消息队列来对请求做排队处理，来达到削峰填谷的目的，但是会带来实时性的问题，同时需要通过消息持久化来解决消息丢失的问题，但这会造成“有状态”的结点，增加了服务调度的难度。 数据分区 把数据按一定的方式分成多个区，不同的数据区来分担不同区的流量。这需要一个数据路由的中间件，会导致跨库的 Join 和跨库的事务非常复杂。 数据镜像 把一个数据库镜像成多份一样的数据，这样就不需要数据路由的中间件了。为了达到内部同步的读写一致，数据镜像中最大的问题就是数据的一致性问题。 ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/0.html/:4:1","tags":["分布式架构","note","架构本质"],"title":"分布式系统架构的本质note","uri":"/posts/distributed_structure/structure/0.html/"},{"categories":["分布式架构"],"content":"提高架构的稳定性 服务拆分 主要有两个目的：一是为了隔离故障，二是为了重用服务模块。但服务拆分完之后，会引入服务调用间的依赖问题。 服务冗余 服务冗余是为了去除单点故障，并可以支持服务的弹性伸缩，以及故障迁移。但是对于某些有状态的服务来说，冗余这些有状态的服务带来了更高的复杂性。其中一个是弹性伸缩时，需要考虑数据的复制或是重新分片，迁移的时候还要迁移数据到其它机器上。 限流降级 通过限流或者功能降级的方式来停掉一部分服务，或是拒绝一部分用户，以确保整个架构不会挂掉。这些技术属于保护措施。 高可用架构 从冗余架构的角度来保障可用性，如多租户隔离，灾备多活，或是数据可以在其中复制保持一致性的集群。主要目的就是解决单点故障。 高可用运维 指的是 DevOps 中的 CI/CD（持续集成 / 持续部署）。良好的运维应该是一条很流畅的软件发布管线，其中涉及到自动化测试和相应的灰度发布，以及对线上系统的自动化控制。这样，可以做到“计划内”或是“非计划内”的宕机事件的时长最短。 ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/0.html/:4:2","tags":["分布式架构","note","架构本质"],"title":"分布式系统架构的本质note","uri":"/posts/distributed_structure/structure/0.html/"},{"categories":["分布式架构"],"content":"分布式系统的关键技术 服务治理 服务拆分、服务调用、服务发现、服务依赖、服务的关键度定义等，服务治理的最大意义是梳理服务间的依赖关系、服务调用链，以及关键的服务，且对这些服务进行性能和可用性方面的管理。 架构软件管理 因为服务之间有依赖且出现兼容性问题，所以整体服务所形成的架构需要有架构版本管理、整体架构的生命周期管理，以及对服务的编排、聚合、事务处理等服务调度功能。 DevOps 为了分布式系统可以更为快速地更新服务，且对于服务的测试和部署，所以需要 DevOps 的全流程，其中包括环境构建、持续集成、持续部署等。 自动化运维 通过DevOps对服务进行自动伸缩、故障迁移、配置管理、状态管理等一系列的自动化运维技术。 资源调度管理 应用层的自动化运维需要基础层的调度支持，即云计算 IaaS 层的计算、存储、网络等资源调度、隔离和管理。 整体架构监控 监控需要对三层系统（应用层、中间件层、基础层）进行监控。 流量控制 包括负载均衡、服务路由、熔断、降级、限流等和流量相关的调度，且包括灰度发布之类的功能等。 ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/0.html/:4:3","tags":["分布式架构","note","架构本质"],"title":"分布式系统架构的本质note","uri":"/posts/distributed_structure/structure/0.html/"},{"categories":["分布式架构"],"content":"分布式系统纲领 分布式系统关键技术总结下来有以下： 全栈系统监控； 服务 / 资源调度； 流量调度； 状态 / 数据调度； 开发和运维的自动化。 ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/0.html/:4:4","tags":["分布式架构","note","架构本质"],"title":"分布式系统架构的本质note","uri":"/posts/distributed_structure/structure/0.html/"},{"categories":["分布式架构"],"content":"分布式系统关键技术：监控 监控系统需要完成的功能： 全栈监控； 关键分析； 跨系统调用的串联； 实时报警和自动处置； 系统性能分析； ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/0.html/:5:0","tags":["分布式架构","note","架构本质"],"title":"分布式系统架构的本质note","uri":"/posts/distributed_structure/structure/0.html/"},{"categories":["分布式架构"],"content":"全栈监控 全栈监控简单来说就是三层监控： 基础层：监控主机和底层资源。 比如：CPU、内存、网络吞吐、硬盘 I/O、硬盘使用等。 中间层：就是中间件层的监控。 比如：Nginx、Redis、ActiveMQ、Kafka、MySQL、Tomcat 等。 应用层：监控应用层的使用。 比如：HTTP 访问的吞吐量、响应时间、返回码、调用链路分析、性能瓶颈，还包括用户端的监控。 监控的标准化： 日志数据结构化； 监控数据格式标准化； 统一的监控平台； 统一的日志分析； ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/0.html/:5:1","tags":["分布式架构","note","架构本质"],"title":"分布式系统架构的本质note","uri":"/posts/distributed_structure/structure/0.html/"},{"categories":["分布式架构"],"content":"好的监控 监控系统主要存在两个很大的问题： 监控数据是隔离开来的。 监控的数据项太多。 一个好的监控系统应该有以下几个特征： 关注于整体应用的SLA。主要从为用户服务的 API 来监控整个系统。 关联指标聚合。把有关联的系统及其指标聚合展示。主要是三层系统数据：基础层、平台中间件层和应用层。把服务和相关的中间件以及主机关联在一起，尤其需要把服务的具体实例和主机关联在一起。 快速故障定位。以跟踪一个用户请求的trace来监控出现整个分布式系统上的调用链，最好是做成没有侵入性的。 一个好的监控系统主要是为以下两个场景设计的： 体验 容量管理。提供全局系统运行时数据展示，可以判断是否需要增加机器或者其他资源。 性能管理。通过查看Dashborad找到系统瓶颈，针对性地优化系统和相应代码。 急诊 定位问题。快速暴露并找到问题的发生点，以助于诊断问题。 性能分析。当出现非预期的流量提升时，可以快速找到系统瓶颈，帮助开发人员深入代码。 ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/0.html/:5:2","tags":["分布式架构","note","架构本质"],"title":"分布式系统架构的本质note","uri":"/posts/distributed_structure/structure/0.html/"},{"categories":["分布式架构"],"content":"如何实现好的监控系统 服务调用链跟踪 对外API——关联后台实际服务——服务依赖关联——直至最后一个服务。 服务调用时长分布 服务调用链上的时间分布。 服务的TOPN视图 系统请求的排名情况，可根据调用量、请求耗时、热点进行排名。 数据库关联操作 执行数据库操作的执行时间，将相关请求对应起来。 服务资源跟踪 将服务运行的机器节点上的数据关联。 在把数据收集好的同时，更重要的是把数据关联好，才能够很快地定位故障，进而才能进行自动化调度。如下图所出现的情况。 ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/0.html/:5:3","tags":["分布式架构","note","架构本质"],"title":"分布式系统架构的本质note","uri":"/posts/distributed_structure/structure/0.html/"},{"categories":["分布式架构"],"content":"分布式系统关键技术：服务调度 关键技术，主要有以下几点： 服务关键程度； 服务依赖关系； 服务发现； 整个架构的版本管理； 服务应用声明周期安全管理； ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/0.html/:6:0","tags":["分布式架构","note","架构本质"],"title":"分布式系统架构的本质note","uri":"/posts/distributed_structure/structure/0.html/"},{"categories":["分布式架构"],"content":"服务关键程度和服务的依赖关系 服务关键程度，主要是要我们梳理和定义服务的重要程度。 服务间的依赖关系，传统的SOA希望通过ESB来解决服务间的依赖关系，而微服务时服务依赖最优解的上限，而下限是千万不要有依赖环。 解决服务依赖环的方案一般是依赖倒置的设计模式，即在分布式架构上通过一个第三方的服务来解决这个事。 服务依赖关系可通过技术手段发现，如Zipkin服务调用跟踪系统。 ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/0.html/:6:1","tags":["分布式架构","note","架构本质"],"title":"分布式系统架构的本质note","uri":"/posts/distributed_structure/structure/0.html/"},{"categories":["分布式架构"],"content":"服务状态和生命周期的管理 需要服务发现的中间件来管理架构的服务、服务版本、服务实例、服务状态等。 有了服务的状态和运行情况后，就需要对服务的生命周期进行管理，服务的生命周期通常有以下几个状态： Provision，代表在供应一个新的服务； Ready，表示启动成功了； Run，表示通过了服务健康检查； Update，表示在升级中； Rollback，表示在回滚中； Scale，表示正在伸缩中（可以有 Scale-in 和 Scale-out 两种）； Destroy，表示在销毁中； Failed，表示失败状态。 ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/0.html/:6:2","tags":["分布式架构","note","架构本质"],"title":"分布式系统架构的本质note","uri":"/posts/distributed_structure/structure/0.html/"},{"categories":["分布式架构"],"content":"整个架构的版本管理 VersionSet，由一堆服务的版本集所形成的整个架构的版本控制。 考虑到版本兼容性的问题，除了各个项目的版本管理之外，还需要在上面再盖一层版本管理。 版本管理为实现服务之间的版本兼容性，则需要定义服务清单即定义了所有服务的版本运行环境，其中包括但不限于： 服务的软件版本； 服务的运行环境——环境变量、CPU、内存等； 服务运行的最大最小实例数； ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/0.html/:6:3","tags":["分布式架构","note","架构本质"],"title":"分布式系统架构的本质note","uri":"/posts/distributed_structure/structure/0.html/"},{"categories":["分布式架构"],"content":"资源/服务调度 服务和资源调度的过程，与操作系统调度进程的方式很相似，主要有以下一些关键技术： 服务状态的维持和拟合； 服务的运行状态，包括一些非预期的变化和预期的变化。 在发布新版本时，需要伸缩，需要回滚。集群管理控制器会把集群从现有的状态迁移到另一个状态，它需要一步一步地向集群发送若干控制命令，即这个过程为“拟合”。 比如，当需要对集群进行 Scale 的时候，我们需要： 先扩展出几个结点； 再往上部署服务； 然后启动服务； 再检查服务的健康情况； 最后把新扩展出来的服务实例加入服务发现中提供服务。 服务的弹性伸缩和故障迁移； 服务伸缩所需要的操作步骤，其中涉及到了： 底层资源的伸缩； 服务的自动化部署； 服务的健康检查； 服务发现的注册； 服务流量的调度。 而对于故障迁移，也就是服务的某个实例出现问题时则需要自动地恢复它。对于服务来说，有两种模式，一种是宠物模式，一种是奶牛模式。 对于这两种模式，在运行中也是比较复杂的，其中涉及到了： 服务的健康监控（这可能需要一个 APM 的监控）。 如果是宠物模式，需要：服务的重新启动和服务的监控报警（如果重试恢复不成功，需要人工介入）。 如果是奶牛模式，需要：服务的资源申请，服务的自动化部署，服务发现的注册，以及服务的流量调度。 作业和应用调度； 作业工作流编排； 服务编排； 一个好的操作系统需要能够通过一定的机制把一堆独立工作的进程给协同起来。在分布式的服务调度中，这个工作叫做 Orchestration，即“编排”。 注意，ESB 的服务编制叫 Choreography，与Orchestration是不一样的，前者较为重量级。 后者的意思是所有服务的交互是通过一个总控制者来进行编排。 前者的意思是在各自完成专属自己的工作的基础上考虑怎样互相协作。 一般来说，可以通过API Gateway或者一个简单的消息队列来做相应的编排工作，如Zuul、Ingress相似等。 关于服务的编排会直接导致一个服务编排的工作流引擎中间件的产生。 ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/0.html/:6:4","tags":["分布式架构","note","架构本质"],"title":"分布式系统架构的本质note","uri":"/posts/distributed_structure/structure/0.html/"},{"categories":["分布式架构"],"content":"分布式系统关键技术：流量和数据调度 流量调度与服务治理需要区分开来，它们属于不同层面： 服务治理是内部系统的事，而流量调度可以是内部的，更是外部接入层的事。 服务治理是数据中心的事，而流量调度的最优应该是数据中心之外的事即边缘计算，在类似于 CDN 上完成的事。 流量调度的主要功能 对于流量调度系统来说应具有的主要功能： 依据系统运行的情况，自动地进行流量调度，在无需人工干预的情况下，提升整个系统的稳定性； 让系统应对爆品等突发事件时，在弹性计算扩缩容的较长时间窗口内或底层资源消耗殆尽的情况下，保护系统平稳运行。 流量调度系统还可以完成以下几方面的事情： 服务流控。服务发现、服务路由、服务降级、服务熔断、服务保护等。 流量控制。负载均衡、流量分配、流量控制、异地灾备（多活）等。 流量管理。协议转换、请求校验、数据缓存、数据计算等。 所有这些都是一个API GateWay应该做的事情。 流量调度的关键技术 调度流量首先需要抗住流量，而且需要一些比较轻量的业务逻辑。 一个好的API GateWay应该具备以下关键技术： 高性能。高性能的技术，也需要使用高性能的语言。 抗流量。关键点在于集群技术，而该技术的关键点为结点之间的数据共享。且因部署在广域网上也需要集群的分组技术。 业务逻辑。需要有简单的业务逻辑。 服务化。能够通过 Admin API 来不停机地管理配置变更。 状态数据调度 状态服务会保存数据，且不能丢失，需要随服务一起调度。 一般通过转移到第三方服务使其变为无状态的服务，但数据存储结点在Scale上比较困难，所以成了一个单点的瓶颈。 分布式事务一致性问题 数据 replication 则会带来数据一致性的问题，进而对性能带来严重的影响。 解决数据不丢失或者说保证数据高可用性的问题，本质就是通过数据冗余来解决。 解决数据副本间的一致性问题，一般有以下方案： M-S方案 M-M方案 两阶段和三阶段提交方案 Paxos方案 通过业务补偿的方式解决数据一致性问题，即是在应用层上解决事务问题。 若要在数据层解决事务问题，则Paxos算法则可以实现。 数据结点的分布式方案 真正完整解决数据 Scale 问题的应该还是数据结点自身。 只有数据结点自身解决了这个问题才能做到对上层业务层的透明，业务层可以像操作单机数据库一样来操作分布式数据库，这样才能做到整个分布式服务架构的调度。即该问题应该解决在数据存储方。数据存储结果有太多不同的Scheme，如AWS的Aurora、PingCAP的TiDB、MySQLCluster、阿里的OceanBase等。而对于一些需要文件存储的，则需要分布式文件系统的支持， 状态数据调度应该是在 IaaS 层的数据存储解决的问题，而不是在 PaaS 层或者 SaaS 层来解决的。 ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/0.html/:7:0","tags":["分布式架构","note","架构本质"],"title":"分布式系统架构的本质note","uri":"/posts/distributed_structure/structure/0.html/"},{"categories":["分布式架构"],"content":"PaaS平台 ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/0.html/:8:0","tags":["分布式架构","note","架构本质"],"title":"分布式系统架构的本质note","uri":"/posts/distributed_structure/structure/0.html/"},{"categories":["分布式架构"],"content":"软件工程能力体现 提高服务的SLA 主要表现在两个方面： 高可用的系统 自动化的运维 能力和资源重用或复用 其主要表现在两个方面： 软件模块的重用 软件运行环境和资源的重用 “软件抽象的能力”和“软件标准化的能力” 过程的自动化 软件生产流水线 软件运维自动化 软件工程的三个本质与分布式技术点是高度一致的，即下面三个方面的能力： 分布式多层的系统架构 服务化的能力供应 自动化的运维能力 ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/0.html/:8:1","tags":["分布式架构","note","架构本质"],"title":"分布式系统架构的本质note","uri":"/posts/distributed_structure/structure/0.html/"},{"categories":["分布式架构"],"content":"PaaS平台的本质 一个好的PaaS平台应该具有分布式、服务化、自动化部署、高可用、敏捷以及分层开放的特征，并可与IaaS实现良好的联动。 PaaS和传统中间件最大的差别： 服务化是PaaS的本质 软件模块重用，服务治理，对外提供能力是PaaS的本质。 分布式是PaaS的根本特性 多租户隔离、高可用、服务编排是PaaS的基本特性。 自动化是PaaS的灵魂 自动化部署安装运维，自动化伸缩调度是PaaS的关键。 ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/0.html/:8:2","tags":["分布式架构","note","架构本质"],"title":"分布式系统架构的本质note","uri":"/posts/distributed_structure/structure/0.html/"},{"categories":["分布式架构"],"content":"PaaS平台的总体架构 一个完整的PaaS平台会包括以下几个部分： PaaS调度层 – 主要是PaaS的自动化和分布式对于高可用高性能的管理。 PaaS能力服务层 – 主要是PaaS真正提供给用户的服务和能力。 PaaS的流量调度 – 主要是与流量调度相关的东西，包括对高并发的管理。 PaaS的运营管理 – 软件资源库、软件接入、认证和开放平台门户。 PaaS的运维管理 – 主要是DevOps相关的东西。 ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/0.html/:8:3","tags":["分布式架构","note","架构本质"],"title":"分布式系统架构的本质note","uri":"/posts/distributed_structure/structure/0.html/"},{"categories":["分布式架构"],"content":"PaaS平台的生产和运维 ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/0.html/:8:4","tags":["分布式架构","note","架构本质"],"title":"分布式系统架构的本质note","uri":"/posts/distributed_structure/structure/0.html/"},{"categories":["分布式架构"],"content":"小结 为了构建分布式系统，我们面临的主要问题如下： 分布式系统的硬件故障发生率更高，故障发生是常态，需要尽可能地将运维流程自动化。 需要良好地设计服务，避免某服务的单点故障对依赖它的其他服务造成大面积影响。 为了容量的可伸缩性，服务的拆分、自治和无状态变得更加重要，可能需要对老的软件逻辑做大的修改。 老的服务可能是异构的，此时需要让它们使用标准的协议，以便可以被调度、编排，且互相之间可以通信。 服务软件故障的处理也变得复杂，需要优化的流程，以加快故障的恢复。 为了管理各个服务的容量，让分布式系统发挥出最佳性能，需要有流量调度技术。 分布式存储会让事务处理变得复杂；在事务遇到故障无法被自动恢复的情况下，手动恢复流程也会变得复杂。 测试和查错的复杂度增大。 系统的吞吐量会变大，但响应时间会变长。 为了解决这些问题，我们深入了解了以下这些解决方案： 需要有完善的监控系统，以便对服务运行状态有全面的了解。 设计服务时要分析其依赖链；当非关键服务故障时，其他服务要自动降级功能，避免调用该服务。 重构老的软件，使其能被服务化；可以参考SOA和微服务的设计方式，目标是微服务化；使用Docker和Kubernetes来调度服务。 为老的服务编写接口逻辑来使用标准协议，或在必要时重构老的服务以使得它们有这些功能。 自动构建服务的依赖地图，并引入好的处理流程，让团队能以最快速度定位和恢复故障。 使用一个API Gateway，它具备服务流向控制、流量控制和管理的功能。 事务处理建议在存储层实现；根据业务需求，或者降级使用更简单、吞吐量更大的最终一致性方案，或者通过二阶段提交、Paxos、Raft、NWR等方案之一，使用吞吐量小的强一致方案。 通过更真实地模拟生产环境，乃至在生产环境中做灰度发布，从而增加测试强度；同时做充分的单元测试和集成测试以发现和消除缺陷；最后，在服务故障发生时，相关的多个团队同 时上线自查服务状态，以最快地定位故障原因。 通过异步调用来减少对短响应时间的依赖；对关键服务提供专属硬件资源，并优化软件逻辑以缩短响应时间。 ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/0.html/:9:0","tags":["分布式架构","note","架构本质"],"title":"分布式系统架构的本质note","uri":"/posts/distributed_structure/structure/0.html/"},{"categories":["分布式架构"],"content":"弹力设计note 以下为左耳听风 记录的笔记。 认识故障和弹力设计 ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/6.html/:0:0","tags":["分布式架构","弹力设计","note"],"title":"弹力设计note","uri":"/posts/distributed_structure/structure/6.html/"},{"categories":["分布式架构"],"content":"系统可用性测量 分布式系统的容错设计，在英文中又叫Resiliency（弹力），即系统在不健康甚至出错的情况下能够承受住或者能够恢复的能力。 系统可用性公式： $$ Avaliability = \\frac{MTTF}{MTTF+MTTR} $$ MTTF(Mean TIme To Failure)：平均故障前的时间，即系统平均能够正常运行多长时间才发生一次故障。系统的可靠性越高，MTTF越长。 MTTR(Mean Time To Recovery)：平均修复时间，即从故障出现到故障修复的这段时间。 根据公式来说，如果要提高可靠性，要么提高系统的无故障时间，要么减少系统的故障恢复时间。 ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/6.html/:1:0","tags":["分布式架构","弹力设计","note"],"title":"弹力设计note","uri":"/posts/distributed_structure/structure/6.html/"},{"categories":["分布式架构"],"content":"故障原因 工业界中会把服务不可用的因素分成两种：一种是有计划的，一种是无计划的。 无计划的宕机原因 系统级故障，包括主机、操作系统、中间件、数据库、网络、电源以及外围设备； 数据和中介的故障，包括人员操作、硬盘故障、数据混乱； 自然灾害、认为破坏、以及供电问题等； 有计划的宕机原因 日常任务：备份和容量规划，用户和安全管理，后台批处理应用； 运维相关：数据库维护、应用维护、中间件维护、操作系统维护、网络维护； 升级相关：数据库、应用、中间件、操作系统、网络，包括硬件升级； 总结来说，一共有以下几类问题： 网络问题； 性能问题； 安全问题； 运维问题； 管理问题； 硬件问题； ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/6.html/:2:0","tags":["分布式架构","弹力设计","note"],"title":"弹力设计note","uri":"/posts/distributed_structure/structure/6.html/"},{"categories":["分布式架构"],"content":"故障不可避免 我们需要意识到： 故障是正常的，而且是常见的； 故障是不可预测突发的，而且相当难缠； 不要尝试去避免故障，而是要把处理故障的代码当成正常的功能做在架构里写在代码里。 弹力设计就是要降低MTTR即故障修复时间： 一方面，在好的情况下，该设计对于我们的用户和内部运维来说是完全透明的，系统自动修复不需要人的干预； 另一方面，若修复不了，系统能够自我保护，而不让事态变得更糟糕； 隔离设计 在分布式软件架构中我们需通过隔离设计来让出现的故障得到隔离，从而不让故障“蔓延“。 一般来说对于系统的分离有两种方式： 以服务的种类来做分离； 以用户来做分离； ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/6.html/:3:0","tags":["分布式架构","弹力设计","note"],"title":"弹力设计note","uri":"/posts/distributed_structure/structure/6.html/"},{"categories":["分布式架构"],"content":"按服务的种类来做分离 一般最常见的架构就是将每个服务实例包括接入层、应用层、数据层都完全隔离开来，这种隔离下在物理来说一个板块的故障就不会影响另一个板块。但是也会存在以下的问题： 若需要调用其他实例的服务和数据，一旦同时调用或者调用过多就不可避免就会降低性能（响应时间，而不是吞吐量）； 若要降数据都抽取到一个数据仓库中进行计算，也会增加数据合并的复杂度； 业务逻辑若需要跨板块则故障会导致流程无法进行，严重会导致整体业务故障； 跨板块交互复杂情况下就需要类似于消息队列中间件来交互各个板块的数据和信息交换； 多个板块可能也会出现分布式事务的问题； ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/6.html/:4:0","tags":["分布式架构","弹力设计","note"],"title":"弹力设计note","uri":"/posts/distributed_structure/structure/6.html/"},{"categories":["分布式架构"],"content":"按用户的请求来做分离 将用户分成不同的组，并把后端的同一个服务根据这些不同的组分成不同的实例，这样让同一个服务对于不同的用户进行冗余和隔离，即所谓的“多租户”模式。 对于“多租户”架构来说，会引入系统设计的复杂度，一方面若完全隔离则资源使用就会较为浪费，若共享的话则会导致程序设计的复杂度增加。 通常来说多租户的方案有以下三种： 完全独立的设计。每个租户有自己完全独立的服务和数据； 独立的数据分区，共享的服务。多租户的服务是共享的，但数据是分开隔离的； 共享的服务，共享的数据分区。每个租户的数据和服务都是共享的； ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/6.html/:5:0","tags":["分布式架构","弹力设计","note"],"title":"弹力设计note","uri":"/posts/distributed_structure/structure/6.html/"},{"categories":["分布式架构"],"content":"隔离设计的重点 做好隔离设计，我们需要有如下一些设计考量： 认真分析业务需求和系统分析，定义好隔离业务的大小和粒度； 需要考虑系统的复杂度、成本、性能、资源使用的问题，找到合适的均衡方案或是分布实施的方案； 隔离模式需要配置一些高可用、重试、异步、消息中间件、流控、熔断等设计模式的方式配套使用； 分布式系统中运维复杂度的提升需要自动化运维工具和系统； 服务的监控系统； 异步通讯设计 通讯一般来说分同步和异步两种。 同步调用虽然让系统间只耦合于接口，且实施性也会比异步调用高，但也有以下问题： 整个同步调用链的性能会由最慢的那个服务所决定； 同步调用会导致所有参与方拥有相同的等待时间； 同步调用只能是一对一的情况，较难做到一对多的通讯方式； 同步调用链只要有一个地方出现故障就会导致整条链出现故障； ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/6.html/:6:0","tags":["分布式架构","弹力设计","note"],"title":"弹力设计note","uri":"/posts/distributed_structure/structure/6.html/"},{"categories":["分布式架构"],"content":"异步通讯的三种方式 请求响应式 通过订阅的方式 通过broker的方式 分布式系统的服务设计是需要向无状态服务努力的，其中第一种方式服务是有状态的，而其他方式可让服务间的依赖只有事件，即事件驱动架构。 事件驱动方式的优势有如下： 服务间是平等的即无依赖且每个服务高度可崇勇并可被替换的； 服务的开发测试运维以及故障处理都是高度隔离的； 服务件通过事件关联即不会出现相互阻塞的情况； 服务之间增加一些Adapter较为容易； 各个服务可按自己的处理速度进行； 但事件驱动的架构也有一些不好的地方： 业务流程不再那么明显且较难管理，整个架构变得复杂； 事件可能会乱序，需要很好地管理一个状态机的控制来解决； 事件处理变得复杂，需要保证一致性等问题； ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/6.html/:7:0","tags":["分布式架构","弹力设计","note"],"title":"弹力设计note","uri":"/posts/distributed_structure/structure/6.html/"},{"categories":["分布式架构"],"content":"异步通讯的设计重点 首先我们要明白为什么要异步通讯的原因： 解耦服务间的依赖，比如通过Broker机制； 解耦的目的是让各个服务的隔离性更好，如故障隔离等； 吞吐量变得均匀即可达到削峰的目的； 服务相对独立会在运维管理上面较为容易； 设计异步通信的时候需要注意以下事宜： 用于异步通讯的中间件需要设计成高可用且不丢消息的，也需要注意消息的顺序问题； 异步通讯导致业务处理流程不那么直观，需要有相关的服务消息跟踪机制，否则出现问题后不容易调试； 服务件只通过消息交互，业务状态最好由一个总控方来管理维护整个业务流程的状态变迁逻辑； 消息传递中需要处理方有幂等的处理； ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/6.html/:8:0","tags":["分布式架构","弹力设计","note"],"title":"弹力设计note","uri":"/posts/distributed_structure/structure/6.html/"},{"categories":["分布式架构"],"content":"服务限流的通用方案 ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/2.html/:0:0","tags":["分布式架构","服务限流"],"title":"服务限流的通用方案","uri":"/posts/distributed_structure/structure/2.html/"},{"categories":["分布式架构"],"content":"服务限流场景 限流常见使用层面： 用户网络层：突发的流量场景如热点事件流量，恶意刷流，竞对爬虫等； 内部应用层：上游服务的异常调用，脚本异常请求，失败重试策略造成的流量突发； ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/2.html/:1:0","tags":["分布式架构","服务限流"],"title":"服务限流的通用方案","uri":"/posts/distributed_structure/structure/2.html/"},{"categories":["分布式架构"],"content":"实现方式 以下实现方式的代码示例参考：repo地址 ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/2.html/:2:0","tags":["分布式架构","服务限流"],"title":"服务限流的通用方案","uri":"/posts/distributed_structure/structure/2.html/"},{"categories":["分布式架构"],"content":"计数器 比较常见且简单的方式即为计数器方式，使用某一公共存储变量进行计数，需要注意的是在计数时保证原子性。 使用场景：适用于做API限流或者根据IP做粒度控制等； 局限：由于计数一般为定速所以对于更细粒度时间控制能力较为有限； ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/2.html/:2:1","tags":["分布式架构","服务限流"],"title":"服务限流的通用方案","uri":"/posts/distributed_structure/structure/2.html/"},{"categories":["分布式架构"],"content":"漏斗桶限流 漏斗桶核心在于它是匀速的，当桶满了，新流量过来就会被限流。 使用场景：与计数器限流相比限流过后的流量还有机会流入而不是直接舍弃，适合于频率控制操作； 局限：若短时有大量突发请求即使负载压力不大，但请求仍需要在队列处等待处理； ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/2.html/:2:2","tags":["分布式架构","服务限流"],"title":"服务限流的通用方案","uri":"/posts/distributed_structure/structure/2.html/"},{"categories":["分布式架构"],"content":"令牌桶限流 令牌桶相对于漏斗桶控制时间粒度和应对突然流量的能力更加优秀。通过匀速往桶里放令牌，控制桶最大容量和放入令牌速率。请求若拿到了令牌则可以通过，反之则被限流。而令牌是可以积累的，说明能够应对流量不同大小的场景。 使用场景：应用较为广泛，如java中的guava就有实现； ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/2.html/:2:3","tags":["分布式架构","服务限流"],"title":"服务限流的通用方案","uri":"/posts/distributed_structure/structure/2.html/"},{"categories":["分布式架构"],"content":"参考 分布式高并发服务限流实现方案 ","date":"2022-01-29","objectID":"/posts/distributed_structure/structure/2.html/:3:0","tags":["分布式架构","服务限流"],"title":"服务限流的通用方案","uri":"/posts/distributed_structure/structure/2.html/"},{"categories":["通用技术"],"content":"程序中的错误处理note ","date":"2022-01-29","objectID":"/posts/common_tech/error/0.html/:0:0","tags":["错误处理","通用技术"],"title":"程序中的错误处理note","uri":"/posts/common_tech/error/0.html/"},{"categories":["通用技术"],"content":"错误返回和异常捕捉 错误可以分为三个大类： 资源的错误。当我们的代码去请求一些资源时导致的错误，比如打开一个没有权限的文件，写文件时出现的写错误，发送文件到网络端发现网络故障的错误，等等。这一类错误属于程序运行环境的问题。对于这类错误，有的我们可以处理，有的我们则无法处理。比如，内存耗尽、栈溢出或是一些程序运行时关键性资源不能满足等等这些情况，我们只能停止运行，甚至退出整个程序。 程序的错误。比如：空指针、非法参数等。这类是我们自己程序的错误，我们要记录下来，写入日志，最好触发监控系统报警。 用户的错误。比如：Bad Request、Bad Format 等这类由用户不合法输入带来的错误。这类错误基本上是在用户的 API 层上出现的问题。比如，解析一个 XML 或 JSON 文件，或是用户输入的字段不合法之类的。对于这类问题，我们需要向用户端报错，让用户自己处理修正他们的输入或操作。然后，我们正常执行，但是需要做统计，统计相应的错误率，这样有利于我们改善软件或是侦测是否有恶意的用户请求。 我们可以看到，这三类错误中，有些是我们希望杜绝发生的，比如程序的 Bug，有些则是我们杜绝不了的，比如用户的输入。而对于程序运行环境中的一些错误，则是我们希望可以恢复的。也就是说，我们希望可以通过重试或是妥协的方式来解决这些环境的问题，比如重建网络连接，重新打开一个新的文件。 所以，是不是我们可以这样来在逻辑上分类： 对于我们并不期望会发生的事，我们可以使用异常捕捉； 对于我们觉得可能会发生的事，使用返回码。 总之，“报错的类型” 和 “错误处理” 是紧密相关的，错误处理方法多种多样，而且会在不同的层面上处理错误。有些底层错误就需要自己处理掉（比如：底层模块会自动重建网络连接），而有一些错误需要更上层的业务逻辑来处理（比如：重建网络连接不成功后只能让上层业务来处理怎么办？降级使用本地缓存还是直接报错给用户？）。 所以，不同的错误类型再加上不同的错误处理会导致我们代码组织层面上的不同，从而会让我们使用不同的方式。也就是说，使用错误码还是异常捕捉主要还是看我们的错误处理流程以及代码组织怎么写会更清楚。 ","date":"2022-01-29","objectID":"/posts/common_tech/error/0.html/:1:0","tags":["错误处理","通用技术"],"title":"程序中的错误处理note","uri":"/posts/common_tech/error/0.html/"},{"categories":["通用技术"],"content":"异步编程及实践 统一分类的错误字典。无论你是使用错误码还是异常捕捉，都需要认真并统一地做好错误的分类。最好是在一个地方定义相关的错误。比如，HTTP 的 4XX 表示客户端有问题，5XX 则表示服务端有问题。也就是说，你要建立一个错误字典。 同类错误的定义最好是可以扩展的。这一点非常重要，而对于这一点，通过面向对象的继承或是像 Go 语言那样的接口多态可以很好地做到。这样可以方便地重用已有的代码。 定义错误的严重程度。比如，Fatal 表示重大错误，Error 表示资源或需求得不到满足，Warning 表示并不一定是个错误但还是需要引起注意，Info 表示不是错误只是一个信息，Debug 表示这是给内部开发人员用于调试程序的。 错误日志的输出最好使用错误码，而不是错误信息。打印错误日志的时候，应该使用统一的格式。但最好不要用错误信息，而应使用相应的错误码，错误码不一定是数字，也可以是一个能从错误字典里找到的一个唯一的可以让人读懂的关键字。这样，会非常有利于日志分析软件进行自动化监控，而不是要从错误信息中做语义分析。比如：HTTP 的日志中就会有 HTTP 的返回码，如：404。但我更推荐使用像PageNotFound这样的标识，这样人和机器都很容易处理。 忽略错误最好有日志。不然会给维护带来很大的麻烦。 对于同一个地方不停的报错，最好不要都打到日志里。不然这样会导致其它日志被淹没了，也会导致日志文件太大。最好的实践是，打出一个错误以及出现的次数。 不要用错误处理逻辑来处理业务逻辑。也就是说，不要使用异常捕捉这样的方式来处理业务逻辑，而是应该用条件判断。如果一个逻辑控制可以用 if - else 清楚地表达，那就不建议使用异常方式处理。异常捕捉是用来处理不期望发生的事情，而错误码则用来处理可能会发生的事。 对于同类的错误处理，用一样的模式。比如，对于null对象的错误，要么都用返回 null，加上条件检查的模式，要么都用抛 NullPointerException 的方式处理。不要混用，这样有助于代码规范。 尽可能在错误发生的地方处理错误。因为这样会让调用者变得更简单。 向上尽可能地返回原始的错误。如果一定要把错误返回到更高层去处理，那么，应该返回原始的错误，而不是重新发明一个错误。 处理错误时，总是要清理已分配的资源。这点非常关键，使用 RAII 技术，或是 try-catch-finally，或是 Go 的 defer 都可以容易地做到。 不推荐在循环体里处理错误。这里说的是 try-catch，绝大多数的情况你不需要这样做。最好把整个循环体外放在 try 语句块内，而在外面做 catch。 不要把大量的代码都放在一个 try 语句块内。一个 try 语句块内的语句应该是完成一个简单单一的事情。 为你的错误定义提供清楚的文档以及每种错误的代码示例。如果你是做 RESTful API 方面的，使用 Swagger 会帮你很容易搞定这个事。 对于异步的方式，推荐使用 Promise 模式处理错误。对于这一点，JavaScript 中有很好的实践。 对于分布式的系统，推荐使用 APM 相关的软件。尤其是使用 Zipkin 这样的服务调用跟踪的分析来关联错误。 ","date":"2022-01-29","objectID":"/posts/common_tech/error/0.html/:2:0","tags":["错误处理","通用技术"],"title":"程序中的错误处理note","uri":"/posts/common_tech/error/0.html/"},{"categories":["Go"],"content":"Go并发编程实战课笔记—Atomic 以下大多为鸟窝大佬的Go 并发编程实战课 中摘录的笔记 代码repo ","date":"2022-01-29","objectID":"/posts/golang/concurrent/0.html/:0:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Atomic","uri":"/posts/golang/concurrent/0.html/"},{"categories":["Go"],"content":"原子操作基础 在并发编程中，很多场景中使用并发原语较为复杂，而原子操作可以更容易实现底层优化。 因为原子操作就是最小的粒子不可分割，并不会像其他操作一样产生竞争的逻辑。由原子操作进行组合就能够实现比较复杂的指令或者操作。 CPU提供了基础的原子操作，但不同的CPU架构甚至不同的版本提供的原子操作的指令是不同的，所以要用一种编程语言实现支持不同架构的原子操作是相当有难度的。 在golang中将更底层的不同的架构下的实现封装成atomic包，提供了一个通用的原子操作API，包括了修改类型的原子操作（read-modify-write|RMW）和加载存储类型的原子操作（Load|Store）。 不同的架构中虽然代码一样，但产生的编译指令却是不同的，比如看起来貌似是一个原子操作但实际上却不是。 我们利用下面这段代码在不同的架构中通过go tool compile来分析其编译指令观察。 package main // 不同架构下相同的代码进行编译测试 // 观察其编译指令是否为原子操作 const x int64 = 1 + 1\u003c\u003c33 func main(){ var i = x _ = i } GOOS=linux GOARCH=386的架构编译 目前Darwin架构不支持386的架构 GOOS=linux GOARCH=amd64的架构编译 可以看到在386架构是拆分成两条赋值语句来执行，amd64架构中是一条指令来执行。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/0.html/:1:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Atomic","uri":"/posts/golang/concurrent/0.html/"},{"categories":["Go"],"content":"应用场景 我们在某些场景可以使用Mutex等并发原语进行优化，虽然可以解决问题，但是这些并发原语的逻辑比较复杂且对性能有一定的影响，但我们使用atomic的一些方法时可以实现更底层的部分优化。 比如下面场景： 通过标志变量来判断某种状态 考虑到读取和修改变量在并发时会产生竞争导致线程不安全，我们可以利用并发原语Mutex进行加锁保证互斥来解决问题，但此场景并不涉及到资源复杂的竞争逻辑，我们就可以使用atomic原子操作来操作该变量进行解决。 func BenchmarkMutex(b *testing.B){ var k int64 = 1 l:=\u0026sync.Mutex{} var w sync.WaitGroup w.Add(10) b.ResetTimer() for i:=0;i\u003c10;i++{ go func() { defer w.Done() for j:=0;j\u003c10000;j++{ l.Lock() k = k + 1 l.Unlock() } }() } w.Wait() } func BenchmarkAtomic(b *testing.B){ var k int64 = 1 var w sync.WaitGroup w.Add(10) b.ResetTimer() for i:=0;i\u003c10;i++{ go func() { defer w.Done() for j:=0;j\u003c10000;j++{ atomic.AddInt64(\u0026k,1) } }() } w.Wait() } 通过性能评测后可以得到如下结果： 有时候我们可以使用atomic实现自己定义的基本并发原语，在网上也能够搜集到相关的实现方法，且有些基本并发原语的底层就是基于通过atomic的方法实现的。 除此之外，atomic原子操作也是实现lock-free数据结构的基石，lock-free就是避免互斥锁来优化并发的性能。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/0.html/:2:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Atomic","uri":"/posts/golang/concurrent/0.html/"},{"categories":["Go"],"content":"atomic提供的方法 因为golang目前没有泛型的特性，导致为了支持不同类型会分别为每种类型提供的相同的方法。 atomic操作的对象是一个地址，需要把可寻址的变量的地址作为参数传递给方法，而不是把变量的值传递给方法。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/0.html/:3:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Atomic","uri":"/posts/golang/concurrent/0.html/"},{"categories":["Go"],"content":"Add Add方法就是给第一个参数地址中的值增加一个delta值。 对于有符号的整数来说delta可以为负数，但对于无符号类型的没有提供减法操作，可以利用计算机补码的规则把减法变成加法。 atomic.AddUint32(\u0026i,^uint32(0)) ","date":"2022-01-29","objectID":"/posts/golang/concurrent/0.html/:3:1","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Atomic","uri":"/posts/golang/concurrent/0.html/"},{"categories":["Go"],"content":"CAS func CompareAndSwapInt32(addr *int32,old,new int32)(swapped bool) 即判断相等才替换。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/0.html/:3:2","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Atomic","uri":"/posts/golang/concurrent/0.html/"},{"categories":["Go"],"content":"Swap 使用Swap方法替换后返回旧值。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/0.html/:3:3","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Atomic","uri":"/posts/golang/concurrent/0.html/"},{"categories":["Go"],"content":"Load 取出addr地址中的值，即使在多处理器、多核、有CPU cache的情况下，此操作也能保证Load是一个原子操作。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/0.html/:3:4","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Atomic","uri":"/posts/golang/concurrent/0.html/"},{"categories":["Go"],"content":"Store 把值存入到指定的addr地址中，即使在多处理器、多核、有CPU cache的情况喜爱，此操作也能保证Store是一个原子操作。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/0.html/:3:5","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Atomic","uri":"/posts/golang/concurrent/0.html/"},{"categories":["Go"],"content":"Value 原子地存取对象类型，但只能存取。常常用在配置变更场景中，如下面代码实例。 package main import ( \"fmt\" \"go.uber.org/atomic\" \"math/rand\" \"sync\" \"time\" ) type Config struct{ AddrName string Addr string Count int32 } func loadNewConfig()Config{ return Config{ AddrName: \"北京\", Addr: \"0.0.0.1\", Count: rand.Int31(), } } func main(){ var config atomic.Value config.Store(loadNewConfig()) var cond = sync.NewCond(\u0026sync.Mutex{}) go func() { for{ time.Sleep(time.Duration(5+rand.Int63n(5))*time.Second) config.Store(loadNewConfig()) cond.Broadcast() } }() go func() { for{ cond.L.Lock() cond.Wait() c := config.Load().(Config) fmt.Printf(\"new config:%+v\\n\",c) cond.L.Unlock() } }() select { } } ","date":"2022-01-29","objectID":"/posts/golang/concurrent/0.html/:3:6","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Atomic","uri":"/posts/golang/concurrent/0.html/"},{"categories":["Go"],"content":"第三方库的扩展 atomic的API较为简单，提供包一级的函数，函数调用起来较为麻烦。 第三方库的扩展中有些提供了面向对象的使用方式，如uber-go/atomic库。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/0.html/:4:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Atomic","uri":"/posts/golang/concurrent/0.html/"},{"categories":["Go"],"content":"Lock-Free Queue atomic常常用来实现Lock-Free的数据结构。下面就通过Lock-Free Queue代码实例来进行了解。 后面可以通过利用Mutex等并发原语来实现同样线程安全的数据结构来进行并发场景下性能的比较。 package main import ( \"sync/atomic\" \"unsafe\" ) type LKQueue struct{ head unsafe.Pointer tail unsafe.Pointer } type node struct{ value interface{} next unsafe.Pointer } func NewLKQueue()*LKQueue{ n:=unsafe.Pointer(\u0026node{}) return \u0026LKQueue{ head: n, tail: n, } } func (q *LKQueue)Enqueue(v interface{}){ n:=\u0026node{value: v} for{ tail:=load(\u0026q.tail) next:=load(\u0026tail.next) if tail == load(\u0026q.tail){ if next == nil{ //尾为空即没有数据入队 if cas(\u0026tail.next,next,n){ //增加到队尾 cas(\u0026q.tail,tail,n) //入队成功，移动尾巴指针 return } }else{ //尾不为空则需要移动尾指针 cas(\u0026q.tail,tail,next) } } } } func (q *LKQueue)Dequeue()interface{}{ for{ head := load(\u0026q.head) tail := load(\u0026q.tail) next := load(\u0026head.next) if head == load(\u0026q.head){ if head == tail{ if next == nil{ //说明为空队列 return nil } // 只是尾指针还没调整，尝试调整它指向下一个 cas(\u0026q.tail,tail,next) }else{ //读取出队的数据 v:=next.value if cas(\u0026q.head,head,next){ return v } } } } } func load(p *unsafe.Pointer)(n *node){ return (*node)(atomic.LoadPointer(p)) } func cas(p *unsafe.Pointer,old,new *node)(ok bool){ return atomic.CompareAndSwapPointer( p,unsafe.Pointer(old),unsafe.Pointer(new)) } ","date":"2022-01-29","objectID":"/posts/golang/concurrent/0.html/:5:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Atomic","uri":"/posts/golang/concurrent/0.html/"},{"categories":["Go"],"content":"Atomic Vs \u0026 我们可能会遇到一个疑问：对一个地址的赋值操作是否为原子操作？ 如果是原子操作的话atomic包存在的意义是什么。 这里总结了一下DaveCheney谈论此问题是给出的解释： 在write的地址的对齐操作中有可能是需要处理器分成两个指令去处理，若执行一个指令，其他就会看到更新了一半的错误数据，这被称为撕裂写（torn write）。所以我们可以认为赋值操作是一个原子操作，这个“原子操作”可以认为是保证数据的完整性。 但在多处理多核的系统中，一个核对地址的更改在更新到主内存中之前，是在多级缓存中存放的，即会产生数据一致性的问题。为处理该类问题使用了内存屏障方式，简单来说就是进行操作时必须要等未完成的操作都被刷新到内存中，并且该操作还会让相关处理器的CPU缓存失效以此来拉取最新的值。 atomic包提供的方法会提供内存屏障的功能，所以atomic不仅可以保证赋值的数据完整性，还能保证多核中的数据可见性。但需要注意的是为了保证数据的一致性，atomic操作也是会降低性能的。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/0.html/:6:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Atomic","uri":"/posts/golang/concurrent/0.html/"},{"categories":["Go"],"content":"Go并发编程实战课笔记—Channel 以下大多为鸟窝大佬的Go 并发编程实战课 中摘录的笔记 代码repo 在channel的发展中关于CSP理论的部分是不可避免的，而Channel的实现就是实现了CSP的思想。 CSP理论讲述了CSP允许使用进程组件来描述系统，它们独立运行，并且只通过消息传递的方式通信。 Channel类型是Go语言内置的类型，无需引入包。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/1.html/:0:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Channel","uri":"/posts/golang/concurrent/1.html/"},{"categories":["Go"],"content":"Channel的应用场景 执行业务处理的goroutine不要通过共享内存的方式通信，而是要通过Channel通信的方式来分享数据。 即“sharing memory by communicating\"，类似于CSP模型的方式。 Channel类型和基本并发原语是有竞争关系的，应用于并发场景，涉及到goroutine之间的通讯，可以提供并发的保护。 总结下来，可以把channel的应用场景分为五种类型： 数据交流 当作并发的buffer或者queue解决类似生产者-消费者问题，且多个goroutine可以并发当作生产者和消费者。 数据传递 一个goroutine将数据交给另一个gorouitne，相当于把数据的拥有权（引用）托付出去。 信号通知 一个goroutine可以将信号（closing、closed、data ready等）传递给另一个或者另一组groutine。 任务编排 可以让一组goroutine按照一定的顺序并发或者串行执行，即编排的功能。 锁机制 利用Channel可以实现互斥锁的机制。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/1.html/:1:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Channel","uri":"/posts/golang/concurrent/1.html/"},{"categories":["Go"],"content":"Channel基本用法 channel分为只能接收、只能发送、既可以接收也可以发送的三种类型。 这个箭头总是射向左边的，元素类型总在最右边； 若箭头指向chan即表示可以往chan中塞数据，若箭头远离chan就表示chan会往外吐数据； chan\u003c- dataType \u003c-chan dataType chan dataType channel根据设置容量分为 buffer chan 和 unbuffered chan。 在 buffer chan 被阻塞时只有在容量已满的时候才会被阻塞，而 unbuffered chan 只有读写都准备好之后才不不会阻塞。 还有需要注意的是 nil 是 chan 的零值，是一种特殊的 chan，对值是 nil 的 chan 的发送接收调用者总是会阻塞。 // send the data ch \u003c- 10 // receive the data x,ok:= \u003c-ch x:= \u003c-ch \u003c-ch 在使用 channel 中我们还可以将 channel 使用于 select 的 case clause，for-range中。 select{ case ch\u003c-i: case v:=\u003c-ch: } for v:=range ch{} for range ch{} // clean the chan ","date":"2022-01-29","objectID":"/posts/golang/concurrent/1.html/:2:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Channel","uri":"/posts/golang/concurrent/1.html/"},{"categories":["Go"],"content":"Channel的实现原理 在此部分中不会贴出相应源码部分，仅讲述实现逻辑思路。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/1.html/:3:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Channel","uri":"/posts/golang/concurrent/1.html/"},{"categories":["Go"],"content":"chand的数据结构 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/1.html/:3:1","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Channel","uri":"/posts/golang/concurrent/1.html/"},{"categories":["Go"],"content":"chan初始化 底层调用makechan实现，根据chan的容量的大小和元素的类型不同，初始化不同的存储空间。最终，针对不同的容量和元素类型，分配不同的对象来初始化 hchan 对象的字段，返回 hchan 对象。 这里需要注意的是在 buffer chan 中元素包含指针和不包含指针中分配内存方式不同，前者是单独分配内存给 buf ，而后者是一段连续的内存给 对象和 buf。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/1.html/:3:2","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Channel","uri":"/posts/golang/concurrent/1.html/"},{"categories":["Go"],"content":"send 在这里我们可以从 send 的各种情况来分析发送的逻辑思路： 往 nil chan 中发送数据 造成死锁阻塞退出 往 close chan 中发送数据 panic退出 往 buffer chan 中发送数据 buffer chan 容量未满 等待队列中有 receiver 优先将数据发送至该 receiver 直接返回 等待队列中无 receiver 将数据发送至 buf 直接返回 buffer chan 容量已满 放入等待队列 sender 中阻塞等待直至被唤醒 往 unbuffered chan 中发送数据 有接收者准备 chan 中拥有数据 阻塞等待至 chan 中数据被读取（及 chan close 情况） chan 中没有数据 将数据发送至 chan 中直接返回 无接收者准备 chan 中拥有数据 造成死锁阻塞退出 chan 中没有数据 造成死锁阻塞退出 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/1.html/:3:3","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Channel","uri":"/posts/golang/concurrent/1.html/"},{"categories":["Go"],"content":"receive 在这里我们可以从 receive 的各种情况来分析发送的逻辑思路： 从 nil chan 中接收数据 造成死锁阻塞退出 从 close chan 中接收数据 buf 则取出数据直接返回 无 buf 则取出类型零值直接返回 从 buffer chan 中接收数据 buffer chan buf 不为空 优先取出 buf 数据直接返回 buffer chan buf 为空 等待队列中无 sender 阻塞等待至 chan 中发送数据（及 chan close 情况） 等待队列中有 sender 接收该 sender 中的数据直接返回 从 unbuffered chan 中接收数据 无数据 阻塞等待至 chan 中发送数据 有数据 接收该数据直接返回 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/1.html/:3:4","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Channel","uri":"/posts/golang/concurrent/1.html/"},{"categories":["Go"],"content":"close 在这里我们可以从 close 的各种情况来分析发送的逻辑思路： nil chan panic not nil chan close chan panic not close chan 把等待队列中的goroutine全部移除并唤醒后返回 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/1.html/:3:5","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Channel","uri":"/posts/golang/concurrent/1.html/"},{"categories":["Go"],"content":"易错场景 使用 channel 中最常见的错误就是 panic 和 grouting 泄漏。 panic的情况共有三种： close the nil chan send the close chan close the close chan 这里提供一套选择的方法： 共享资源的并发访问使用传统并发原语； 复杂的任务编排和消息传递使用 Channel； 消息通知机制使用 Channel，除非只想 signal 一个 goroutine 才使用 Cond； 简单等待所有任务的完成用 WaitGroup，也可以使用 Channel； 需要和 Select 语句结合，使用 Channel； 需要和超时配合，使用 Channel 和 Context； ","date":"2022-01-29","objectID":"/posts/golang/concurrent/1.html/:4:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Channel","uri":"/posts/golang/concurrent/1.html/"},{"categories":["Go"],"content":"Go并发编程实战课笔记—Cond 以下大多为鸟窝大佬的Go 并发编程实战课 中摘录的笔记 代码repo ","date":"2022-01-29","objectID":"/posts/golang/concurrent/2.html/:0:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Cond","uri":"/posts/golang/concurrent/2.html/"},{"categories":["Go"],"content":"Go标准库的Cond Go 标准库提供 Cond 原语的目的是，为等待 / 通知场景下的并发问题提供支持。 Cond通常应用于等待某个条件的一组goroutine，等条件变为true的时候，其中一个goroutine或者所有的goroutine都会被唤醒执行。 开发实践中使用到Cond场景比较少，且Cond场景一般也能用Channel方式实现，所以更多人会选择使用Channel。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/2.html/:1:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Cond","uri":"/posts/golang/concurrent/2.html/"},{"categories":["Go"],"content":"Cond基本用法 标准库中的Cond并发原语初始化的时候需要关联一个Locker接口的实例，一般使用Mutex或者RWMutex。 type Cond func NeWCond(l Locker) *Cond func (c *Cond) Broadcast() //允许调用者Caller唤醒所有等待此Cond的goroutine即清空Cond等待队列中所有的goroutine func (c *Cond) Signal() //允许调用者Caller按等待队列顺序唤醒一个等待此Cond的goroutine func (c *Cond) Wait() //会把调用者Caller放入Cond的等待队列中并阻塞，直到被Signal或者Broadcast方法从等待队列中唤醒。 Go 实现的 sync.Cond 的方法名是 Wait、Signal 和 Broadcast，这是计算机科学中条件变量的通用方法名。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/2.html/:2:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Cond","uri":"/posts/golang/concurrent/2.html/"},{"categories":["Go"],"content":"Cond的实现原理 type Cond struct { noCopy noCopy // 当观察或者修改等待条件的时候需要加锁 L Locker // 等待队列 notify notifyList // 辅助结构，在运行时检查Cond是否被复制使用 checker copyChecker } func NewCond(l Locker) *Cond { return \u0026Cond{L: l} } func (c *Cond) Wait() { c.checker.check() // 增加到等待队列中 t := runtime_notifyListAdd(\u0026c.notify) c.L.Unlock() // 阻塞休眠直到被唤醒 runtime_notifyListWait(\u0026c.notify, t) c.L.Lock() } func (c *Cond) Signal() { c.checker.check() runtime_notifyListNotifyOne(\u0026c.notify) } func (c *Cond) Broadcast() { c.checker.check() runtime_notifyListNotifyAll(\u0026c.notify） } ","date":"2022-01-29","objectID":"/posts/golang/concurrent/2.html/:3:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Cond","uri":"/posts/golang/concurrent/2.html/"},{"categories":["Go"],"content":"使用注意 调用 Wait 的时候没有加锁。 只调用了一次Wait，没有检查等待条件是否满足，结果条件没满足，程序就继续执行了。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/2.html/:4:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Cond","uri":"/posts/golang/concurrent/2.html/"},{"categories":["Go"],"content":"Go并发编程实战课笔记—Context 以下为鸟窝大佬的Go 并发编程实战课 中摘录的笔记 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/3.html/:0:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Context","uri":"/posts/golang/concurrent/3.html/"},{"categories":["Go"],"content":"Go并发编程实战课笔记—Etcd 以下为鸟窝大佬的Go 并发编程实战课 中摘录的笔记 代码repo ETCD 提供了较多的分布式并发原语，比如分布式互斥锁、分布式读写锁、Leader选举。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/4.html/:0:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Etcd","uri":"/posts/golang/concurrent/4.html/"},{"categories":["Go"],"content":"Leader 选举 主从架构的服务节点分为主（Leader、Master）和从（Follower、Slave）两种角色。 主节点通常执行写操作，从节点通常执行读操作，若读写都在主节点而从节点提供备份则主从架构就会退化成主备模式架构。 选主机制就是选择一个节点作为主节点，保证主节点的唯一性从而保证数据的一致性。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/4.html/:1:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Etcd","uri":"/posts/golang/concurrent/4.html/"},{"categories":["Go"],"content":"选举 有以下和选主相关的提供的方法： // 把一个节点选举为主节点且会设置一个值 // 这是一个阻塞方法，在调用它的时候会被阻塞，直到满足下面的三个条件之一，才会取消阻塞。成功当选为主；此方法返回错误；ctx 被取消 func (e *Election) Campaign(ctx context.Context,val string)error // 重新设置 Leader 的值，但是不会重新选主，这个方法会返回新值设置成功或者失败的信息 func (e *Election) Proclaim(ctx context.Context, val string) error // 开始新一次选举。这个方法会返回新的选举成功或者失败的信息 func (e *Election) Resign(ctx context.Context) (err error) ","date":"2022-01-29","objectID":"/posts/golang/concurrent/4.html/:1:1","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Etcd","uri":"/posts/golang/concurrent/4.html/"},{"categories":["Go"],"content":"查询 有以下和查询相关的提供的方法： // 查询当前 Leader 的方法 Leader，如果当前还没有 Leader，就返回一个错误 // 可以使用这个方法来查询主节点信息。 func (e *Election) Leader(ctx context.Context) (*v3.GetResponse, error) // 每次主节点的变动都会生成一个新的版本号，你还可以查询版本号信息（Rev 方法），了解主节点变动情况 func (e *Election) Rev() int64 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/4.html/:1:2","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Etcd","uri":"/posts/golang/concurrent/4.html/"},{"categories":["Go"],"content":"监控 有以下和监控相关的提供的方法： // 监控主节点的变化 // 它会返回一个 chan，显示主节点的变动信息 // 需要注意的是它不会返回主节点的全部历史变动信息，而是只返回最近的一条变动信息以及之后的变动信息 func (e *Election) Observe(ctx context.Context) \u003c-chan v3.GetResponse ","date":"2022-01-29","objectID":"/posts/golang/concurrent/4.html/:1:3","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Etcd","uri":"/posts/golang/concurrent/4.html/"},{"categories":["Go"],"content":"互斥锁 主要关注在分布在不同机器中的不同进程内的 goroutine，如何利用分布式互斥锁来保护共享资源。 在同一时刻，分布式互斥锁在所有节点中只允许其中的一个节点持有锁。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/4.html/:2:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Etcd","uri":"/posts/golang/concurrent/4.html/"},{"categories":["Go"],"content":"Locker 类似于标准库中的 sync.Locker 接口，提供了 Lock/Unlock 的机制： func NewLocker(s *Session,pfx string) sync.Locker ","date":"2022-01-29","objectID":"/posts/golang/concurrent/4.html/:2:1","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Etcd","uri":"/posts/golang/concurrent/4.html/"},{"categories":["Go"],"content":"Mutex Mutex 并没有实现 sync.Locker 接口，它的 Lock/Unlock 方法需要提供一个 context.Context 实例做参数，可以得知在请求锁的时候，你可以设置超时时间，或者主动取消请求。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/4.html/:2:2","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Etcd","uri":"/posts/golang/concurrent/4.html/"},{"categories":["Go"],"content":"读写锁 ETCD 提供的读写锁，可以在分布式环境中的不同的节点使用。 提供的方法是和功能与标准库中的读写锁一致，分别提供了 RLock/RUnlock、Lock/Unlock方法。 如果持有互斥锁或者读写锁的节点意外宕机了，它持有的锁会被释放。这里我们可以关注对应的 session 相关处理。 etcd 提供的读写锁中的读和写按照实现来说是写锁应该比读锁优先级高。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/4.html/:3:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Etcd","uri":"/posts/golang/concurrent/4.html/"},{"categories":["Go"],"content":"Go并发编程实战课笔记—GroupOperation 分组操作：处理一组子任务执行的并发原语 以下为鸟窝大佬的Go 并发编程实战课 中摘录的笔记 代码repo ","date":"2022-01-29","objectID":"/posts/golang/concurrent/5.html/:0:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—GroupOperation","uri":"/posts/golang/concurrent/5.html/"},{"categories":["Go"],"content":"ErrGroup ErrGroup为官方提供的一个同步扩展库。应用场景就是将一个大的任务拆分成几个小任务并发执行。它主要提供的功能包括： 和Context集成； error向上传播，可将子任务的错误传递给Wait的调用者。 该原语底层也是基于WaitGroup实现的。 在使用ErrGroup时我们会用到的三个方法： // 一旦有子任务返回错误或是Wait调用返回则该返回的context会被cancel // 注意当传递ctx参数是一个可以cancel的Context的话则被cancel的时候并不会终止该正在执行的子任务 func WithContext(ctx context.Context)(*Group,context.Context) // 任务函数f若执行成功则返回nil，若不成功则返回error且会cancel那个新的Context // Wait方法只会返回第一个错误 func (g *Group)Go(f func() error) // 阻塞等待至等所有子任务执行完成后才会返回 func (g *Group)Wait()error ","date":"2022-01-29","objectID":"/posts/golang/concurrent/5.html/:1:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—GroupOperation","uri":"/posts/golang/concurrent/5.html/"},{"categories":["Go"],"content":"扩展库 原生库中若无限制地使用 Go 方法则会创建非常多的 goroutine，过多的 goroutine 会带来调度和 GC 的压力，而且也会占用更多的内存资源。 解决此问题的常用手段就是利用 worker pool(goroutine pool)，或者类似利用信号量来控制并行的 goroutine 的数量。 bilibili/errgroup 该库可以使用一个固定数量的 goroutine 处理子任务。 除了可以控制并发 goroutine 的数量，还提供了以下功能： cancel：失败的子任务可以 cancel 所有正在执行任务； recover：会把 panic 的堆栈信息放到 error 中，避免子任务 panic 导致的程序崩溃。 需要注意： 若并发的子任务超过了设置的并发数则需要等到调用者调用 Wait 之后才会执行，而不是 goroutine 空闲则会执行； 若高并发情况下任务数大于设定的 goroutine 的数量，且这些任务被集中加入到 Group 中，该库的处理方式是把子任务加入到数组中，而该数组不是线程安全的。 neilotoole/errgroup 在官方的 ErrGroup 基础上增加了可以控制并发 goroutine 的功能。 新增的 WithContextN 可设置并发的 goroutine 数，以及等待处理的子任务队列的大小。当队列满的时候则调用 Go 方法会被阻塞，直到子任务可以放入到队列中才返回。 facebookgo/errgroup 实际上为标准库 WaitGroup 的扩展，增加了 Wait 方法可返回 error，而且可以包含多个 error。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/5.html/:1:1","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—GroupOperation","uri":"/posts/golang/concurrent/5.html/"},{"categories":["Go"],"content":"其他 Group 并发原语 SizedGroup/ErrSizedGroup SizeGroup 内部使用信号量和 WaitGroup 实现，通过信号量控制并发的 goroutine 数量或者是不控制该数量而控制子任务并发执行时候的数量。 默认情况下，SizedGroup 控制的是子任务的并发数量，而不是 goroutine 的数量。在这种方式下，每次调用 Go 方法都不会被阻塞，而是新建一个 goroutine 去执行。如果想控制 goroutine 的数量，你可以使用 syncs.Preemptive 设置这个并发原语的可选项。 package main import ( \"context\" \"fmt\" \"sync/atomic\" \"time\" \"github.com/go-pkgz/syncs\" ) func main() { // 设置goroutine数是10 swg := syncs.NewSizedGroup(10) // swg := syncs.NewSizedGroup(10, syncs.Preemptive) var c uint32 // 执行1000个子任务，只会有10个goroutine去执行 for i := 0; i \u003c 1000; i++ { swg.Go(func(ctx context.Context) { time.Sleep(5 * time.Millisecond) atomic.AddUint32(\u0026c, 1) }) } // 等待任务完成 swg.Wait() // 输出结果 fmt.Println(c) } ErrSizedGroup 为 SizedGroup 提供了 error 处理的功能，与 Go 官方扩展库的功能一样，如等待子任务完成并返回第一个出现的 error。实现的额外功能如下： 控制并发的 goroutine 数量； 设置了 termOnError 时子任务出现第一个错误的时候会 cancel Context，且后续的 Go 调用会直接返回，Wait 调用者会得到这个错误，默认则返回所有子任务的错误。 SizedGroup 可以把 Context 传递给子任务，可通过 cancel 让子任务中断执行，但ErrSizedGroup 却没有实现。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/5.html/:1:2","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—GroupOperation","uri":"/posts/golang/concurrent/5.html/"},{"categories":["Go"],"content":"gollback 用来处理一组子任务的执行的，解决了 ErrGroup 收集子任务返回结果的问题，且会把结果和 error 信息都返回。 提供的三个方法如下： // 等待所有异步函数执行完才返回，且返回结果顺序和传入顺序一致 func All(ctx context.Context, fns ...AsyncFunc)([]interface{},[]error) // type AsyncFunc func(ctx context.Context) (interface{}, error) // 与 All 方法类似，区别在于只要一个异步函数执行没有错误则立马返回，而不会返回所有子任务的信息 func Race(ctx context.Context, fns ...AsyncFunc)(interface{},error) // 执行一个子任务，若执行失败会尝试一定的次数，若一直不成功就会返回失败错误，若执行成功会立即返回，若 retires 为 0 则会永远尝试直至成功 func Retry(ctx context.Context, retires int, fn AsyncFunc)(interface{},error) ","date":"2022-01-29","objectID":"/posts/golang/concurrent/5.html/:2:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—GroupOperation","uri":"/posts/golang/concurrent/5.html/"},{"categories":["Go"],"content":"Hunch 与 gollback 类似，不过提供的方法更多，且与 rollback 的方法也有一些不同。 提供的方法如下： // type Executable func(context.Context) (interface{}, error) // 会传入一组可执行的函数且返回子任务的执行结果，一旦一个子任务出现错误则会返回错误信息，执行结果为 nil func All(parentCtx context.Context, execs ...Executable) ([]interface{}, error) // 只要有 num 个子任务正常执行完没有错误，该方法就会返回这几个子任务的结果。若出现错误与 All 方法类似 func Take(parentCtx context.Context, num int, execs ...Executable）([]interface{}, error) // 只返回最后 num 个正常执行的、没有错误的子任务的结果。若出现错误与 All 方法类似 func Last(parentCtx context.Context, num int, execs ...Executable) ([]interface{}, error) // 与 gollback Retry 方法一致 func Retry(parentCtx context.Context, retries int, fn Executable) (interface{}, error) // 所有子任务是串行执行的，前一个子任务的执行结果会被当作参数传给下一个子任务，直至所有的任务都完成，返回最后的执行结果 func Waterfall(parentCtx context.Context, execs ...ExecutableInSequence) (interface{}, error) ","date":"2022-01-29","objectID":"/posts/golang/concurrent/5.html/:3:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—GroupOperation","uri":"/posts/golang/concurrent/5.html/"},{"categories":["Go"],"content":"schedgroup 该并发原语与时间处理相关，可为 worker pool 指定任务在某个时间或者某个时间之后执行。 type Group func New(ctx context.Context) *Group // 会在 time.Now()+delay 之后执行函数 func (g *Group) Delay(delay time.Duration, fn func()) // 指定明确的某个时间执行 func (g *Group) Schedule(when time.Time, fn func()) // 阻塞调用者，直到之前安排的所有子任务都执行完才返回 // 调用了 Wait 方法则不能再调用上面两个方法，否则会panic // Wait 方法只能调用一次，若多次调用则会 panic func (g *Group) Wait() error ","date":"2022-01-29","objectID":"/posts/golang/concurrent/5.html/:4:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—GroupOperation","uri":"/posts/golang/concurrent/5.html/"},{"categories":["Go"],"content":"Go并发编程实战课笔记—Map 以下为鸟窝大佬的Go 并发编程实战课 中摘录的笔记 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/6.html/:0:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Map","uri":"/posts/golang/concurrent/6.html/"},{"categories":["Go"],"content":"基本使用方法 Go内建的map类型如下： map[k]v Key类型的K必须是可比较的； Golang中布尔型、整数、浮点数、复数、字符串、指针、Channel、接口都是可比较的，包含可比较元素的struct和数组，而slice、map、函数值都是不可比较的。 若使用struct类型做key，则需要保证struct逻辑不变； Map[key]返回结果可以是一个值或者两个值； map无序的，遍历时迭代的元素顺序是不确定的； 若想保证元素有序则可以使用辅助的数据结构ordermap。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/6.html/:1:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Map","uri":"/posts/golang/concurrent/6.html/"},{"categories":["Go"],"content":"常见错误 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/6.html/:2:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Map","uri":"/posts/golang/concurrent/6.html/"},{"categories":["Go"],"content":"未初始化 在使用map对象之前必须初始化，否则会直接panic。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/6.html/:2:1","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Map","uri":"/posts/golang/concurrent/6.html/"},{"categories":["Go"],"content":"并发读写 Go内建的map对象不是线程安全的，并发读写的时候运行时会检查，遇到并发问题时会导致panic。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/6.html/:2:2","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Map","uri":"/posts/golang/concurrent/6.html/"},{"categories":["Go"],"content":"实现线程安全的map类型 为了实现线程安全的map类型一般常见的方案有两种，分别是： 读写锁 分片加锁 在追求更高性能时分片加锁更好，能够降低锁的粒度来提高此map对象的吞吐，若并发性能要求不高的场景简单加锁更简单。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/6.html/:3:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Map","uri":"/posts/golang/concurrent/6.html/"},{"categories":["Go"],"content":"读写锁 利用内置读写锁的方式来使map对象使用时保证线程安全： type RWMap struct{ sync.RWMutex m map[int]int } ","date":"2022-01-29","objectID":"/posts/golang/concurrent/6.html/:3:1","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Map","uri":"/posts/golang/concurrent/6.html/"},{"categories":["Go"],"content":"分片加锁 在并发编程中，我们会尽量减少锁的使用，因为在大量并发读写情况下锁的竞争会非常激烈，锁时性能下降的万恶之源之一。 所以我们会尽量减少锁的粒度和锁的持有时间，而减少锁的力度常用的方法就是分片，即将一把锁分成几把锁，每一个锁控制一个分片。 在golang中比较著名的分片并发实现时orcaman/concurrent-map。 它默认采用32个分片，GetShard是一个关键的方法，能够根据key计算出分片索引。 增加或者查询的时候首选根据分片索引来得到分片对象，然后对分片对象加锁进行操作。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/6.html/:3:2","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Map","uri":"/posts/golang/concurrent/6.html/"},{"categories":["Go"],"content":"sync.Map Sync.Map并不是用来替换内建的map类型，它只能被应用到一些特殊的场景里，比如在以下场景中使用该类型比加锁的方式性能要更好： 只会增长的缓存系统中，一个key只写入一次而被读很多次； 多个goroutine为不相交的键集读、写和重写键值对； 一般建议是针对于自己的实际场景做性能评测，如果确实能够显著提高性能，再使用sync.Map。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/6.html/:4:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Map","uri":"/posts/golang/concurrent/6.html/"},{"categories":["Go"],"content":"实现 主要优化点如下： 空间换时间。通过冗余的两个数据结构（只读的read字段、可写的dirty字段），来减少锁对性能的影响，对只读字段的操作不需要加锁； 优先从read字段读取、更新、删除，因为对read字段的读取不需要锁； 动态调整。miss次数达到某值将dirty数据提升为read，避免总是从dirty中加锁读取； Double-checking。加锁之后先还要检查read字段，确定真的不存在才操作dirty字段； 延迟删除。删除一个键值只是打标记，只有在提升dirty字段为read字段的时候才清理删除的数据； 具体可以查看相关的源码理解。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/6.html/:4:1","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Map","uri":"/posts/golang/concurrent/6.html/"},{"categories":["Go"],"content":"Go并发编程实战课笔记—Mutex 以下为鸟窝大佬的Go 并发编程实战课 中摘录的笔记 代码repo ![image-20210103003621340](https://img.zhengyua.cn/img/ 同步原语的使用场景： 共享资源。并发读写并发资源，会出现数据竞争的问题，所以需要Mutex、RWMutex等并发原语的保护。 任务编排。需要goroutine按照一定的规律执行，而goroutine之间有相互等待或者依赖的顺序关系，常常使用WaitGroup或者Channel来实现。 消息传递。信息交流以及不同的goroutine之间的线程安全的数据交流，常常使用channel来实现。 互斥锁Mutex提供两个方法Lock和Unlock：进入临界区之前调用Lock方法，退出临界区的时候调用Unlock方法。 利用go race detector检测data race的情况。 有时候采用嵌入字段的方式，来标注不同作用的mutex。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/7.html/:0:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Mutex","uri":"/posts/golang/concurrent/7.html/"},{"categories":["Go"],"content":"源码解析 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/7.html/:1:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Mutex","uri":"/posts/golang/concurrent/7.html/"},{"categories":["Go"],"content":"初版互斥锁 // CAS操作，当时还没有抽象出atomic包 func cas(val *int32, old, new int32) bool func semacquire(*int32) func semrelease(*int32) // 互斥锁的结构，包含两个字段 type Mutex struct { key int32 // 锁是否被持有的标识 sema int32 // 信号量专用，用以阻塞/唤醒goroutine } // 保证成功在val上增加delta的值 func xadd(val *int32, delta int32) (new int32) { for { v := *val if cas(val, v, v+delta) { return v + delta } } panic(\"unreached\") } // 请求锁 func (m *Mutex) Lock() { if xadd(\u0026m.key, 1) == 1 { //标识加1，如果等于1，成功获取到锁 return } semacquire(\u0026m.sema) // 否则阻塞等待 } func (m *Mutex) Unlock() { if xadd(\u0026m.key, -1) == 0 { // 将标识减去1，如果等于0，则没有其它等待者 return } semrelease(\u0026m.sema) // 唤醒其它阻塞的goroutine } 由于Mutex 本身并没有包含持有这把锁的goroutine的信息，Unlock也不会对此进行检查，Unlock方法可以被任意的goroutine调用释放锁，即使是没持有这个互斥锁的goroutine也可以。 所以需要注意在使用 Mutex 的时候，必须要保证 goroutine 尽可能不去释放自己未持有的锁，遵循**“谁申请，谁释放”**的原则。 在真实的实践中，我们使用互斥锁的时候， 很少在一个方法中单独申请锁，而在另外一个方法中单独释放锁，一般都会在同一个方法中获取锁和释放锁。 初版互斥锁有一个问题在于请求锁的goroutine会排队等待获取互斥锁，在高并发场景下不是性能最优的策略，如果能够把锁交给正在占用CPU时间片的goroutine就不用做上下文的切换。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/7.html/:1:1","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Mutex","uri":"/posts/golang/concurrent/7.html/"},{"categories":["Go"],"content":"新来的goroutine竞争 func (m *Mutex) Lock() { // Fast path: 幸运case，能够直接获取到锁 if atomic.CompareAndSwapInt32(\u0026m.state, 0, mutexLocked) { return } awoke := false for { old := m.state new := old | mutexLocked // 新状态加锁 if old\u0026mutexLocked != 0 { new = old + 1\u003c\u003cmutexWaiterShift //等待者数量加一 } if awoke { // goroutine是被唤醒的， // 新状态清除唤醒标志 new \u0026^= mutexWoken } if atomic.CompareAndSwapInt32(\u0026m.state, old, new) {//设置新状态 if old\u0026mutexLocked == 0 { // 锁原状态未加锁 break } runtime.Semacquire(\u0026m.sema) // 请求信号量 awoke = true } } } Fast Path：首先通过CAS检测state字段的标志，判断是否持有锁，若没有则直接获得锁； 下面循环检查，涉及到state不同标志位的操作， func (m *Mutex) Unlock() { // Fast path: drop lock bit. new := atomic.AddInt32(\u0026m.state, -mutexLocked) //去掉锁标志 if (new+mutexLocked)\u0026mutexLocked == 0 { //本来就没有加锁 panic(\"concurrent_programming: unlock of unlocked mutex\") } old := new for { if old\u003e\u003emutexWaiterShift == 0 || old\u0026(mutexLocked|mutexWoken) != 0 return } new = (old - 1\u003c\u003cmutexWaiterShift) | mutexWoken // 新状态，准备唤醒goroutine if atomic.CompareAndSwapInt32(\u0026m.state, old, new) { runtime.Semrelease(\u0026m.sema) return } old = m.state } 若没有锁的情况下进行Unlock则直接panic； 若有锁的情况下有两种情况： 若没有waiter，则直接返回； 若有waiter，且没有被唤醒，则需要唤醒一个等待的waiter； 相对于初版的设计，这次改动主要就是新来的goroutine有机会先获取到锁。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/7.html/:1:2","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Mutex","uri":"/posts/golang/concurrent/7.html/"},{"categories":["Go"],"content":"自旋检查锁释放 若新来的或者是被唤醒的goroutine首次获取不到锁则会进行自旋状态尝试检测锁是否被释放。在尝试一定的自旋次数后，再执行原来的逻辑。 func (m *Mutex) Lock() { // Fast path: 幸运之路，正好获取到锁 if atomic.CompareAndSwapInt32(\u0026m.state, 0, mutexLocked) { return } awoke := false iter := 0 for { // 不管是新来的请求锁的goroutine, 还是被唤醒的goroutine，都不断尝试请求锁 old := m.state // 先保存当前锁的状态 new := old | mutexLocked // 新状态设置加锁标志 if old\u0026mutexLocked != 0 { // 锁还没被释放 if runtime_canSpin(iter) { // 还可以自旋 if !awoke \u0026\u0026 old\u0026mutexWoken == 0 \u0026\u0026 old\u003e\u003emutexWaiterShift atomic.CompareAndSwapInt32(\u0026m.state, old, old|mutexWok awoke = true } runtime_doSpin() iter++ continue // 自旋，再次尝试请求锁 } new = old + 1\u003c\u003cmutexWaiterShift } if awoke { // 唤醒状态 if new\u0026mutexWoken == 0 { panic(\"concurrent_programming: inconsistent mutex state\") } new \u0026^= mutexWoken // 新状态清除唤醒标记 } if atomic.CompareAndSwapInt32(\u0026m.state, old, new) { if old\u0026mutexLocked == 0 { // 旧状态锁已释放，新状态成功持有了锁，直接返回 break } runtime_Semacquire(\u0026m.sema) // 阻塞等待 awoke = true // 被唤醒 iter = 0 } } 对于临界区代码执行非常短的场景来说这是一个很好的优化，因为临界区的代码耗时较短，锁很快就能释放，而抢夺锁的goroutine不用通过休眠唤醒方式等待调度，直接通过自旋可能就获取到了锁。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/7.html/:1:3","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Mutex","uri":"/posts/golang/concurrent/7.html/"},{"categories":["Go"],"content":"解决饥饿 上面我们看到为了性能优化将新来的和唤醒的goroutine参与竞争，必然就会引入饥饿的问题。 Mutex 不能容忍这种事情发生。所以，2016 年 Go 1.9 中 Mutex 增加了饥饿模式，让锁变得更公平，不公平的等待时间限制在1毫秒，并且修复了一个大 Bug：总是把唤醒的 goroutine 放在等待队列的尾部，会导致更加不公平的等待时间。之后，2018 年，Go 开发者将 fast path 和 slow path 拆成独立的方法，以便内联，提高性能。2019 年也有一个 Mutex 的优化，虽然没有 Mutex 做修改，但是，对 Mutex唤醒后持有锁的那个 waiter，调度器可以有更高的优先级去执行，这已经是很细致的性能优化了。 Mutex让每一个goroutine都有机会获取到锁，而且它也尽可能地让等待较长的 goroutine 更有机会获取到锁。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/7.html/:1:4","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Mutex","uri":"/posts/golang/concurrent/7.html/"},{"categories":["Go"],"content":"饥饿模式和正常模式 请求锁时调用的 Lock 方法中一开始是 fast path，这是一个幸运的场景，当前的 goroutine 幸运地获得了锁，没有竞争，直接返回，否则就进入了 lockSlow 方法。 这种设计，方便编译器对Lock方法进行内联，你也可以在程序开发中应用这个技巧。 正常模式下，waiter 都是进入先入先出队列，被唤醒的 waiter 并不会直接持有锁，而是要和新来的 goroutine 进行竞争。新来的 goroutine有先天的优势，它们正在CPU中运行，可能它们的数量还不少，所以在高并发情况下，被唤醒的waiter可能比较悲剧地获取不到锁，这时，它会被插入到队列的前面。如果waite 获取不到锁的时间超过阈值1毫秒，那么，这个Mutex就进入到了饥饿模式。 在饥饿模式下，Mutex的拥有者将直接把锁交给队列最前面的 waiter。新来的goroutine不会尝试获取锁，即使看起来锁没有被持有，它也不会去抢，也不会spin，它会乖乖地加入到等待队列的尾部。 如果拥有 Mutex 的 waiter 发现下面两种情况的其中之一，它就会把这个 Mutex 转换成正常模式 正常模式拥有更好的性能，因为即使有等待抢锁的 waiter，goroutine 也可以连续多次获取到锁。 饥饿模式是对公平性和性能的一种平衡，它避免了某些 goroutine 长时间的等待锁。在饥饿模式下，优先对待的是那些一直在等待的waiter。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/7.html/:1:5","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Mutex","uri":"/posts/golang/concurrent/7.html/"},{"categories":["Go"],"content":"常用错误场景 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/7.html/:2:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Mutex","uri":"/posts/golang/concurrent/7.html/"},{"categories":["Go"],"content":"Lock/Unlock不是成对出现 若Lock/Unlock没有成对出现，则会出现死锁的情况，或者是因为Unlock一个未加锁的Mutex而导致panic。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/7.html/:2:1","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Mutex","uri":"/posts/golang/concurrent/7.html/"},{"categories":["Go"],"content":"Copy已使用的Mutex Package sync的同步原语在使用后是不能复制的，因为Mutex是一个有状态的对象，它的state字段会记录这个锁的状态。 Go在运行时会有死锁的检查机制（checkdead()方法），能够发现死锁的goroutine。 vet工具可以检查同步原语。也可以使用第三方库进行检查。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/7.html/:2:2","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Mutex","uri":"/posts/golang/concurrent/7.html/"},{"categories":["Go"],"content":"重入 可重入锁（递归锁）：当一个线程获取锁时，如果没有其它线程拥有这个锁，那么，这个线程就成功获取到这个锁。之后，如果其它线程再请求这个锁，就会处于阻塞等待的状态。但是，如果拥有这把锁的线程再请求这把锁的话，不会阻塞，而是成功返回。 在通过递归实现一些算法时，调用者不会阻塞或者死锁。 Mutex不是可重入锁。Mutex实现没有记录goroutine是否拥有该锁。 若要实现可重入锁，则需要记录持有该锁的goroutine标识。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/7.html/:2:3","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Mutex","uri":"/posts/golang/concurrent/7.html/"},{"categories":["Go"],"content":"死锁 死锁：两个及以上的进程或线程在执行过程中，因争夺共享资源而处于一种互相等待的状态，若无外部干涉则会无法推进下去。 可引入第三方的锁来依赖这个锁进行业务处理，避免出现死锁。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/7.html/:2:4","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Mutex","uri":"/posts/golang/concurrent/7.html/"},{"categories":["Go"],"content":"Go并发编程实战课笔记—Once 以下为鸟窝大佬的Go 并发编程实战课 中摘录的笔记 代码repo ","date":"2022-01-29","objectID":"/posts/golang/concurrent/8.html/:0:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Once","uri":"/posts/golang/concurrent/8.html/"},{"categories":["Go"],"content":"Once基本概念 Once的使用方法较为简单，可以用来执行且仅仅执行一次动作。 它的使用场景较多应用于单例对象的初始化或者延迟初始化等场景。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/8.html/:1:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Once","uri":"/posts/golang/concurrent/8.html/"},{"categories":["Go"],"content":"Once使用 Once常常用来初始化单例资源，或者并发访问只需初始化一次的共享资源，或者在测试的时候初始化一次测试资源。 只暴露了一个方法 Do，可以多次调用 Do 方法，但是只有第一次调用 Do 方法时 f 参数才会执行，这里的 f 是一个无参数无返回值的函数。 func (o *Once) Do(f func()) 标准库中较为常见的比如在标准库内部cache的实现上就使用了Once初始化Cache资源，包括defaultDir的获取，还有一些测试的时候初始化测试的资源。 其中math/big/sqrt.go种实现的一个数据结构，通过Once封装了一个只初始化一次的值，值得我们学习。 // 值是3.0或者0.0的一个数据结构 var threeOnce struct { sync.Once v *Float } // 返回此数据结构的值，如果还没有初始化为3.0，则初始化 func three() *Float { threeOnce.Do(func() { // 使用Once初始化 threeOnce.v = NewFloat(3.0) }) return threeOnce.v } ","date":"2022-01-29","objectID":"/posts/golang/concurrent/8.html/:2:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Once","uri":"/posts/golang/concurrent/8.html/"},{"categories":["Go"],"content":"实现Once 一个正确的Once实现要使用一个互斥锁，这样初始化的时候如果有并发的 goroutine，就会进入doSlow方法。互斥锁的机制保证只有一goroutine进行初始化，同时利用双检查的机制（double-checking），再次判断o.done 是否为0，如果为0，则是第一次执行，执行完毕后，就将o.done设置为 1，然后释放锁。 type Once struct { done uint32 m sync.Mutex } func (o *Once) Do(f func()) { if atomic.LoadUint32(\u0026o.done) == 0 { o.doSlow(f) } } func (o *Once) doSlow(f func()) { o.m.Lock() defer o.m.Unlock() // 双检查 if o.done == 0 { defer atomic.StoreUint32(\u0026o.done, 1) f() } } ","date":"2022-01-29","objectID":"/posts/golang/concurrent/8.html/:3:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Once","uri":"/posts/golang/concurrent/8.html/"},{"categories":["Go"],"content":"可能出现的错误 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/8.html/:4:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Once","uri":"/posts/golang/concurrent/8.html/"},{"categories":["Go"],"content":"死锁 无限递归导致的死锁。 var once sync.Once once.Do(func() { once.Do(func() { fmt.Println(\"初始化\") }) }) ","date":"2022-01-29","objectID":"/posts/golang/concurrent/8.html/:4:1","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Once","uri":"/posts/golang/concurrent/8.html/"},{"categories":["Go"],"content":"未初始化 如果f方法执行的时候panic，或者f执行初始化资源的时候失败了，这个时候，Once还是会认为初次执行已经成功了，即使再次调用Do方法，也不会再次执行f。 可以通过自己实现一个类似Once的并发原语解决 type ReOnce struct { m sync.Mutex done uint32 } // 传入的函数f有返回值error，如果初始化失败，需要返回失败的error // Do方法会把这个error返回给调用者 func (o *ReOnce) Do(f func() error) error { if atomic.LoadUint32(\u0026o.done) == 1 { //fast path return nil } return o.slowDo(f) } // 如果还没有初始化 func (o *ReOnce) slowDo(f func() error) error { o.m.Lock() defer o.m.Unlock() var err error if o.done == 0 { // 双检查，还没有初始化 err = f() if err == nil { // 初始化成功才将标记置为已初始化 atomic.StoreUint32(\u0026o.done, 1) } } return err } 若想要查询初始化过的情况则还需要一个辅助变量来检查 type AnimalStore struct { once sync.Once inited uint32 } func (a *AnimalStore) Init(){ // 可以被并发调用 a.once.Do(func() { longOperationSetupDbOpenFilesQueuesEtc() atomic.StoreUint32(\u0026a.inited, 1) }) } func (a *AnimalStore) CountOfCats() (int, error) { // 另外一个goroutine if atomic.LoadUint32(\u0026a.inited) == 0 { // 初始化后才会执行真正的业务逻辑 return 0, NotYetInitedError } //Real operation } 但若是有一个Done方法直接使用判断是否初始化过则更为简便 // Once 是一个扩展的sync.Once类型，提供了一个Done方法 type Once struct { sync.Once } // Done 返回此Once是否执行过 // 如果执行过则返回true // 如果没有执行过或者正在执行，返回false func (o *Once) Done() bool { return atomic.LoadUint32((*uint32)(unsafe.Pointer(\u0026o.Once))) == 1 } ","date":"2022-01-29","objectID":"/posts/golang/concurrent/8.html/:4:2","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Once","uri":"/posts/golang/concurrent/8.html/"},{"categories":["Go"],"content":"重复初始化 若类似于想要实现增加一个Reset方法来重新初始化Once，使之能够重新使用。 Go核心开发者lan Lance Taylor提供了一个简单的解决方案即reset时使用new方法重新给sync.Once变量遍历重新赋值一个新的实例。 但下面这种情况就会直接panic： type MuOnce struct { sync.RWMutex sync.Once mtime time.Time vals []string } // 相当于reset方法，会将m.Once重新复制一个Once func (m *MuOnce) refresh() { m.Lock() defer m.Unlock() m.Once = sync.Once{} m.mtime = time.Now() m.vals = []string{m.mtime.String()} } // 获取某个初始化的值，如果超过某个时间，会reset Once func (m *MuOnce) strings() []string { now := time.Now() m.RLock() if now.After(m.mtime) { defer m.Do(m.refresh) // 使用refresh函数重新初始化 } vals := m.vals m.RUnlock() return vals } 我们需要注意从once的实现原理中的lock，若更改了Once的指针则会影响该lock也重新生成即初始化，则defer之后的unlock就会释放未加锁的mutex，所以最后就会报错。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/8.html/:4:3","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Once","uri":"/posts/golang/concurrent/8.html/"},{"categories":["Go"],"content":"Go并发编程实战课笔记—Pool 以下为鸟窝大佬的Go 并发编程实战课 中摘录的笔记 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/9.html/:0:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Pool","uri":"/posts/golang/concurrent/9.html/"},{"categories":["Go"],"content":"Go并发编程实战课笔记—RWMutex 以下为鸟窝大佬的Go 并发编程实战课 中摘录的笔记 代码repo 针对读写场景，即考虑readers-writers问题，同时可能有多个读或者多个写，但只要有一个线程在执行写操作，则其他线程都不能执行写操作，即读锁为共享锁，写锁为排他锁。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/10.html/:0:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—RWMutex","uri":"/posts/golang/concurrent/10.html/"},{"categories":["Go"],"content":"RWMutex标准库 Lock/Unlock：写操作时调用的方法。 RLock/RUlock：读操作时调用的方法。 RLocker：返回调用RLock/RUnlock的Lokcer接口的对象。 当出现明确区分并发读写场景，且有大量的并发读和少量的并发写，可以考虑使用读写锁RWMutex替换Mutex。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/10.html/:1:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—RWMutex","uri":"/posts/golang/concurrent/10.html/"},{"categories":["Go"],"content":"RWMutex的实现原理 Go标准库中的RWMutex是基于Mutex实现的。 readers-writers问题一般有三类，基于对读和写操作的优先级，读写锁的设计和实现也分成三类： Read-preferring：读优先设计提供很高的并发型，但是会在竞争激烈的情况下导致写饥饿。 Writer-preferring：写优先设计针对新来的请求优先保障writer，避免writer饥饿问题。 不指定优先级。 Go标准库中的RWMutex设计是写优先的方案，即一个正在阻塞的Lock调用会排除新的reader请求到锁。 type RWMutex struct { w Mutex // 互斥锁解决多个writer的竞争 writerSem uint32 // writer信号量 readerSem uint32 // reader信号量 readerCount int32 // reader的数量 readerWait int32 // writer等待完成的reader的数量 } const rwmutexMaxReaders = 1 \u003c\u003c 30 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/10.html/:2:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—RWMutex","uri":"/posts/golang/concurrent/10.html/"},{"categories":["Go"],"content":"RLock/Rulock实现 func (rw *RWMutex) RLock() { if atomic.AddInt32(\u0026rw.readerCount, 1) \u003c 0 { // rw.readerCount是负值的时候，意味着此时有writer等待请求锁，因为writer优先 runtime_SemacquireMutex(\u0026rw.readerSem, false, 0) } } func (rw *RWMutex) RUnlock() { if r := atomic.AddInt32(\u0026rw.readerCount, -1); r \u003c 0 { rw.rUnlockSlow(r) // 有等待的writer } } func (rw *RWMutex) rUnlockSlow(r int32) { if atomic.AddInt32(\u0026rw.readerWait, -1) == 0 { // 最后一个reader了，writer终于有机会获得锁了 runtime_Semrelease(\u0026rw.writerSem, false, 1) } } ","date":"2022-01-29","objectID":"/posts/golang/concurrent/10.html/:2:1","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—RWMutex","uri":"/posts/golang/concurrent/10.html/"},{"categories":["Go"],"content":"Lock/Unlock实现 func (rw *RWMutex) Lock() { // 首先解决其他writer竞争问题 rw.w.Lock() // 反转readerCount，告诉reader有writer竞争锁 r := atomic.AddInt32(\u0026rw.readerCount, -rwmutexMaxReaders) + rwmutexMaxReaders // 如果当前有reader持有锁，那么需要等待 if r != 0 \u0026\u0026 atomic.AddInt32(\u0026rw.readerWait, r) != 0 { runtime_SemacquireMutex(\u0026rw.writerSem, false, 0) } } func (rw *RWMutex) Unlock() { // 告诉reader没有活跃的writer了 r := atomic.AddInt32(\u0026rw.readerCount, rwmutexMaxReaders) // 唤醒阻塞的reader们 for i := 0; i \u003c int(r); i++ { runtime_Semrelease(\u0026rw.readerSem, false, 0) } // 释放内部的互斥锁 rw.w.Unlock() } ","date":"2022-01-29","objectID":"/posts/golang/concurrent/10.html/:2:2","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—RWMutex","uri":"/posts/golang/concurrent/10.html/"},{"categories":["Go"],"content":"陷阱 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/10.html/:3:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—RWMutex","uri":"/posts/golang/concurrent/10.html/"},{"categories":["Go"],"content":"不可复制 互斥锁是不可复制的，再加上四个有状态的字段则更加不能复制使用，因为复制记录的状态与本身修改的状态不同步。 解决方案与互斥锁一样，可以借助vet工具进行检查。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/10.html/:3:1","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—RWMutex","uri":"/posts/golang/concurrent/10.html/"},{"categories":["Go"],"content":"重入导致死锁 重入导致的死锁情况较多且很难确认。 writer重入调用Lock时会出现死锁。 func re(l *sync.RWMutex){ l.Lock(); re(l); l.UnLock(); } 若在reader读操作时调用writer写操作，则会形成相互依赖的死锁关系。 环形依赖问题：writer依赖活跃的reader-\u003e活跃的reader依赖新来的reader-\u003e新来的reader依赖writer。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/10.html/:3:2","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—RWMutex","uri":"/posts/golang/concurrent/10.html/"},{"categories":["Go"],"content":"释放未加锁的RWMutex 使用读写锁的时候注意不要遗漏和多余。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/10.html/:3:3","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—RWMutex","uri":"/posts/golang/concurrent/10.html/"},{"categories":["Go"],"content":"Go并发编程实战课笔记—Semaphore 以下为鸟窝大佬的Go 并发编程实战课 中摘录的笔记 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/11.html/:0:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—Semaphore","uri":"/posts/golang/concurrent/11.html/"},{"categories":["Go"],"content":"Go并发编程实战课笔记—SingleFlight\u0026CyclicBarrier 以下为鸟窝大佬的Go 并发编程实战课 中摘录的笔记 代码repo ","date":"2022-01-29","objectID":"/posts/golang/concurrent/13.html/:0:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—SingleFlight\u0026CyclicBarrier","uri":"/posts/golang/concurrent/13.html/"},{"categories":["Go"],"content":"请求合并 SingleFlight SingleFlight 是 Go 开发组提供的一个扩展并发原语，其作用为在处理多个 goroutine 同时调用同一个函数的时候，只让一个 goroutine 去调用这个函数，等待这个 goroutine 返回结果的时候，再把结果返回给这个几个同时调用的 goroutine。这样可以减少并发调用的数量。 与 sync.Once 面对的场景不同，前者是主要用在单次初始化场景中，而 SingleFlight 主要用在合并并发请求的场景中，尤其是缓存场景。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/13.html/:1:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—SingleFlight\u0026CyclicBarrier","uri":"/posts/golang/concurrent/13.html/"},{"categories":["Go"],"content":"实现原理 使用互斥锁 Mutex 和 Map 来实现，其中 Mutex 提供并发时的读写保护， Map 用来保存同一个 key 的正在处理（in flight）的请求。 SingleFlight 的数据结构是 Group，它提供了三个方法： Do：执行一个函数，并返回函数执行的结果。需要提供一个 key，对于同一个 key，在同一时间只有一个在执行，同一个 key 并发的请求会等待。第一个执行的请求返回的结果就是它的返回结果。函数 fn 是一个无参的函数，返回一个结果或者 error，而 Do 方法会返回函数执行的结果或者是 error，shared 会指示 v 是否返回给多个请求； DoChan：类似 Do 方法但是返回 fn 函数结果的 chan 来对结果进行接收； Forget：告诉 Group 忘记这个 key。这样一来，之后这个 key 请求会执行 f，而不是等待前一个未完成的 fn 函数的结果。 // 代表一个正在处理的请求，或者已经处理完的请求 type call struct { wg sync.WaitGroup // 这个字段代表处理完的值，在waitgroup完成之前只会写一次 // waitgroup完成之后就读取这个值 val interface{} err error // 指示当call在处理时是否要忘掉这个key forgotten bool dups int chans []chan\u003c- Result } // group代表一个singleflight对象 type Group struct { mu sync.Mutex // protects m m map[string]*call // lazily initialized } func (g *Group) Do(key string, fn func() (interface{}, error)) (v interface{}, err error, shared bool) { g.mu.Lock() if g.m == nil { g.m = make(map[string]*call) } if c, ok := g.m[key]; ok {//如果已经存在相同的key c.dups++ g.mu.Unlock() c.wg.Wait() //等待这个key的第一个请求完成 return c.val, c.err, true //使用第一个key的请求结果 } c := new(call) // 第一个请求，创建一个call c.wg.Add(1) g.m[key] = c //加入到key map中 g.mu.Unlock() g.doCall(c, key, fn) // 调用方法 return c.val, c.err, c.dups \u003e 0 } func (g *Group) doCall(c *call, key string, fn func() (interface{}, error)) { c.val, c.err = fn() c.wg.Done() g.mu.Lock() if !c.forgotten { // 已调用完，删除这个key // 在默认情况下 forgotten == false delete(g.m, key) } for _, ch := range c.chans { ch \u003c- Result{c.val, c.err, c.dups \u003e 0} } g.mu.Unlock() } ","date":"2022-01-29","objectID":"/posts/golang/concurrent/13.html/:1:1","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—SingleFlight\u0026CyclicBarrier","uri":"/posts/golang/concurrent/13.html/"},{"categories":["Go"],"content":"应用场景 Go 代码库中有两个地方用到了 SingleFlight： net/lookup.go中如果同时有查询同一个 host 的请求，lookupGroup 会把这些请求 merge 到一起，只需要一个请求就可以了； Go 在查询仓库版本信息时，将并发的请求合并成一个请求； func metaImportsForPrefix(importPrefix string, mod ModuleMode, security web.SecurityMode) (*urlpkg.URL, []metaImport, error) { // 使用缓存保存请求结果 setCache := func(res fetchResult) (fetchResult, error) { fetchCacheMu.Lock() defer fetchCacheMu.Unlock() fetchCache[importPrefix] = res return res, nil // 使用 SingleFlight请求 resi, _, _ := fetchGroup.Do(importPrefix, func() (resi interface{}, err error) { fetchCacheMu.Lock() // 如果缓存中有数据，那么直接从缓存中取 if res, ok := fetchCache[importPrefix]; ok { fetchCacheMu.Unlock() return res, nil } fetchCacheMu.Unlock() ...... 其中都涉及到了缓存的问题。用 SingleFlight 来解决缓存击穿问题较为合适，并发的请求可以共享同一个查询结构，且因为为缓存查询不用考虑其幂等性问题。 SingleFilght 时可以通过合并请求的方式降低对下游服务的并发压力，从而提高系统的性能，常常用于缓存系统中。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/13.html/:1:2","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—SingleFlight\u0026CyclicBarrier","uri":"/posts/golang/concurrent/13.html/"},{"categories":["Go"],"content":"循环栅栏 CyclicBarrier 循环栅栏 CyclicBarrier 常常应用于重复进行一组 goroutine 同时执行的场景中。该并发原语允许一组 goroutine 彼此等待，到达一个共同的执行点。同时可以被重复使用。 这其实与 WaitGroup 并发原语的功能较为类似，但是其在重用时需要注意其 panic 的情况，且在处理可重用的多 goroutine 等待同一个执行点的场景的时候，两种并发原语的方法调用的对应关系如下： ","date":"2022-01-29","objectID":"/posts/golang/concurrent/13.html/:2:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—SingleFlight\u0026CyclicBarrier","uri":"/posts/golang/concurrent/13.html/"},{"categories":["Go"],"content":"实现原理 两个初始化方法： func New(parties int) CyclicBarrier //指定循环栅栏参与者的数量 func NewWithAction(parties int, barrierAction func() error) CyclicBarrier //提供一个函数可以在每一次到达执行点的时候执行一次 其中第二个方法中具体的时间点是在最后一个参与者到达之后，但是其它的参与者还未被放行之前，我们可以利用它，做放行之前的一些共享状态的更新等操作。 使用的时候循环栅栏的参与者只需要调用 Await() 方法等待，等所有的参与者到达后再执行下一步，同时循环栅栏的状态恢复到初始的状态，可迎接下一轮同样多的参与者。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/13.html/:2:1","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—SingleFlight\u0026CyclicBarrier","uri":"/posts/golang/concurrent/13.html/"},{"categories":["Go"],"content":"参考 Go 并发编程实战课 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/13.html/:3:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—SingleFlight\u0026CyclicBarrier","uri":"/posts/golang/concurrent/13.html/"},{"categories":["Go"],"content":"Go并发编程实战课笔记—WaitGroup 以下为鸟窝大佬的Go 并发编程实战课 中摘录的笔记 代码repo ","date":"2022-01-29","objectID":"/posts/golang/concurrent/14.html/:0:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—WaitGroup","uri":"/posts/golang/concurrent/14.html/"},{"categories":["Go"],"content":"基本用法 func (wg *WaitGroup) Add(delta int) //设置计数值 func (wg *WaitGroup) Done() //Add(-1) func (wg *WaitGroup) Wait() //阻塞直至计数值为0 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/14.html/:1:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—WaitGroup","uri":"/posts/golang/concurrent/14.html/"},{"categories":["Go"],"content":"WaitGroup实现 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/14.html/:2:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—WaitGroup","uri":"/posts/golang/concurrent/14.html/"},{"categories":["Go"],"content":"数据结构定义 WaitGroup的数据结构定义以及state信息的获取方法如下： type WaitGroup struct { // 避免复制使用的一个技巧，可以告诉vet工具违反了复制使用的规则 noCopy noCopy // 64bit(8bytes)的值分成两段，高32bit是计数值，低32bit是waiter的计数 // 另外32bit是用作信号量的 // 因为64bit值的原子操作需要64bit对齐，但是32bit编译器不支持，所以数组中的元素在不同的 // 总之，会找到对齐的那64bit作为state，其余的32bit做信号量 state1 [3]uint32 } // 得到state的地址和信号量的地址 func (wg *WaitGroup) state() (statep *uint64, semap *uint32) { if uintptr(unsafe.Pointer(\u0026wg.state1))%8 == 0 { // 如果地址是64bit对齐的，数组前两个元素做state，后一个元素做信号量 return (*uint64)(unsafe.Pointer(\u0026wg.state1)), \u0026wg.state1[2] } else { // 如果地址是32bit对齐的，数组后两个元素用来做state，它可以用来做64bit的原子操作 return (*uint64)(unsafe.Pointer(\u0026wg.state1[1])), \u0026wg.state1[0] } } 其中在64位环境和32位环境中的state字段组成是不一样的： 除了方法本身的实现外，需要一些race检查和异常检查的额外代码，避免出现panic。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/14.html/:2:1","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—WaitGroup","uri":"/posts/golang/concurrent/14.html/"},{"categories":["Go"],"content":"Add/Done方法 该方法主要操作的是state的计数部分，通过原子操作来操作该计数值。 func (wg *WaitGroup) Add(delta int) { statep, semap := wg.state() // 高32bit是计数值v，所以把delta左移32，增加到计数上 state := atomic.AddUint64(statep, uint64(delta)\u003c\u003c32) v := int32(state \u003e\u003e 32) // 当前计数值 w := uint32(state) // waiter count if v \u003e 0 || w == 0 { return } // 如果计数值v为0并且waiter的数量w不为0，那么state的值就是waiter的数量。 // 将waiter的数量设置为0，因为计数值v也是0,所以它们俩的组合*statep直接设置为0即可。 *statep = 0 for ; w != 0; w-- { runtime_Semrelease(semap, false, 0) } } // Done方法实际就是计数器减1 func (wg *WaitGroup) Done() { wg.Add(-1) } ","date":"2022-01-29","objectID":"/posts/golang/concurrent/14.html/:2:2","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—WaitGroup","uri":"/posts/golang/concurrent/14.html/"},{"categories":["Go"],"content":"Wait方法 该方法的试下逻辑即不断检查state的值： 若该值为0则说明所有任务完成，调用者不等待直接返回； 若该值大于0则说明还有任务未完成，则调用者变成等待者，加入waiter队列且阻塞自己。 func (wg *WaitGroup) Wait() { statep, semap := wg.state() for { state := atomic.LoadUint64(statep) v := int32(state \u003e\u003e 32) // 当前计数值 w := uint32(state) // waiter的数量 if v == 0 { // 如果计数值为0, 调用这个方法的goroutine不必再等待，继续执行它后面的逻辑即可 return } // 否则把waiter数量加1。期间可能有并发调用Wait的情况，所以最外层使用了一个for循环 if atomic.CompareAndSwapUint64(statep, state, state+1) { // 阻塞休眠等待 runtime_Semacquire(semap) // 被唤醒，不再阻塞，返回 return } } } ","date":"2022-01-29","objectID":"/posts/golang/concurrent/14.html/:2:3","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—WaitGroup","uri":"/posts/golang/concurrent/14.html/"},{"categories":["Go"],"content":"WaitGroup常见错误 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/14.html/:3:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—WaitGroup","uri":"/posts/golang/concurrent/14.html/"},{"categories":["Go"],"content":"计数值设置为负值 WaitGroup 的计数器的值必须大于等于0。 我们在更改这个计数值的时候，WaitGroup 会先做检查，如果计数值被设置为负数，就会导致panic。 一般情况下有两种方法会导致计数器设置为负值： 调用Add的时候传递负数； 调用Done方法次数太多超过了计数值； 使用WaitGroup应该预先确定好WaitGroup的计数值，然后调用相同次数的Done完成相应的任务。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/14.html/:3:1","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—WaitGroup","uri":"/posts/golang/concurrent/14.html/"},{"categories":["Go"],"content":"不期望的Add时机 需要遵循的原则：等所有的Add方法调用之后再调用Wait，否则就可能导致panic或者不期望的结果。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/14.html/:3:2","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—WaitGroup","uri":"/posts/golang/concurrent/14.html/"},{"categories":["Go"],"content":"前一个Wait还没结束就重用WaitGroup 因为WaitGroup是可以重用的，只要将计数值恢复为0值则可以被看作是新创建的WaitGroup被重复使用。但是如果在计数值没有恢复至0值时就重用，就会导致程序panic。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/14.html/:3:3","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—WaitGroup","uri":"/posts/golang/concurrent/14.html/"},{"categories":["Go"],"content":"noCopy：辅助vet检查 noCopy 字段的类型是 noCopy，它只是一个辅助的、用来帮助 vet 检查用的类型: type noCopy struct{} // Lock is a no-op used by -copylocks checker from `go vet`. func (*noCopy) Lock() {} func (*noCopy) Unlock() {} 通过给WaitGroup添加一个noCopy字段，可以为WaitGroup实现 Locker接口，这样vet工具就可以做复制检查了，且因为noCopy 字段是未输出类型，所以WaitGroup不会暴露Lock/Unlock方法。 如果想要自己定义的数据结构不被复制使用，或者说，不能通过 vet 工具检查出复制使用的报警，就可以通过嵌入 noCopy 这个数据类型来实现。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/14.html/:4:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—WaitGroup","uri":"/posts/golang/concurrent/14.html/"},{"categories":["Go"],"content":"小结 关于如何避免错误使用 WaitGroup 的情况，我们只需要尽量保证下面几点： 不重用 WaitGroup。新建一个 WaitGroup 不会带来多大的资源开销，重用反而更容易出错。 保证所有的Add方法调用都在Wait之前。 不传递负数给Add方法，只通过 Done 来给计数值减 1。 不做多余的Done方法调用，保证Add的计数值和Done方法调用的数量是一样的。 不遗漏Done方法的调用，否则会导致Wait hang住无法返回。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/14.html/:5:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—WaitGroup","uri":"/posts/golang/concurrent/14.html/"},{"categories":["Go"],"content":"Go并发编程实战课笔记—读写顺序 Go官方文档里真闷介绍了Go的内存模型，而这里的内存模型不是指Go对象的内存分配、内存回收和内存整理的规范，而描述的是并发环境中多goroutine读相同变量的时候，变量的可见性条件。 编程语言需要一个规范来明确多线程同时访问同一个变量的可见性和顺序，而这个规范就被叫做内存模型。而这主要目的在于： 提供保证，方便在同一个数据同时被多个goroutine访问的情况时可以做一些串行化访问的控制； 允许编译器和硬件对程序做一些优化； ","date":"2022-01-29","objectID":"/posts/golang/concurrent/12.html/:0:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—读写顺序","uri":"/posts/golang/concurrent/12.html/"},{"categories":["Go"],"content":"重排和可见性的问题 由于指令重排，代码并不一定会按照你写的顺序执行。 就比如下面这个代码的例子，运行的时候可能会出现半初始化、未初始化的问题等。 var a string var done bool func setup(){ a = \"hello\" done = true } func main(){ go setup() for !done{ } print(a) } 我们就需要了解happens-before概念，其用来描述两个时间的顺序关系，若操作之间能提供happens-before关系，那么我们就可以确定保证它们之间的顺序。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/12.html/:1:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—读写顺序","uri":"/posts/golang/concurrent/12.html/"},{"categories":["Go"],"content":"happens-before 在一个 goroutine 内部，程序的执行顺序和它们的代码指定的顺序是一样的，即使编译器或者 CPU 重排了读写顺序，从行为上来看，也和代码指定的顺序一样。但是别于另外一个 goroutine 来说，重排就会产生非常大的影响。因为 Go 只保证 goroutine 内部重排对读写的顺序没有影响。 a Happens-before b 定义的顺序为 a -\u003e b 比如我们想要保证对于某一个变量的读操作 r1 绝对能够观察到 写操作 w1，我们就需要同时满足两个条件： w1 happens before r1； 如果对该变量有其他的写操作 w2，就需要保证 w2-\u003ew1-\u003er1或者 w1-\u003er1-\u003ew2，绝对不会和 w1、r1 同时发生，或者是在它们之间发生。 在单个的 goroutine 内部，happens-before 的关系和代码编写的顺序是一致的。 在 Go 语言中，对变量进行零值的初始化就是一个写操作； 如果对超过机器 word（64bit、32bit 或者其他）大小的值进行读写，那么，就可以看作是对拆成 word 大小的几个读写无序进行； Go 并不提供直接的 CPU 屏障（CPU fence）来提示编译器或者 CPU 保证顺序性，而是使用不同架构的内存屏障指令来实现统一的并发原语。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/12.html/:2:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—读写顺序","uri":"/posts/golang/concurrent/12.html/"},{"categories":["Go"],"content":"Go 语言中保证的 happens-before 关系 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/12.html/:3:0","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—读写顺序","uri":"/posts/golang/concurrent/12.html/"},{"categories":["Go"],"content":"init 函数 main函数一定在导入的包的 init 函数之后执行，且每个文件最多只能有一个 init 函数，包下面的多个 init 函数按照它们的文件名顺序逐个初始化。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/12.html/:3:1","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—读写顺序","uri":"/posts/golang/concurrent/12.html/"},{"categories":["Go"],"content":"goroutine 启动 goroutine 的 go 语句的执行，一定 happens before 此 goroutine 内的代码执行。 根据这个规则可说明如果 go 语句传入的参数是一个函数执行的结果，那这个函数一定先于 goroutine 内部的代码被执行。 var a string func f(){ fmt.Println(a) } func main(){ a = \"hello\" go f() } ","date":"2022-01-29","objectID":"/posts/golang/concurrent/12.html/:3:2","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—读写顺序","uri":"/posts/golang/concurrent/12.html/"},{"categories":["Go"],"content":"Channel 通用的 Channel happens before 关系保证有4条规则： 往 Channel 中的发送操作，happens before 从该 Channel 接收相应数据的动作完成之前，即 nSend -\u003e nReceive。 Close 一个 Channel 的调用，肯定 happens before 从关闭的 Channel 中读取出一个零值。 var ch = make(chan struct{},10) var s string func f(){ s = \"hello\" //ch\u003c-struct{}{} close(ch) } func main(){ go f() \u003c-ch //能够在close之前保证取出零值 fmt.Println(s) } 对于 unbuffered 的 Channel，读取数据的调用一定 happens before 往此 Channel 发送数据的调用完成。 如果 Channel 的容量是 m（m\u003e0），那么 nReceive -\u003e (n+m)Send。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/12.html/:3:3","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—读写顺序","uri":"/posts/golang/concurrent/12.html/"},{"categories":["Go"],"content":"Mutex/RWMutex 根据官方描述的 happens-before 关系的保证如下： 对于读写锁 l 的 l.RLock 方法调用，如果存在一个 n，这次的 l.RLock 调用 happens after 第 n 次的 l.Unlock，那么和这个 RLock 相对应的 l.RUnlock 一定 happens before 第 n+1 次 l.Lock。意思是，读写锁的 Lock 必须等待既有的读锁释放后才能获取到。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/12.html/:3:4","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—读写顺序","uri":"/posts/golang/concurrent/12.html/"},{"categories":["Go"],"content":"WaitGroup WaitGroup的保证是 Wait 方法等到计数值归零之后才返回。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/12.html/:3:5","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—读写顺序","uri":"/posts/golang/concurrent/12.html/"},{"categories":["Go"],"content":"Once 提供的保证为以下： 对于 once.Do(f) 调用，f 函数的那个单次调用一定 happens before 任何 once.Do(f) 调用返回。即函数 f 一定会在 Do 方法返回之前执行。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/12.html/:3:6","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—读写顺序","uri":"/posts/golang/concurrent/12.html/"},{"categories":["Go"],"content":"atomic 对于 Go 1.15 的官方实现来说，可以保证使用 atomic 的 Load/Store 的变量之间的顺序性。 但现阶段还是不要使用 atomic 来保证顺序性。 ","date":"2022-01-29","objectID":"/posts/golang/concurrent/12.html/:3:7","tags":["Go","并发编程","note"],"title":"Go并发编程实战课笔记—读写顺序","uri":"/posts/golang/concurrent/12.html/"},{"categories":["Go"],"content":"pprof性能排查分析note ","date":"2022-01-28","objectID":"/posts/golang/pprof/0.html/:0:0","tags":["pprof","性能优化","Go"],"title":"pprof性能排查分析note","uri":"/posts/golang/pprof/0.html/"},{"categories":["Go"],"content":"pprof ","date":"2022-01-28","objectID":"/posts/golang/pprof/0.html/:1:0","tags":["pprof","性能优化","Go"],"title":"pprof性能排查分析note","uri":"/posts/golang/pprof/0.html/"},{"categories":["Go"],"content":"概述 工具 runtime/pprof net/hhtp/pprof 采样 CPU Heap Goroutine Mutex Block ThreadCreate 分析 网页 可视化终端 展示 Top Graph Source FlameGraph Peek Disassemble ","date":"2022-01-28","objectID":"/posts/golang/pprof/0.html/:1:1","tags":["pprof","性能优化","Go"],"title":"pprof性能排查分析note","uri":"/posts/golang/pprof/0.html/"},{"categories":["Go"],"content":"性能排查[炸弹] 前置准备 使用 net/http/pprof 浏览器查看指标 ip:port/debug/pprof/ CPU go tool pprof \"http://localhost:6060/dubug/pprof/profile?seconds=10\" topN:查看占用资源最多的函数 其中需要注意两个参数： flat:本函数的执行耗时； cum:累计量。指该函数加上该函数调用的函数总耗时； 在 flat==cum 情况下说明该函数没有子函数，若 flat == 0 则说明函数中只有其他函数的调用； list:根据给定的正则表达式查找代码行 web:生成调用关系图 Heap go tool pprof -http=:8080 \"http://localhost:6060/debug/pprof/heap\" Top视图 Source视图：源码视角查看资源占用 SAMPLE选项： alloc_objects:程序累计申请的对象数 alloc_sapce:程序累计申请的内存大小 infuse_objects:当前持有的对象数 infuse_sapce:当前持有的内存大小 Goroutine goroutine泄漏也会导致内存泄漏 go tool pprof -http=:8080 \"http://localhost:6060/debug/pprof/goroutine\" FlameGraph：火焰图 Mutex go tool pprof -http=:8080 \"http://localhost:6060/debug/pprof/mutex\" Block go tool pprof -http=:8080 \"http://localhost:6060/debug/pprof/block\" 若阻塞操作消耗时间相对于其他阻塞过小，则 pprof 视图中可能会被忽略，出现这种问题我们可以通过调整比例发现，且 pprof 的 http端口监听若出现阻塞也会被记录下来。 ","date":"2022-01-28","objectID":"/posts/golang/pprof/0.html/:1:2","tags":["pprof","性能优化","Go"],"title":"pprof性能排查分析note","uri":"/posts/golang/pprof/0.html/"},{"categories":["Go"],"content":"proof指标采样的流程和原理 ","date":"2022-01-28","objectID":"/posts/golang/pprof/0.html/:2:0","tags":["pprof","性能优化","Go"],"title":"pprof性能排查分析note","uri":"/posts/golang/pprof/0.html/"},{"categories":["Go"],"content":"CPU采样 采样对象：函数调用和它们的占用时间 采样率：100次/秒，固定值 采样时间：从手动启动到手动结束 核心在于通过设定和取消信号处理函数，来开启和关闭定时器。 操作系统：每10ms向进程发送一次SIGPROF信号 进程：每次接受到SIGPROF会记录调用栈 写缓冲：每100ms读取一次已经记录的调用栈并写入输出流 ","date":"2022-01-28","objectID":"/posts/golang/pprof/0.html/:2:1","tags":["pprof","性能优化","Go"],"title":"pprof性能排查分析note","uri":"/posts/golang/pprof/0.html/"},{"categories":["Go"],"content":"Goroutine\u0026ThreadCreate采样 Goroutine 记录所有用户发起且在运行中的 goroutine（即入口非 runtime 开头的）和 runtime.main 的调用栈信息 Stop-\u003e遍历allgs切片-\u003e输出创建g的堆栈-\u003eStart ThreadCreate 记录程序创建的所有系统线程的信息 Stop-\u003e遍历allm链表-\u003e输出创建m的堆栈-\u003eStart ","date":"2022-01-28","objectID":"/posts/golang/pprof/0.html/:2:2","tags":["pprof","性能优化","Go"],"title":"pprof性能排查分析note","uri":"/posts/golang/pprof/0.html/"},{"categories":["Go"],"content":"Heap（堆内存）采样 采样程序通过内存分配器在堆上分配和释放且参与GC的内存，记录分配/释放的大小和数量 底层使用cgo、调用就回收的栈内存等是不会被记录的。 采样率：每分配512KB记录一次，可以在运行开头修改，1为每次分配均记录 采样时间：从程序运行开始到采样时 采用指标：alloc_space、alloc_objects、inuse_space、inuse_objects； 计算方式：inuse = alloc - free ","date":"2022-01-28","objectID":"/posts/golang/pprof/0.html/:2:3","tags":["pprof","性能优化","Go"],"title":"pprof性能排查分析note","uri":"/posts/golang/pprof/0.html/"},{"categories":["Go"],"content":"Block\u0026Mutex采样 阻塞 采用阻塞操作的次数和耗时 采样率：阻塞耗时超过阈值值才会被记录，1为每次阻塞均记录 阻塞操作-上报调用栈和消耗时间-\u003eprofiler-采样时-\u003e遍历阻塞记录-\u003e统计阻塞次数和耗时 锁竞争 采样争抢锁的次数和耗时 采样率：只记录固定比例的锁操作，1为每次加锁均记录 锁操作-上报调用栈和消耗时间-\u003eprofiler-\u003e采样时-遍历锁记录-\u003e统计锁竞争次数和耗时 ","date":"2022-01-28","objectID":"/posts/golang/pprof/0.html/:2:4","tags":["pprof","性能优化","Go"],"title":"pprof性能排查分析note","uri":"/posts/golang/pprof/0.html/"},{"categories":["rust"],"content":"（WIP）Rust学习笔记 以下是学习张汉东大神的Rust课程 中摘录的笔记 后续会持续更新 ","date":"2022-01-28","objectID":"/posts/rust/0.html/:0:0","tags":["rust","WIP","note"],"title":"（WIP）Rust学习笔记","uri":"/posts/rust/0.html/"},{"categories":["rust"],"content":"Rust语言版本说明 Rust语言的版本包括以下三个相互正交的概念： 语义化版本（Sem Ver，Semantic Versioning） 发行版本 Edition版次 语义化版本（Sem Ver, Semantic versioning） 其格式为：主版本号.次版本号.修订号 语义版本号递增规则： 主版本号：当做了不兼容的API修改； 次版本号：当做了向下兼容的功能性新增； 修订号：当做了向下兼容的问题修正； 发行版本 Edition版次 ","date":"2022-01-28","objectID":"/posts/rust/0.html/:1:0","tags":["rust","WIP","note"],"title":"（WIP）Rust学习笔记","uri":"/posts/rust/0.html/"},{"categories":["rust"],"content":"Rust词法结构 ","date":"2022-01-28","objectID":"/posts/rust/0.html/:2:0","tags":["rust","WIP","note"],"title":"（WIP）Rust学习笔记","uri":"/posts/rust/0.html/"},{"categories":["rust"],"content":"Rust编译过程 ","date":"2022-01-28","objectID":"/posts/rust/0.html/:2:1","tags":["rust","WIP","note"],"title":"（WIP）Rust学习笔记","uri":"/posts/rust/0.html/"},{"categories":["rust"],"content":"Rust词法结构 包含其六大部分： 关键字（Keywords） 标识符（Identifier） 注释（Comment） 空白（Whitespace） 词条（Tokens） 路径（Path） 关键字 严格关键字（Strict） as/ break/ const/ continue/ crate/ if/ else/ struct/ enum/ true/ false/ fn/ for/ in/ let/ loop/ impl/ mod/ match/ move / mut /pub/ ref/ return/ self/ Self/ static/ super/ trait/ type/ unsafe/ use/ where/ while /async/ await/ dyn / main 保留字（Reserved） abstract/ become/ box/ do/ final/ macro/ override/ priv/ typeof/ unsized/ virtual/ yield / try 被保留的关键字不代表将来一定会使用 弱关键字（Weak） 2018 Edition: union, ‘static 2015 Edition: dyn 被保留的关键字不代表将来一定会使用 标识符 注释 空白 Rust中空白字符包括：\\n、\\t、tab等； 任何形式的空白字符在 Rust 中只用于分隔标记，无语义意义； 词条 语言项（item） 块（block） 语句（Stmt） 表达式 （Expr） 模式（Pattern） 关键字（Keyword） 标识符（Ident） 字面量（Literal） 生命周期（Lifetime） 可见性（Vis） 标点符号（Punctuation） 分隔符（delimiter） 词条树（Token Tree） 属性（Attribute） 路径 ","date":"2022-01-28","objectID":"/posts/rust/0.html/:2:2","tags":["rust","WIP","note"],"title":"（WIP）Rust学习笔记","uri":"/posts/rust/0.html/"},{"categories":["rust"],"content":"Rust语法面面观：面向表达式 ","date":"2022-01-28","objectID":"/posts/rust/0.html/:3:0","tags":["rust","WIP","note"],"title":"（WIP）Rust学习笔记","uri":"/posts/rust/0.html/"},{"categories":["rust"],"content":"表达式和语句 表达式和语句中的广义视角：每行代码都可以看作是一个语句。 语句的四种类型： 声明语句 流程控制语句 表达式语句 宏语句 Rust 语法的“骨架”可以缩为一行，并最终可以简化为三个关键元素： 属性，类似于#![...]； 分号;即行分隔符； 花括号{...}即块分隔符； Rust 为面向表达式的语言，借鉴了函数式语言，面向表达式。 面向表达式 分号和块是 Rust 语言的两个基本表达式。 分号表达式 单元类型（Unit Type） ; -\u003e () 块表达式 块中最后一个表达式的值 Rust 中的求值规则： 分号表达式返回值永远为自身的单元（Unit）类型：()； 分号表达式只有在块表达式最后一行才会进行求值，其他时候只作为连接符存在； 块表达式只对其最后一行表达式进行求值； 验证求值规则 fn main(){;;{()}{();usestd::vec::Vec;}();\u0026{;};// -\u003e \u0026() ;// -\u003e () ;} 另一种划分方式 基本语句 声明语句 表达式语句 表达式 块中最后一行不加分号的表达式 流程控制也是表达式。 除了基本的声明语句，其他皆为表达式。 ","date":"2022-01-28","objectID":"/posts/rust/0.html/:3:1","tags":["rust","WIP","note"],"title":"（WIP）Rust学习笔记","uri":"/posts/rust/0.html/"},{"categories":["rust"],"content":"编译期计算 编译期计算：最先由 Lisp/Cpp 语言支持，CTFE（compile time function evaluation）。 编译期计算 Rust 支持两种方式： 过程宏 + Build 脚本（build.rs） 类似于 Cpp 中 constexpr 的 CTFE 功能 Rust 中的 CTFE： 常量函数（const fn） 常量泛型（const generic） 常量函数 const fn 常量表达式与常量上下文： fn main(){letan=(42,).0;constAN:i32 =an;// Error: attempt to use a non-constant value in a constant // fixed error: constAN:i32 =(42,).0;} **常量上下文（const context）**包含： 常量值初始化位置 静态数组的长度表达式，[T; N] 重复的长度表达式，类似于：[0; 10] 静态变量、枚举判别式的初始化位置 我们需要注意： 常量上下文是编译器唯一运行进行编译期求值的地方 在非常量上下文的地方，常量表达式不一定会在编译期求值 常量传播（Const Propagation） 常量传播和编译期计算是不同的： 常量传播是编译器的一种优化； 常量传播并不能改变程序的任何行为，并且对开发者是隐藏的； 编译期计算则是指编译时执行的代码，必须知道其结果，才能继续执行； constX:u32 =3+4;// CTFE letx:uew=4+3;// 不是 CTFE，但可能会被常量传播优化，因为它不在常量上下文 常量安全（Const Safe） Rust 里的大部分表达式都可用作常量表达式； 并不是所有常量表达式都可以用在常量上下文； 编译期求值必须得到一个确定性的结果； constfn gcd(a:u32,b:u32)-\u003eu32{match(a,b){(x,0)|(0,x)=\u003ex,(x,y)ifx%2==0\u0026\u0026y%2==0=\u003e2*gcd(x/2,y/2),(x,y)|(y,x)ifx%2==0=\u003egcd(x/2,y),(x,y)ifx\u003cy=\u003egcd((y-x)/2,x),(x,y)=\u003egcd((x-y)/2,y),}}constGCD: u32 =gcd(21,7);fn main(){println!(\"{}\",GCD);} constfn fib(n:u128)-\u003eu128{constfn helper(n:u128,a:u128,b:u128,i:u128)-\u003eu128{ifi\u003c=n{helper(n,b,a+b,i+1)}else{b}}helper(n,1,1,2)}constX:u128 =fib(10);fn main(){println!(\"{}\",GCD);} 常量安全子类型系统： 普通的 fn 关键字定义的函数，是 Safe Rust 主类型系统保证安全。 const fn 定义的函数，是 Safe Rust 主类型系统下有一个专门用于常量计算的子类型系统来保证常量安全。 常量上下文可接受的常量表达式 const fn 函数 元组结构体 元组的值 // Error constfn hello()-\u003eString{\"Hello\".to_string()}// Error constS: String =hello();// Correct constfn hello()-\u003e\u0026'static str{\"Hello\"}// Correct constY: \u0026str =hello();fn main(){println!(\"{:?}\",S);} #[derive(Debug)]struct Answer(u32);constA:Answer=Answer(42);fn main(){println!(\"{}\",A);} 编译期计算如何实现 MIR（中级中间语言） Miri（编译期内置 MIR 解释器） 比如下面的代码在编译期： constfn answer()-\u003e u32 {42}constA:u32 =answer();fn main(){leta=A;} 其 Miri 中求值主要过程为： const A 赋值 在 bb0 中调用 const answer const answer 返回常量 42 流程跳到 bb1 返回42 While true vs loop 想使用无限循环的时候建议使用 loop，而非 while true fn main(){letmuta;whiletrue{a=1;break;}println!(\"{}\",a);// Error } fn main(){letmuta;loop{a=1;break;}println!(\"{}\",a);} 关于编译期为什么不识别 while true 主要有以下原因： 要考虑：while(constexpr = true)的情况； 使用 #[allow(while_true)]属性在某些情况下允许使用 while true（上面的例子不符合使用情况）； ","date":"2022-01-28","objectID":"/posts/rust/0.html/:3:2","tags":["rust","WIP","note"],"title":"（WIP）Rust学习笔记","uri":"/posts/rust/0.html/"},{"categories":["rust"],"content":"常量泛型 const generic Rust 中的静态数组一直以来都属于“二等公民”，不方便使用。 #![feature(min_const_generics)]usecore::mem::MaybeUninit;constX:u128 =fib(10);pubstruct ArrayVec\u003cT,constN: usize\u003e{items: [MaybeUninit\u003cT\u003e;N],length: usize,} 目前存在的缺陷： 目前仅限于整数原生类型，包括有符号和无符号整数类型，布尔值和字符还不允许使用复合类型和自定义类型，也不允许使用引用，即意味着不允许使用字符串； 常量泛型参数目前只支持两种表达式： 一个简单的常量泛型参数 可以在不依赖于任何类型或常量参数的常量上下文中使用的表达式 类型理论 常量泛型是一种依赖类型（Depended Type） 因为数组 [T;N] 的类型，最终是要依赖于 N 的具体值来确定。 #![feature(min_const_generics)]#![feature(const_in_array_repeat_expressions)]usecore::mem::MaybeUninit;#[derive(Debug)]pubstruct ArrayVec\u003cT,constN: usize\u003e{items: [MaybeUninit\u003cT\u003e;N],length: usize,}impl\u003cT,constN:usize\u003eArrayVec\u003cT,{N}\u003e{pubconstfn new()-\u003eArrayVec\u003cT,{N}\u003e{ArrayVec{items:[MaybeUninit::uninit();N],length:0,}}#[inline]pubconstfn len(\u0026self)-\u003e usize {self.length}#[inline]pubconstfn is_empty(\u0026self)-\u003e bool {self.len()==0}#[inline]pubconstfn capacity(\u0026self)-\u003e usize {N}#[inline]pubconstfn is_full(\u0026self)-\u003e bool {self.len()\u003e=self.capacity()}} ","date":"2022-01-28","objectID":"/posts/rust/0.html/:3:3","tags":["rust","WIP","note"],"title":"（WIP）Rust学习笔记","uri":"/posts/rust/0.html/"},{"categories":["rust"],"content":"从表达式的分类角度来看 Rust 的变量绑定和引用 ","date":"2022-01-28","objectID":"/posts/rust/0.html/:4:0","tags":["rust","WIP","note"],"title":"（WIP）Rust学习笔记","uri":"/posts/rust/0.html/"},{"categories":["rust"],"content":"Rust 中表达式的分类 位置表达式\u0026值表达式 表达式背后的内存管理 let 绑定 位置表达式包括以下： 静态变量初始化： static mut LEVELS:u32 = 0; 解引用表达式： *expr 数组索引表达： express[expr] 字段表达式： expr.field 以及上述加上括号的位置表达式： (expr) 除此之外的都是值表达式。 位置上下文 除了赋值左侧的位置上下文之外，还有复合赋值操作的左侧； letmuta=1;a+=1; 一元借用和解引用操作中的操作数所在区域； leta=\u0026mut7;*a=42;letb=\u0026a; 字段表达式的操作数所在区域 struct A{name:\u0026'static str,}leta=A{name:\"Alex\"};a.name; 数组索引表达式的操作数所在区域 letmuta=[1,2,3];letb=\u0026muta;a[1]=42; 任意隐式借用操作数所在区域 letmutv=vec![1,2,3];v.push(4); let 语句初始化 leta:i32;a=42; if let / while let / match 的匹配表达式所在区域 letdish={\"Ham\",\"Eggs\"};iflet(\"Bacon\",b)=dish{// 匹配表达式所在区域就是位置上下文 println!(\"Bacon is serverd with {}\",b);}else{println!(\"No bacon will be served\");}// while let (位置上下文) = ... { ... } // match (位置上下文) { ... } 结构体更新语法中的 base 表达式（ ..操作符后面的操作数区域） letmutbase=Point3d{x:1,y:2,z:3};lety_ref=\u0026mutbase.y;Point3d{y:0,z:10,..base};// 得到 base.x ","date":"2022-01-28","objectID":"/posts/rust/0.html/:4:1","tags":["rust","WIP","note"],"title":"（WIP）Rust学习笔记","uri":"/posts/rust/0.html/"},{"categories":["rust"],"content":"Rust 所有权语义在表达式上的体现 Copy 语义代表可以安全在栈内存复制 Move 语义代表必须旧的绑定失效，避免内存不安全 ","date":"2022-01-28","objectID":"/posts/rust/0.html/:4:2","tags":["rust","WIP","note"],"title":"（WIP）Rust学习笔记","uri":"/posts/rust/0.html/"},{"categories":["rust"],"content":"不可变与可变 Rust 借鉴了函数式语言的不可变特性，包括： 不可变绑定与可变绑定 不可变引用与可变引用 不可变绑定与可变绑定 默认不可变： 如果想修改变量的值： letanswer=42;letanswer=44;// 这种方式也叫变量遮蔽 可变绑定使用 mut 修饰符： 不可变引用与可变引用 不可变引用也叫做共享引用： 同样可变引用使用 mut 修饰引用： letmutanswer=42;letr=\u0026mutanswer;*r=43;println!(\"{:?}\",answer);// 43 可变引用也叫独占引用： ","date":"2022-01-28","objectID":"/posts/rust/0.html/:4:3","tags":["rust","WIP","note"],"title":"（WIP）Rust学习笔记","uri":"/posts/rust/0.html/"},{"categories":["rust"],"content":"数据类型 ","date":"2022-01-28","objectID":"/posts/rust/0.html/:5:0","tags":["rust","WIP","note"],"title":"（WIP）Rust学习笔记","uri":"/posts/rust/0.html/"},{"categories":["rust"],"content":"基本数据类型 字符串 字符串为实际存在的 UNIQUE 的字符，占四个字节。 根据字符串使用场景设计类型： 字面量 动态可增长字符串 从一段字符串中截取的片段 字符串编码 FFi 中需要转换字符串到 C 或 OS 本地字符串 遵循特定格式的文件路径 字符类型众多就是为了保证覆盖到所有场景且为了保证类型安全。 字符串与切片 字符串为 UTF-8 编码的 u8 序列。但 u8 序列切片并不一定是合法的字符串切牌呢。 Rust 内存分配默认是在栈上分配内存，并且通过栈来管理堆内存，所以必须在编译期来确定类型的大小。但编译期不可能知道开发者字符串的长度，所以这里 str 类型是一个动态大小的类型。 str -\u003e \u0026str 这类引用被称为胖指针，它表示栈上存储一个指向静态区域或者是堆内存的指针。以及数据的长度，它比普通指针占用的空间更大，所以叫做胖指针。字符串可以存储于静态存储区，栈内存只存指针，所以它是一个静态引用字符串切片类型 。 [T] -\u003e \u0026[T] String -\u003e Vec 指针类型 三种指针类型： 原始指针，*mut T和*const T； NonNull指针。它是 Rust 语言建议的 *mut T指针的替代指针。NonNull指针是非空指针，并且是遵循生命周期类型协变规则。 函数指针：函数指针是指向代码的指针，而非数据。可以使用它直接调用函数。 引用 两种引用类型： \u0026T 和 \u0026mut T； 引用与指针的主要区别： 引用不可能为空； 拥有生命周期； 受借用检查器保护不会发生悬垂指针等问题； 元组 唯一的异构序列： 不同长度的元组是不同类型； 单元类型的唯一实例等价于空元组； 当元组只有一个元素的时候，要在元素末尾加逗号分隔，以此方便和括号操作符区分开来。 Never 类型 为了保证类型安全就需要考虑所有可能的情况，所有也要考虑不可能返回值的计算情况。 代表的是不可能返回值的计算类型： 类型理论中叫做底类型，底类型不包含任何值，但它可以合一到任何其他类型； Never 类型用!叹号表示； 目前还未稳定，但在 Rust 内部已经在使用了； ","date":"2022-01-28","objectID":"/posts/rust/0.html/:5:1","tags":["rust","WIP","note"],"title":"（WIP）Rust学习笔记","uri":"/posts/rust/0.html/"},{"categories":["rust"],"content":"自定义复合类型 结构体 Struct 枚举体 Enum 联合体 Union 分别有具名结构体、元组结构体、单元结构体。 // 具名结构体，包含了字段 struct Point{x: f32,y: f32,}// 元组结构体 struct Pair(i32,f32);// 单元结构体 struct Unit;fn main(){letpoint=Point{x:1.0,y:2.0};letpair=Pair(1,2.0);assert_eq!(pair.0,1);letunit1=Uint;letunit2=Unit;} 当元组结构体只包含一个成员时： struct Score(u32);implScore{fn pass(\u0026self)-\u003e bool{self.0\u003e=60}}fn main(){lets=Score(50);assert_eq!(s.pass(),false)} 对于结构体内存布局方面，编译器会对结构体进行内存对齐，以此提升 CPU 的访问效率。使用内存布局属性可以指定 C 内存布局，防止编译器重排。 struct A{a: u8,b: u32,c: u16,}// 编译器重排字段，优化内存占用 //struct A{ // a: u32, // b: u16, // c: u8, //} // 8 //#[repr(C)] //struct A{ // a: u8, // b: u32, // c: u16, //} // 12 fn main(){println!(\"{:?}\",std::mem::size_of::\u003cA\u003e());// 8 letv=A{a:1,b:2,c:3};} 枚举体与联合体内存布局中，以枚举类型成员最大的对齐值为准不需要为每个枚举值都对齐。 enum A{One,Two,}enum E{N,H(u32),M(Box\u003cu32\u003e)}union U{u: u32,v: u64 }fn main(){println!(\"E: {:?}\",std::mem::size_of::\u003cA\u003e());// 1 println!(\"Box\u003cu32\u003e: {:?}\",std::mem::size_of::\u003cBox\u003cu32\u003e\u003e());// 8 println!(\"U: {:?}\",std::mem::size_of::\u003cE\u003e());// 16 println!(\"U: {:?}\",std::mem::size_of::\u003cU\u003e());// 8 } ","date":"2022-01-28","objectID":"/posts/rust/0.html/:5:2","tags":["rust","WIP","note"],"title":"（WIP）Rust学习笔记","uri":"/posts/rust/0.html/"},{"categories":["rust"],"content":"容器类型 内部可变性（interior mutability） 与继承式可变相对应； 由可变性核心原语 UnsafeCell 提供支持； 基于 UnsafeCell 提供了 Cell 和 RefCell。 Cell \u0026 RefCell usestd::cell::Cell;struct Foo{x: u32,y: Cell\u003cu32\u003e}fn main(){letfoo=Foo{x: 1,y: Cell::new(3)};assert_eq!(1,foo.x);assert_eq!(3,foo.y.get());foo.y.set(5);assert_eq!(5,foo.y.get());lets=\"hello\".to_string();letbar=Cell::new(s);letx=bar.into_inner();// bar; // error: use of moved value: `bar` } usestd::cell::RefCell;fn main(){letx=RefCell::new(vec![1,2,3,4]);println!(\"{:?}\",x.borrow());x.borrow_mut().push(5);println!(\"{:?}\",x.borrow());} 运行时借用检查 ","date":"2022-01-28","objectID":"/posts/rust/0.html/:5:3","tags":["rust","WIP","note"],"title":"（WIP）Rust学习笔记","uri":"/posts/rust/0.html/"},{"categories":["rust"],"content":"泛型 所谓泛型就是参数化类型。 fn foo\u003cT\u003e(x: T)-\u003eT{returnx;}fn main(){assert_eq!(foo(1),1);assert_eq!(foo(\"hello\"),\"hello\");} 静态分发 fn main(){fn foo_1(x: i32)-\u003ei32{returnx;}fn foo_2(x:\u0026'static' str)-\u003e\u0026'static str{returnx;}foo_1(1);foo_2(\"2\");} 当类型推断失效，需要手工指定类型的时候使用 foo(1);//= foo::\u003ci32\u003e(1);foo(\"hello\");//= foo::\u003c\u0026'staticstr\u003e(\"hello\"); ","date":"2022-01-28","objectID":"/posts/rust/0.html/:5:4","tags":["rust","WIP","note"],"title":"（WIP）Rust学习笔记","uri":"/posts/rust/0.html/"},{"categories":["rust"],"content":"特定类型 所谓特定类型，是指专门有特殊用途的类型 PhantomData ， 幻影类型。 一般用于 Unsafe Rust 的安全抽象。 Pin， 固定类型。为了支持异步开发而特意引进，防止被引用的值发生移动的类型。 ","date":"2022-01-28","objectID":"/posts/rust/0.html/:5:5","tags":["rust","WIP","note"],"title":"（WIP）Rust学习笔记","uri":"/posts/rust/0.html/"},{"categories":["os"],"content":"程序运行原理 Thinking 以下是个人理解（大白话较多） ","date":"2022-01-28","objectID":"/posts/os/common/0.html/:0:0","tags":["os"],"title":"程序运行原理Thinking","uri":"/posts/os/common/0.html/"},{"categories":["os"],"content":"程序是如何运行起来的 首先对于程序，可以简单理解为可执行的代码，程序是静态的，即存储在磁盘上。 而如果想要程序处理数据，完成计算任务，就必须把程序从外部加载到内存中。加载到内存中后就会在操作系统的管理调度下交给CPU去执行，将该程序运行起来。而程序被运行起来后就可以被称作为进程。 进程除了包含可执行的代码，还包括进程在运行期间使用的内存堆空间、栈空间、供操作系统管理用的数据结构等。 而如果从指令角度看，那就是操作系统把可执行代码加载到内存中后生成相应的数据结构和变量，而生成这些就需要在进程的堆空间申请相应的内存空间，并把内存的首地址信息记录在进程的栈中。 堆是一块无序的内存空间，任何时候进程需要申请内存，都会从堆空间中分配，分配到的内存地址则记录在栈中。而栈主要是来记录函数内部的局部变量、堆空间分配的内存空间地址等。 最后在内存空间后，就可以从可执行代码的起始位置读取指令交给CPU顺序执行。指令执行过程中所遇到的跳转指令，就可能是由我们常见的循环、控制判断等导致的。且每次函数调用时，就会利用到栈帧，函数在栈顶执行，且每个栈帧都是隔离的即不发生混乱。 ","date":"2022-01-28","objectID":"/posts/os/common/0.html/:1:0","tags":["os"],"title":"程序运行原理Thinking","uri":"/posts/os/common/0.html/"},{"categories":["os"],"content":"一台计算机如何同时处理多个任务 主要依靠操作系统的CPU分时共享技术，即宏观上是并行，微观上是串行的。 所以进程不是总在运行，进程的生命周期中主要有三种状态：运行、就绪、阻塞。 不同进程轮流在CPU上执行，就会涉及到进程之间的切换问题。线程也是同样的问题，但线程的切换代价更小。 一台计算机同时处理多个任务的性能还是要看其进程调度，或者线程调度，本质就是为了保证每一个合理的任务尽可能地被处理运行。 ","date":"2022-01-28","objectID":"/posts/os/common/0.html/:2:0","tags":["os"],"title":"程序运行原理Thinking","uri":"/posts/os/common/0.html/"},{"categories":["os"],"content":"系统为什么会变慢或者崩溃 这里的问题以web应用程序作为例子。 高并发请求，导致系统变慢或者严重至崩溃，也就是用户请求响应变慢，而这里可能导致的原因可能就是资源限制导致的线程阻塞问题，一个线程阻塞可能就会影响到其他线程的运行，被阻塞的线程越多，占据的系统资源也越多，这些被阻塞的线程既不能继续执行，也不能释 放当前已经占据的资源，在系统中一边等待一边消耗资源。 而发生阻塞的原因可能就是出现了等待其他服务完成或者等待信号，最常见的就是我们平常遇见的排队等锁，而出现锁就涉及到了线程安全问题。 解决的主要手段，在单机中可以通过设计更优的保证线程安全的执行调度方案，而在多机中就可以使用分布式系统架构，通过集群来共同处理用户的并发请求，减少资源限制的压力，必要时可以在入口进行限流以减少并发请求数，在于应用内进行降级以减小线程的资源消耗。 ","date":"2022-01-28","objectID":"/posts/os/common/0.html/:3:0","tags":["os"],"title":"程序运行原理Thinking","uri":"/posts/os/common/0.html/"},{"categories":["network"],"content":"simple socks5 proxy \u0026 the c10k problem ","date":"2022-01-28","objectID":"/posts/network/socks5/0.html/:0:0","tags":["network","socks5"],"title":"simple socks5 proxy \u0026 the c10k problem","uri":"/posts/network/socks5/0.html/"},{"categories":["network"],"content":"socks5 socks5简介 SOCKS是一种网络传输协议，主要用于客户端与外网服务器之间通讯的中间传递。SOCKS是\"SOCKetS\"的缩写。 当防火墙后的客户端要访问外部的服务器时，就跟SOCKS代理服务器连接。这个代理服务器控制客户端访问外网的资格，允许的话，就将客户端的请求发往外部的服务器。 主要重点在于： 认证 建立连接 转发数据 ","date":"2022-01-28","objectID":"/posts/network/socks5/0.html/:1:0","tags":["network","socks5"],"title":"simple socks5 proxy \u0026 the c10k problem","uri":"/posts/network/socks5/0.html/"},{"categories":["network"],"content":"c10k问题 通过socks5二进制协议可以实现一个高性能的网络服务器代理。而说到服务器代理的性能相关，我们不由得想到经典的c10k问题。 解决c10k问题的主要思路其实非常简单，分别是： 对于每个连接处理分配一个独立的进程/线程； 用同一进程/线程来同时处理若干连接； 首先第一个思路是最直接的方式，但是资源是有限的（在我看来，性能问题的起因就是资源有限）。 所以问题非常明显，资源占用过多，可扩展性也非常差。 而第二个思路就是我们常说的IO多路复用的问题。 那IO多路复用最直接的方式就是从循环处理开始，遍历处理各个socket，若socket中都有数据，这种方式是可行的。但当应用读取某个socket的文件数据产生了堵塞，即非ready状态，则整个应用会阻塞在这里等待该文件句柄，从而无法处理其他文件句柄。 这里就需要提到select、poll、epoll这三个技术了。 这里我简单总结一下： select 每次调用初始化fd_set 结构体，利用fd_set结构体在内核同时监控多个文件句柄，通过FD_ISSET来查看具体某个文件句柄是否发生变化(ready/unready)。 思路：有连接请求进行无差别轮询检查。 问题：句柄上限+重复初始化+逐个排查所有文件句柄效率低。 poll poll本质上和select没有区别，主要是结构体是通过一个 pollfd 数组向内核传递需要关注的事件消除文件句柄上限（通过链表），同时使用不同字段分别标注关注事件和发生事件，来避免重复初始化。 思路：设计新的数据结构提供使用效率。 问题：逐个排查所有文件句柄效率低。 epoll 事件驱动（每个事件关联上fd），调用返回的时候只给发生了状态变化的应用提供（很可能是数据 ready）的文件句柄，即利用callback方式进行异步回调。且利用mmap()文件映射内存加速与内核空间的消息传递；即epoll使用mmap减少复制开销。 思路：只返回状态变化的文件句柄。 问题：依赖特定平台（Linux）+ 存在上限（但是相对于前面两种很大）。 由于epoll, kqueue, IOCP每个接口都有自己的特点，程序移植非常困难，于是需要对这些接口进行封装，以让它们易于使用和移植。而就是libevent库就是其中之一。 目前，libevent已支持以下接口/dev/poll, kqueue, event ports, select, poll 和 epoll。 select，poll，epoll都是IO多路复用的机制。I/O多路复用就通过一种机制，可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。但select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，而异步I/O则无需自己负责进行读写，异步I/O的实现会负责把数据从内核拷贝到用户空间。 epoll技术中还有几个点值得我们学习下。 epoll支持水平触发和边缘触发。且对文件描述符的操作有两种模式： LT模式：缺省的工作方式，并且同时支持block和no-block socket；当epoll_wait检测到描述符事件发生并将此事件通知应用程序，应用程序可以不立即处理该事件。下次调用epoll_wait时，会再次响应应用程序并通知此事件。 ET模式：高速工作方式，只支持no-block socket。当epoll_wait检测到描述符事件发生并将此事件通知应用程序，应用程序必须立即处理该事件。如果不处理，下次调用epoll_wait时，不会再次响应应用程序并通知此事件。 水平触发：如果报告了fd后，没有被处理，那么下次poll时会再次报告该fd。 边缘触发：它只告诉进程哪些fd刚刚变为就绪态，并且只会通知一次。 ","date":"2022-01-28","objectID":"/posts/network/socks5/0.html/:2:0","tags":["network","socks5"],"title":"simple socks5 proxy \u0026 the c10k problem","uri":"/posts/network/socks5/0.html/"},{"categories":["network"],"content":"协程 从前面的I/O多路复用技术可以看出，实际上epoll已经能够很好的处理c10k问题，但是我们也知道epoll的上限还是存在的，对于如今这个对百万并发常见的时代，如果要进行进一步的拓展，我们就需要引入新的技术来解决。 了解并发问题的朋友可能都知道这么一个道理：内核不是解决方案，而是问题所在！ 即可以理解为，内核处理核心任务，而其他的尽量交给应用程序处理或者交给用户态去处理。 而协程就是能够实现这一目的的技术，其核心思路为试图用一组少量的线程来实现多个任务，一旦某个任务阻塞，则可能用同一线程继续运行其他任务，避免大量上下文的切换。每个协程所独占的系统资源往往只有栈部分。而且，各个协程之间的切换，往往是用户通过代码来显式指定的（跟各种 callback 类似），不需要内核参与，可以很方便的实现异步。 其本质就是异步非阻塞技术。 Golang中对于协程的实现，和其GPM调度策略等处理器策略是非常优秀的，当然这里就不详细解释了，篇幅有限。 当然同步阻塞策略在一些并发量较小的场景也非常优秀，不会浪费资源，效率较高，调度较异步非阻塞更加容易。 但我们也需要知道异步回调程序的性能是要优于协程模型的，因为异步回调是没有切换开销的。 ","date":"2022-01-28","objectID":"/posts/network/socks5/0.html/:2:1","tags":["network","socks5"],"title":"simple socks5 proxy \u0026 the c10k problem","uri":"/posts/network/socks5/0.html/"},{"categories":["network"],"content":"socks代码示例 package socks5 import ( \"encoding/binary\" \"errors\" \"fmt\" \"io\" \"net\" ) /** * user: ZY * Date: 2020/11/24 15:38 */ func main() { server, err := net.Listen(\"tcp\", \":1080\") if err != nil { fmt.Printf(\"Listen failed: %v\\n\", err) return } for { client, err := server.Accept() if err != nil { fmt.Printf(\"Accept failed: %v\", err) continue } go process(client) } } func process(client net.Conn) { if err := Socks5Auth(client); err != nil { fmt.Println(\"auth error:\", err) client.Close() return } target, err := Socks5Connect(client) if err != nil { fmt.Println(\"connect error:\", err) client.Close() return } Socks5Forward(client, target) } func Socks5Auth(client net.Conn) (err error) { buf := make([]byte, 256) // 读取 VER 和 NMETHODS n, err := io.ReadFull(client, buf[:2]) if n != 2 { return errors.New(\"reading header: \" + err.Error()) } ver, nMethods := int(buf[0]), int(buf[1]) if ver != 5 { return errors.New(\"invalid version\") } // 读取 METHODS 列表 n, err = io.ReadFull(client, buf[:nMethods]) if n != nMethods { return errors.New(\"reading methods: \" + err.Error()) } //无需认证 n, err = client.Write([]byte{0x05, 0x00}) if n != 2 || err != nil { return errors.New(\"write rsp: \" + err.Error()) } return nil } func Socks5Connect(client net.Conn) (net.Conn, error) { buf := make([]byte, 256) n, err := io.ReadFull(client, buf[:4]) if n != 4 { return nil, errors.New(\"read header: \" + err.Error()) } ver, cmd, _, atyp := buf[0], buf[1], buf[2], buf[3] if ver != 5 || cmd != 1 { return nil, errors.New(\"invalid ver/cmd\") } addr := \"\" switch atyp { case 1: n, err = io.ReadFull(client, buf[:4]) if n != 4 { return nil, errors.New(\"invalid IPv4: \" + err.Error()) } addr = fmt.Sprintf(\"%d.%d.%d.%d\", buf[0], buf[1], buf[2], buf[3]) case 3: n, err = io.ReadFull(client, buf[:1]) if n != 1 { return nil, errors.New(\"invalid hostname: \" + err.Error()) } addrLen := int(buf[0]) n, err = io.ReadFull(client, buf[:addrLen]) if n != addrLen { return nil, errors.New(\"invalid hostname: \" + err.Error()) } addr = string(buf[:addrLen]) case 4: return nil, errors.New(\"IPv6: no supported yet\") default: return nil, errors.New(\"invalid atyp\") } n, err = io.ReadFull(client, buf[:2]) if n != 2 { return nil, errors.New(\"read port: \" + err.Error()) } port := binary.BigEndian.Uint16(buf[:2]) destAddrPort := fmt.Sprintf(\"%s:%d\", addr, port) dest, err := net.Dial(\"tcp\", destAddrPort) if err != nil { return nil, errors.New(\"dial dst: \" + err.Error()) } n, err = client.Write([]byte{0x05, 0x00, 0x00, 0x01, 0, 0, 0, 0, 0, 0}) if err != nil { dest.Close() return nil, errors.New(\"write rsp: \" + err.Error()) } return dest, nil } func Socks5Forward(client, target net.Conn) { forward := func(src, dest net.Conn) { defer src.Close() defer dest.Close() io.Copy(src, dest) } go forward(client, target) go forward(target, client) } ","date":"2022-01-28","objectID":"/posts/network/socks5/0.html/:3:0","tags":["network","socks5"],"title":"simple socks5 proxy \u0026 the c10k problem","uri":"/posts/network/socks5/0.html/"},{"categories":["network"],"content":"参考 C10K问题 聊聊IO多路复用之select、poll、epoll详解 ","date":"2022-01-28","objectID":"/posts/network/socks5/0.html/:4:0","tags":["network","socks5"],"title":"simple socks5 proxy \u0026 the c10k problem","uri":"/posts/network/socks5/0.html/"},{"categories":["Go"],"content":"netrpc源码阅读 [源码地址] ","date":"2022-01-28","objectID":"/posts/golang/rpc/1.html/:0:0","tags":["rpc","框架设计","源码解析","Go"],"title":"netrpc源码阅读","uri":"/posts/golang/rpc/1.html/"},{"categories":["Go"],"content":"Client.go 核心结构体： Call：主要表示调用RPC过程中的信息 Client：主要负责RPC客户端 gobClientCodec：主要负责调用RPC具体实现方法 核心接口： ClientCodec 重点： ClientCodec接口规定了在调用过程最主要的几个具体步骤：WriteRequest()、ReadResponseHeader()、ReadResponseBody()、Close()； Client封装了RPC客户端未暴露的主要的两个动作即send()、input()，分别是发送操作和接收操作，利用互斥锁保证线程安全，其循环接收response且若出现错误则终结掉所有Call结构体，需要其中对于Call结构体的处理。而Client暴露的Go()和Call()就是其客户端库的入口，实质上是同步进行操作而异步进行请求具体方法返回其call信息，且区别主要在于前者多了一个done通道可以进行异步通信操作，Call()实质上也是对于Go()方法的调用； NewClientWith()方法即客户端初始化其中包含了NewClientWithCodec()，默认以gobClientCodec方式作为RPC调用具体实现方法，且初始化时协程开启进行input()函数进行接收； DailHTTP()方法和Dial()方法即客户端网络处理，前者包含了DialHTTPPath()方法即以HTTP协议进行通信，且默认为HTTP1.0协议，后者可以连接其特殊网络地址； ","date":"2022-01-28","objectID":"/posts/golang/rpc/1.html/:1:0","tags":["rpc","框架设计","源码解析","Go"],"title":"netrpc源码阅读","uri":"/posts/golang/rpc/1.html/"},{"categories":["Go"],"content":"Server.go 核心结构体： Request/Response：主要负责请求返回涉及rpc的信息 Server：主要负责RPC服务端 gobServerCodec：主要负责调用RPC具体实现方法 methodType/service：主要表示调用方法和注册服务的信息 核心接口： ServerCodec 重点： isExportedOrBuiltinType()方法判断类型是否为导出还是内置的，反射方法调用中需要保证其类型和方法为可导出的； Server封装了其服务端主要的方法，涉及到Register()、ServerConn()、ServerCode()、ServerRequest()、ServerResponse()、Accepct()、ServerHTTP()等； Register()方法及RegisterName()方法都是主要作用为注册服务，其中注册的服务中的参数格式和参数类型都有相关要求，涉及到参数个数、参数类型、参数是否可导出等，主要是利用反射获取相关信息进行判断； ServeCodec()方法和ServeRequest()方法就是接收处理请求，将得到的信息进行相应的Call操作，前者为循环接收及异步调用，且保证阻塞直至所有Call调用完成，后者则为一次性调用且为同步调用； 请求中的信息需要先进行解码后，经过处理后再编码发送其返回信息，其中call()就是处理过程中的核心方法，涉及invoke服务方法并编码进行返回； ServeHTTP()方法处理其服务端的网络连接，注册初始化服务端连接相关信息，进行HTTP1.0协议的通信； ","date":"2022-01-28","objectID":"/posts/golang/rpc/1.html/:2:0","tags":["rpc","框架设计","源码解析","Go"],"title":"netrpc源码阅读","uri":"/posts/golang/rpc/1.html/"},{"categories":["Go"],"content":"RPC框架设计概要 以下为个人理解，欢迎一起探讨 （个人理解）RPC 就是把拦截到的方法参数，转成可以在网络中传输的二进制，并保证在服务提供方能正确地还原出语义，最终实现像调用本地一样地调用远程的目的。 ","date":"2022-01-28","objectID":"/posts/golang/rpc/0.html/:0:0","tags":["rpc","框架设计","Go"],"title":"RPC框架设计概要","uri":"/posts/golang/rpc/0.html/"},{"categories":["Go"],"content":"RPC调用涉及到的环节 socket 协议 寻址 序列化和反序列化 异常处理 可以参考Dubbo框架的调用过程： ","date":"2022-01-28","objectID":"/posts/golang/rpc/0.html/:1:0","tags":["rpc","框架设计","Go"],"title":"RPC框架设计概要","uri":"/posts/golang/rpc/0.html/"},{"categories":["Go"],"content":"RPC框架需要解决的问题 开发效率 框架的初衷即是为了开发效率。 通信效率 RPC调用环节中的通信是不可避免的。 数据传输 需要注意序列化与反序列化，还有就是协议决定的传输效率。 通用化 做到所有的组件是微服务化，且组件支持业务自定义的。 服务治理 涉及到服务发现、负载均衡、超时、重试、限流、熔断、分布式链路追踪等。 单机架构： ","date":"2022-01-28","objectID":"/posts/golang/rpc/0.html/:2:0","tags":["rpc","框架设计","Go"],"title":"RPC框架设计概要","uri":"/posts/golang/rpc/0.html/"},{"categories":["Go"],"content":"源码阅读推荐 GoNetRpc(https://github.com/golang/go/tree/master/src/net/rpc) grpc(https://github.com/grpc/grpc) rpcx(https://rpcx.io/) ","date":"2022-01-28","objectID":"/posts/golang/rpc/0.html/:3:0","tags":["rpc","框架设计","Go"],"title":"RPC框架设计概要","uri":"/posts/golang/rpc/0.html/"},{"categories":["日常生活"],"content":"2021 又到了新的一年，想了想还是来总结下吧。 还是来梳理下这糟糕的一年吧： 1月：准备学校期末考试，月底实习入职； 2月：熟悉公司内部各种环境，接触分享方面业务； 3月：完成了在公司的第一个需求（属于优化的技术需求且难度不大）； 4月-5月上旬：完成了在公司的第一个P0级活动需求和复盘； 5月中下旬：完成了在公司的第一个重构优化方面的技术需求； 6月：回到学校准备期末考试和集中处理学校事务； 7月：深挖理解公司内部的某业务技术组件和熟悉交接过来的业务技术组件，并准备串讲和实习转正答辩； 8月-10月上旬：完成了在公司的第二个P0级活动需求和复盘； 10月中下旬：实习转正答辩顺利通过； 11月-12月：实习前最后的活动项目需求进行中； 概括下来其实大部分就是来到北京之后的实习生活，虽然逐渐沦为打工人的形状，但还是尝试了很多新的事物。 做得好的地方看下来，最为重要的就是熟悉了项目工程化和更为深刻的业务场景理解。 遗憾的地方看下来，缺少让自己满意的亮点和养成了各个场景容易形成惯性的坏毛病。 有很多想做的事情因为自己的懒惰和效率，暂时被搁置在一旁，总得来说还是变懒了呀。 在按照惯例梳理2022年的flag之前，先看看去年的flag完成情况： 满意的实习经历 理想的转正工作 拒绝挂科 完成学习清单上面的任务(1/5) 书单进度(1/10) 写一个属于自己的项目 Begonia(0%) 红岩网校工作站工作圆满结束 开创个人微信公众号兼博客(1%) 为网校在技术方面贡献力量(0%) 健身(5%) 每周跑步两次(10%) 因为实习工作的事情，无论是心态还是时间，好多flag都没有完成，也是遗憾的一点。 那么2022年的flag希望都能够实现吧。 技术学习相关： 实习\u0026正式相关工作达到预期 毕业设计 书单\u0026课程进度（架构方向、技术基础方向、Rust方向） Begonia 落实自己满意的亮点（性能优化方向、技术组件方向、业务场景方向） 经营微信公众号兼博客 做到独当一面要求（工程研发模型） 个人生活相关： 滑板\u0026篮球\u0026跳舞 尝试健身 每周运动 尝试写作 ","date":"2022-01-28","objectID":"/posts/daily_life/life_record/1.html/:0:0","tags":["生活随笔","年度总结","日常生活"],"title":"2021年度总结","uri":"/posts/daily_life/life_record/1.html/"},{"categories":["日常生活"],"content":"2020 又到了一年的末尾，见到周围许多人都开始书写自己的年终总结，虽然每一年自己都有这个想法，但是每次写到一半的时候心情就开始发生一些变化，可能回想着其他人的总结与自己的对比会产生的落差感，也可能回想着自己没有实现的目标会觉得有很多遗憾，最后全选所选内容删除，以复杂的心情为自己的一年画上不完美的句号。 于是又到今年的这一天，想下定决心认真写下自己的年终总结。首先我不由得思考年终总结的意义，又或者我有没有必要花上这段时间去做事情，因为很多时候，我总是会被周围的想法所引导，这是一个不好的习惯，由此也可以看出我在对于某件事情若不够自信，就会不自觉地会被他人所影响，从而放弃自己的思考。 回到年终总结的意义，应该是对于生活的记录，对于自我的反思，对于他人的分享。随着年龄越来越大，越发感觉时间越走越快，对于以前不喜欢记录生活的我，现在也变得越发珍惜如今的生活。说起长大的变化，可能最大的就是自我的反思，见识得越多越能认识到自己的渺小与无助，总是觉得自己还缺少些什么，无论与他人相比还是与自己相比，总是不能让自己满意。 好了，说了这么多还是来回顾一下这糟糕的一年吧。 1月：学校期末考试的阶段，除了每天复习，也在忙着学习springboot2、python、卷积分神经网络相关的，当时对于计算机视觉这一块比较感兴趣，还是花了很多时间在这上面的。 2月：寒假期间发生的疫情打乱了过年的计划，做的事情比较少，大概还是在学习MySQL高性能分析和卷积分神经网络相关的，还有就是在做Go网盘项目和掌上重邮项目。 3月-5月：在家无论是上课还是学习，学习效率都比较低，其中还尝试开学习直播之类的，期间一直也在学习机器学习相关，在忙着轻语项目和一些网课实战的项目。 6月：回到学校就开始组织的换届和期末考试了，虽然之前有很多顾虑，但是想了想还是决定去竞选组织的负责人，最后学长学姐也比较信任我，重任也就担任在我身上了。 7月-9月：这段时间开始接手和管理组织的所有事情，同时也在忙着重邮帮微信小程序的项目，算下来一共写了差不多有将近60多个接口，第一次写这么大的项目，收获还是蛮多的，这个时间段几乎组织大多项目我都在接手，因为短时间还不能转接给干事。一边在处理组织的事情（也是组织事情最多的时间段），也一边写着项目，这段时间过得非常累，可能不太适应领导者的身份，还不太习惯将任务事情分担。对了这段事情也在学习前端的相关知识，但是学习的时间不是很多。 10月：这个月在渐渐把项目转接给学弟，自己也适应了组织领导者的身份，虽然事情还是比较多，但是没有之前那么慌乱和心急了。同时这个月接触到分布式系统架构还有就是CNCF相关的项目，自己觉得挺感兴趣的，但是无奈要学习的东西太多了，平时也开始逛Go社区论坛之类的。 11月：还是一如既往处理组织上的事情，同时也在准备开始刷leetcode算法题，也在学习K8S源码相关和Istio服务网格相关，平常也在阅读一些博客文章之类的，月末的时候就开始着手准备面试的事情了。 12月：较为心累的一个月，那就是面试的折磨，无论是准备面试还是等待面试，每一天都过得胆战心惊，看着周围的人都陆陆续续收到offer，心里面都比较着急，但还好自己最后也收到了腾讯和字节的offer，算是给面试生活画上句话。 总的来看，上半年接触到的机器学习、卷积分神经网络、springboot2等，虽然与我最后的就业方向不同但还是学习到了很多东西，拓宽了自己的领域，下半年开始渐渐找到自己的学习状态和学习方式，摆脱了以前被动的状态，开始找到自己感兴趣的方向和知识去学习。 这一年在技术方面并没有太多成长，算是自己的一个遗憾，希望下一年能够好好把握时间认真学习；而在作为领导者方面，自己也有很多不足的地方需要多多改进，唯一的遗憾是还并没有对组织技术方面带来一些较大的帮助，下一年会慢慢去做这件事情。 按照惯例，明年的flag有以下，希望都能实现： 满意的实习经历 理想的转正工作 拒绝挂科 完成学习清单上面的任务 书单进度 写一个属于自己的项目 Begonia 红岩网校工作站工作圆满 开创个人微信公众号兼博客 为网校在技术方面贡献力量 补充一些个人生活方面的flag吧，希望都能实现： 健身 每周跑步两次 ","date":"2022-01-28","objectID":"/posts/daily_life/life_record/0.html/:0:0","tags":["生活随笔","年度总结","日常生活"],"title":"2020年度总结","uri":"/posts/daily_life/life_record/0.html/"},{"categories":["日常生活"],"content":"（WIP）书单进度 后续会持续补充 ","date":"2022-01-28","objectID":"/posts/daily_life/reading/0.html/:0:0","tags":["书单","WIP","日常生活"],"title":"（WIP）书单进度","uri":"/posts/daily_life/reading/0.html/"},{"categories":["日常生活"],"content":"非技术 ","date":"2022-01-28","objectID":"/posts/daily_life/reading/0.html/:1:0","tags":["书单","WIP","日常生活"],"title":"（WIP）书单进度","uri":"/posts/daily_life/reading/0.html/"},{"categories":["日常生活"],"content":"心理学 《蛤蟆先生去看心理医生》（20220129 done） 《认知觉醒》（70%） ","date":"2022-01-28","objectID":"/posts/daily_life/reading/0.html/:1:1","tags":["书单","WIP","日常生活"],"title":"（WIP）书单进度","uri":"/posts/daily_life/reading/0.html/"},{"categories":["日常生活"],"content":"文学 《晚熟的人》(10%) ","date":"2022-01-28","objectID":"/posts/daily_life/reading/0.html/:1:2","tags":["书单","WIP","日常生活"],"title":"（WIP）书单进度","uri":"/posts/daily_life/reading/0.html/"},{"categories":["日常生活"],"content":"技术相关 ","date":"2022-01-28","objectID":"/posts/daily_life/reading/0.html/:2:0","tags":["书单","WIP","日常生活"],"title":"（WIP）书单进度","uri":"/posts/daily_life/reading/0.html/"},{"categories":["日常生活"],"content":"机器学习 《白话机器学习的数学》(20220227 done) ","date":"2022-01-28","objectID":"/posts/daily_life/reading/0.html/:2:1","tags":["书单","WIP","日常生活"],"title":"（WIP）书单进度","uri":"/posts/daily_life/reading/0.html/"},{"categories":["日常生活"],"content":"《蛤蟆先生去看心理医生》阅读 以下为阅读《蛤蟆先生去看心理医生》的摘抄和笔记 ","date":"2022-01-28","objectID":"/posts/daily_life/reading/2.html/:0:0","tags":["note","心理学","日常生活"],"title":"《蛤蟆先生去看心理医生》阅读","uri":"/posts/daily_life/reading/2.html/"},{"categories":["日常生活"],"content":"文章摘录 ","date":"2022-01-28","objectID":"/posts/daily_life/reading/2.html/:1:0","tags":["note","心理学","日常生活"],"title":"《蛤蟆先生去看心理医生》阅读","uri":"/posts/daily_life/reading/2.html/"},{"categories":["日常生活"],"content":"儿童自我状态 自然型儿童（基础情感下的模式，如快乐和深情、愤怒、悲伤、恐惧等） 适应型儿童（在基础情感之上适应不同类型的依赖得到的模式，如为适应严厉的父母而掩饰自己的悲伤恐惧等） ","date":"2022-01-28","objectID":"/posts/daily_life/reading/2.html/:1:1","tags":["note","心理学","日常生活"],"title":"《蛤蟆先生去看心理医生》阅读","uri":"/posts/daily_life/reading/2.html/"},{"categories":["日常生活"],"content":"儿童如何释放愤怒 强\u003c-\u003e弱： 叛逆\u003c-撒泼｜怄气｜任性｜郁闷｜拖延｜厌烦-\u003e退缩 怄气是输家在对强大的赢家做出反应。 当成年人怄气、撒泼、郁闷或是厌烦的时候，我们会想他们究竟是行为不当，还是无意识或无法控制地重演童年的行为模式。 这类行为会导致两个后果，都是负面的。第一个就是会被嘲笑，而更严重的后果是这类行为告诉别人，这人是个失败者。 ","date":"2022-01-28","objectID":"/posts/daily_life/reading/2.html/:1:2","tags":["note","心理学","日常生活"],"title":"《蛤蟆先生去看心理医生》阅读","uri":"/posts/daily_life/reading/2.html/"},{"categories":["日常生活"],"content":"秘密协议 ‘共谋’： 偷偷地或无意识地配合对方，来给自己制造不快，这就是在玩心理游戏，而且在游戏里输的人才算是赢家。 PLOM(可怜弱小的我呀) 情商： 聪明不仅仅是智商的事儿，我们也需要情商。 打破心理平衡： 乍一听会显得愚蠢、不合逻辑，甚至让人害怕。但是越是能帮助你深入自我的概念，也越容易引发激烈的阻抗。 这些概念最容易打破我们的心理平衡，它们最有可能带你走向深层的蜕变，而这个过程往往是痛苦的，我想你现在也感受到了。我们看到的自己，并不一定总是我们喜欢的样子。从当下的你，变成你想成为的自己，必定要经历行为和态度的转变，需要付出艰辛的努力，需要勇气和决心。所以你现在应该懂了，为什么你会拒绝打开这扇学习之门，因为它通向一条艰苦之路。 这扇门也可能通向深刻的领悟。 ","date":"2022-01-28","objectID":"/posts/daily_life/reading/2.html/:1:3","tags":["note","心理学","日常生活"],"title":"《蛤蟆先生去看心理医生》阅读","uri":"/posts/daily_life/reading/2.html/"},{"categories":["日常生活"],"content":"父母状态 父母状态： 挑剔型父母人格 困惑阶段： 困惑是学习过程的第一阶段，即说明固有知识的局限开始被打破了，直面新的信息来会挑战现有的观念和行为模式。 困惑产生的焦虑是让你改变的动力，很可能也会开启你的创造力。 父母状态下的自我批判： 自我批判最为强烈，因为没有一个法官比我们自己更严苛。 自我批判会严厉地惩罚，包括折磨自己，在极端案例里，甚至会施以极刑。但问题是，即便对自己轻判，这种谴责和惩罚也可能伴随一生，变成无期徒刑。 ","date":"2022-01-28","objectID":"/posts/daily_life/reading/2.html/:1:4","tags":["note","心理学","日常生活"],"title":"《蛤蟆先生去看心理医生》阅读","uri":"/posts/daily_life/reading/2.html/"},{"categories":["日常生活"],"content":"成人状态 成人自我状态： 理性而非情绪化行事，也应对着此时此地正在发生的现实状况。 父母成人儿童的不同： 在成人状态下：能计划、考虑、决定、行动，能理性而合理地行事。处于这个状态时，我们所有的知识和技能都能为自己所用，而不再被脑子里父母过去的声音所驱使，也不会被童年的情绪所围困。相反，我们能思考当下的状况，基于事实来决定要怎么做。只有在‘成人自我状态’里，才能学到关于自我的新知识。 在儿童状态下：体验到童年的感受，好的坏的都有。你会再现过去的情形，再次体验过去的情绪，可你学不到任何新的东西。 在父母状态下：基本上你不是在挑剔就是在教育别人。不管是哪种，你都在用言行重复从父母那里学来的观念和价值观，你会想证明给别人看，让别人接受你的观念和价值观。这种确信无疑的状态，就没法给新知识和新理念留出一席之地。旧的思想主宰着你，这就是为什么单靠争论不能改变一个人的想法，只会让人更固执己见。 ","date":"2022-01-28","objectID":"/posts/daily_life/reading/2.html/:1:5","tags":["note","心理学","日常生活"],"title":"《蛤蟆先生去看心理医生》阅读","uri":"/posts/daily_life/reading/2.html/"},{"categories":["日常生活"],"content":"如何进入成人状态 进入成人状态前的选择： 难，是因为这个过程需要艰辛的努力和刻意的思考。我们在另外两种状态时，像父母或儿童一样行事，几乎不需要去思考，因为我们知道要做什么、说什么，就好像在演戏一样。 我认为没有人能‘让’我们产生什么感受，除非他们用蛮力胁迫你。说到底，是我们‘选择’了自己的感受。我们‘选择’了愤怒，我们‘选择’了悲伤。 是的，选择就是：你还要为自己的不快乐责怪别人多久？ 我不确定是否懂你的意思，你是说，我该为自己的行为负责？ 还包括你自己的情绪，这才是成年人会做的事情。毫无疑问，这很难，但相比于责怪别人，它还真有个天大的好处。 就是，你能开始对此行动了。如果你为自己负责，就会认识到你对自己是有自主权的。因此你就知道自己有力量来改变处境，更重要的是，有力量改变你自己。 “原谅他们。” 活得真实： 所谓活得真实，就是真诚地回应当下的需求。这能打破从童年延续而来的因果循环，让真实的自我摆脱过去经历的束缚，在自由中成为真正的自己。 剧本设计： 人生的剧情若能避开‘残暴命运如投石飞箭般的摧残’，该多好。我们在之后可以探究一下怎样应对那样的状况。不过，我理解你的意思是说，你想探索你的基本态度和行为源自哪里，对吗？ 是的，很不同。不过要记住，我们说的不只是物理世界，而是你内在的、包含着情绪和情感的心理世界，那是通过你早年的经历而形成的。童年的经历如此强大、如此鲜活，于是便塑造了每个孩子对世界的独特看法。换句话说，外面的世界变成了在我这里的世界。”苍鹭边说，边拍了拍胸口的位置。“无论你对生活形成了怎样的态度，从此你的行为和幸福感都会受到影响，往后余生都会如此。除非——”此时，苍鹭直视着蛤蟆，“你决心要改变。” 恐怕没有别的办法。每一个生命一定都得经历开始、中间和结束这三个阶段，而开始的阶段会显著地影响后来的阶段。因此你对世界的看法是在人生的最初阶段里形成的。 ","date":"2022-01-28","objectID":"/posts/daily_life/reading/2.html/:1:6","tags":["note","心理学","日常生活"],"title":"《蛤蟆先生去看心理医生》阅读","uri":"/posts/daily_life/reading/2.html/"},{"categories":["日常生活"],"content":"人生坐标和心理游戏 人生坐标： “第一个问题是：‘我是怎么看自己的？我好吗？’第二个问题是：‘我是怎么看别人的？他们好吗？’” “一切的关键就在于那是‘人生坐标’。一旦我们在童年决定用哪种态度和观点，我们就会在随后的人生里始终坚持自己的选择。这些态度和观点，变成我们存在的底层架构。从那以后，我们便建构出一个世界，不断确认和支持这些信念和预期。换一个词来说，我们把自己的人生变成了一个‘自证预言’。” 心理游戏： “确实如此。这类游戏的发起基本上都不是出于真诚，不像正常游戏那样只是让人觉得兴奋好玩，而是会产生非常戏剧化的结果。表面看起来实事求是，其实真正的意图却并不正大光明。游戏体现在两个层面：在社交层面上，似乎一切都是公开诚实的。而游戏玩家的真正动机却隐藏在心理层面，同时也隐藏着欺骗。至于游戏的必然后果呢，全都是让人产生负面情绪。” 一方面，他努力想要在理性层面上理解这些理念，可在更深的潜意识层面，它们却也触动了他的自我，让他情绪混乱。他想一个人静静地让这些理念慢慢渗透，让思绪跟随到该去的地方。他不知道会探索到哪里，但肯定是去往成长的方向 ","date":"2022-01-28","objectID":"/posts/daily_life/reading/2.html/:1:7","tags":["note","心理学","日常生活"],"title":"《蛤蟆先生去看心理医生》阅读","uri":"/posts/daily_life/reading/2.html/"},{"categories":["日常生活"],"content":"人生坐标：我不好，你好 我不好，你不好： “这代表了一个人的行为态度，这类人认为自己很差劲，别人都比他好。” 几乎在任何方面。低自尊的人通常觉得生活对他们不好，却更善待别人。概括地说，处在这个坐标的人认为自己是生活的受害者，所以他们就玩那些会把他们变成受害者的游戏。” “我说的是更严重的情况，有些人会竭尽所能地选择记住那些悲伤和不快乐的事件，而忘记或忽略美好的时光。 受害者游戏： “因为玩这个游戏的人确实会抑郁。他们认为自己的人生被不好的力量影响，无法掌控人生，这让他们焦虑，觉得自己不够好。” “不过我不是在怪你玩游戏。我的目的不是责怪你，而是帮助你看到你在玩什么样的游戏，这样你才可以就此打住。” “我同意。你现在开始明白这些游戏有多危险了，它们会严重伤害你的健康。” 悲伤的儿童状态。 ","date":"2022-01-28","objectID":"/posts/daily_life/reading/2.html/:1:8","tags":["note","心理学","日常生活"],"title":"《蛤蟆先生去看心理医生》阅读","uri":"/posts/daily_life/reading/2.html/"},{"categories":["日常生活"],"content":"人生坐标：我好，你不好 我好，你不好： “我想是的。这描述的是觉得自己比别人好的那一类人。我想他们会玩游戏去证实这一点，对吗？” “是的，确实如此。这类游戏通常能让玩家感到愤怒，或者至少能让他们对别人评头论足。处于这个心理坐标的人常常会占据权力和权威的制高点，这样就能玩他们的游戏了。” 他们会证明别人根本上都是无能而不可信的，接下来，他们会把斥责和惩罚别人视为己任。他们会说：‘要不然，那帮人会觉得犯了错还能侥幸逃脱！’” NIGYYSOB（我抓到你了，你个坏蛋） 你为什么总让我失望？ 你怎么敢？ 内心的施虐者： “完全正确。这些施虐者利用任何时机来制造一些能让他们评判和惩罚别人的情境。是他们内心的施虐者让他们这么做，可内心的施虐者是谁呢，这是个有意思的问题。” “永远都是‘挑剔型父母状态'，当然了，有时候他们会摆出‘养育型父母’的姿态。” “他们从来不会抑郁。因为愤怒能够非常有效地抵抗抑郁。愤怒的人从不觉得内疚，因为他们总在怪罪别人。他们自卫的方式，是把自己内在的恐惧对外投射到别人身上，这样就能把对自己的怒火转向别人。” ","date":"2022-01-28","objectID":"/posts/daily_life/reading/2.html/:1:9","tags":["note","心理学","日常生活"],"title":"《蛤蟆先生去看心理医生》阅读","uri":"/posts/daily_life/reading/2.html/"},{"categories":["日常生活"],"content":"理解自己的行为 理解自己的行为： “不，当然不是。这些理念不是用来给人贴标签，攻击羞辱别人的。它们只是用来理解行为的方法，尤其是理解我们自己的行为。” ","date":"2022-01-28","objectID":"/posts/daily_life/reading/2.html/:1:10","tags":["note","心理学","日常生活"],"title":"《蛤蟆先生去看心理医生》阅读","uri":"/posts/daily_life/reading/2.html/"},{"categories":["日常生活"],"content":"人生坐标：我好，你也好 我好，你也好： “这是个勇敢的选择。” “说‘勇敢’是因为选择了这个人生坐标时，你不仅为当下做出了选择，还许下了一个承诺，终生的承诺。” “我的意思是，‘我好；你也好’的人生坐标并非静止的状态，而是动态的过程。你不能说‘好了，我终于到了’，好像登顶珠穆朗玛峰一样。你觉得自己是好的，也相信别人是好的，那就得靠行为和态度持续地对自己和别人展示出来。而这个选择肯定不能给你庇护，免于‘残暴命运如投石飞箭般的摧残’。” ","date":"2022-01-28","objectID":"/posts/daily_life/reading/2.html/:1:11","tags":["note","心理学","日常生活"],"title":"《蛤蟆先生去看心理医生》阅读","uri":"/posts/daily_life/reading/2.html/"},{"categories":["日常生活"],"content":"道别与新生 移情： “因为你把对父亲的情感转移到我身上，这叫作‘移情’。通过对我表达自己的想法，你终于也能对他表达了。你真正找到了力量和勇气，表现得像个男人，而不再是个男孩。那是个转折点，你成长了，也成年了，在你宣告对自己拥有主权的那刻起，你便能独立行事了。” 情绪是真正的核心： “在咨询过程中，我们不仅用头脑去思考，也用情感去体验。虽然你开始在理智上理解自己的行为，但要充分理解自我，唯有通过和自己的情绪做联结。当你对情绪的感受越来越清晰时，就能明白它们并非可有可无，也不会对它们不闻不问，因为情绪正是自我的核心。” 情商： “意思是理解你内心的情感世界，并且还能掌控它。你也能看出来，这和智商完全不是一回事。” “概括而言，他们都有强大的自我意识，了解自己的情感。他们能管理情绪，能从悲伤和不幸中重新振作。但也许最重要的是，他们能控制冲动，也懂得延迟满足，从而避免轻率的决定和不妥的行为。” ““有。情商和理解别人有关，一个高情商的人能辨识他人的感受，这种技能称为‘共情’。但也许情感智力中最大的技能是通过理解和回应对方的情感，与他人建立良好的关系。” 独立与共生： “情感智力能让你在自我成长和完善的路上走得更远，因为它将带你从独立的个体走向共生的关系。” “独立性隐含了做自己的自豪感，还包括独特的才能、与众不同的部分。独立的人时刻守护新发现的自主权，如同一个曾被殖民的国家重获自由一样。这当然没错，但共生性则体现出成熟和自我接纳，还包括求同存异地接纳他人。共生性可以让你在社交和工作上与别人有效联结，协同合作。” ","date":"2022-01-28","objectID":"/posts/daily_life/reading/2.html/:1:12","tags":["note","心理学","日常生活"],"title":"《蛤蟆先生去看心理医生》阅读","uri":"/posts/daily_life/reading/2.html/"},{"categories":["日常生活"],"content":"总结\u0026感悟 读完这本书，其实没想象中的花了那么多时间，加起来阅读时间应该有5小时左右。 我觉得这本书的特点就体现在： 通过\"蛤蟆\"的自我对话和\"蛤蟆\"与\"苍鹭\"的对话，来让我们有足够的时间和耐心去代入进去，专注进去； 本文出现的心理学知识或者理念应该是非常容易理解的，能让我们有足够的基础去反思我们的行为，理解我们的行为； 通读下来，在于理解以下核心点： 儿童/成人/父母 心理游戏\u0026人生坐标 情商\u0026情绪 独立\u0026与共生 通过自白和对话，渐渐了解这些的含义理念，然后反思，直到最后理解意义。每个过程都是自然而然地，让我感觉非常舒适和平静。 就像书中提到的，理念不是用来给人贴标签，攻击羞辱别人的。它们只是用来理解行为的方法，尤其是理解我们自己的行为。 只有理解自己的行为，理解自己的情绪，进入自己的内心情感世界，才能够掌控，才能够管理情绪，提高自己的情商。 同时也让我理解到\"情商\"的重要性，它不仅仅是理解和回应对方的情感，还有理解和回应自己的情感，这对于我们人生的美好是不可或缺的。 书中最后提到了\"独立\"和\"共生”，我想就是\"独立\"是当前的自己与过去的自己和解，“共生\"是当前的自己与未来的自己和解。 ","date":"2022-01-28","objectID":"/posts/daily_life/reading/2.html/:2:0","tags":["note","心理学","日常生活"],"title":"《蛤蟆先生去看心理医生》阅读","uri":"/posts/daily_life/reading/2.html/"},{"categories":["日常生活"],"content":"《认知觉醒》阅读 以下为阅读《认知觉醒》的摘抄和笔记 持续更新中………. ","date":"2022-01-28","objectID":"/posts/daily_life/reading/1.html/:0:0","tags":["note","心理学","日常生活"],"title":"《认知觉醒》阅读","uri":"/posts/daily_life/reading/1.html/"},{"categories":["日常生活"],"content":"上篇：内观自己，摆脱焦虑 ","date":"2022-01-28","objectID":"/posts/daily_life/reading/1.html/:1:0","tags":["note","心理学","日常生活"],"title":"《认知觉醒》阅读","uri":"/posts/daily_life/reading/1.html/"},{"categories":["日常生活"],"content":"第一章 大脑—一切问题的起源 理智脑\u003c情绪脑\u003c本能脑。 生活中所做的大部分决策往往源于本能和情绪，而非理智。 避难趋易和急于求成。 大多数时候我们以为自己在思考，其实都是在对自身的行为和欲望进行合理化。 习惯之所以难以改变，就是因为它是自我巩固的——越用越强，越强越用。要想从既有的习惯中跳出来，最好的方法不是依靠自制力，而是依靠知识。 因为本能脑强大的运算能力和情绪脑强大的行动能力，都是不可多得的宝贵资源。 理智脑不是直接干活的，干活是本能脑和情绪脑的事情，因为它们的“力气”大；上天赋予理智脑智慧，是让它驱动本能和情绪，而不是直接取代它们。 无论个体还是群体，人类的安全感都源于自己在某一方面拥有的独特优势：或能力，或财富，或权力，或影响力。 焦虑的几种形式：完成焦虑，定位焦虑，选择焦虑，环境焦虑，难度焦虑。 急于求成，想同时做很多事；避难趋易，想不怎么努力就立即看到效果。 ","date":"2022-01-28","objectID":"/posts/daily_life/reading/1.html/:1:1","tags":["note","心理学","日常生活"],"title":"《认知觉醒》阅读","uri":"/posts/daily_life/reading/1.html/"},{"categories":["日常生活"],"content":"第二章　潜意识——生命留给我们的彩蛋 模糊的意识简化运作流程，但也有副作用。 思考和学习是本能生存的威胁。 学习知识的目的是“消除模糊”，而获取知识的方法也是“消除模糊”，目的和方法相统一。 提升思考能力的方法正是不断明确核心困难和心得感悟，并专注于此。 具体事件一旦变模糊，其边界就会无限扩大，原本并不困难的小事，也会在模糊的潜意识里变得难以解决。 要想不受其困扰，唯一的办法就是正视它、看清它、拆解它、化解它，不给它进入潜意识的机会，不给它变模糊的机会；即使已经进入潜意识，也要想办法将它挖出来。所以，当你感到心里有说不清、道不明的难受的感觉时，赶紧坐下来，向自己提问。 恐惧就是一个欺软怕硬的货色，你躲避它，它就张牙舞爪，你正视它，它就原形毕露。 行动力不足的真正原因是选择模糊。 选择模糊就是一种不确定性，而人类面对不确定性时会不自觉逃避， 当我们没有足够清晰的指令或者目标时，就很容易选择享乐，放弃那些本该坚持但比较烧脑的选项。 在诸多可能性中建立一条单行通道，让自己始终处于“没得选”的状态。 感性的处理能力远大于理性，潜意识就是感性的一部分，俗称“凭感觉”。 先用感性能力帮助自己选择，再用理性能力帮助自己思考。 感知真正适合自己并需要的东西，让自己处于学习的“拉伸区”。 只取一个全书最触动自己的点，然后尽可能去实践、改变。这样读书不仅收获更大，而且也不会焦虑。 目标是存放我们热情和精力的地方。 用感知力来代替思考力。 ","date":"2022-01-28","objectID":"/posts/daily_life/reading/1.html/:1:2","tags":["note","心理学","日常生活"],"title":"《认知觉醒》阅读","uri":"/posts/daily_life/reading/1.html/"},{"categories":["日常生活"],"content":"第三章　元认知——人类的终极能能力 元认知，就是最高级别的认知，它能对自身的“思考过程”进行认知和理解。 被动元认知和主动元认知。被动到主动是个转折点。 你能意识到自己在想什么，进而意识到这些想法是否明智，再进一步纠正那些不明智的想法，最终做出更好的选择。 获取元知力，第一通过学习前人的智慧和反思自身的经历，第二通过自身的经历，第三通过召唤“灵魂伴侣”，第四通过冥想。 元认知能力总能让你站在高处俯瞰全局，不会让你一头扎进生活的细节，迷失其中。 反馈是这个世界的进化机制，而元认知就是认知能力的反馈回路。 元认知能力就是自我审视，主动控制，防止被潜意识左右的能力。 理智脑的战斗力表现在两方面：一方面是侧重学习、理解、记忆、运算的认知能力，即我们在校学习时主要锻炼的部分，另一方面则是侧重观察、反思、判断、选择的元认知能力。 每当遇到需要选择的情况时，我们要是能先停留几秒思考一下，就有可能激活自己的理智脑，启用元认知来审视当前的思维，然后做出不一样的选择。 在选择节点上多花“元时间”。 元时间通常分布在“选择的节点”上，比如一件事情、一个阶段或一天开始或结束时。善用这些时间会极大程度地优化后续时间的质量。 ","date":"2022-01-28","objectID":"/posts/daily_life/reading/1.html/:1:3","tags":["note","心理学","日常生活"],"title":"《认知觉醒》阅读","uri":"/posts/daily_life/reading/1.html/"},{"categories":["日常生活"],"content":"下篇　外观世界，借力前行 ","date":"2022-01-28","objectID":"/posts/daily_life/reading/1.html/:2:0","tags":["note","心理学","日常生活"],"title":"《认知觉醒》阅读","uri":"/posts/daily_life/reading/1.html/"},{"categories":["日常生活"],"content":"第四章　专注力——情绪和智慧的交叉地带 第一节　情绪专注：一招提振你的注意力 分心走神的本质是逃避，所以，面对困难时，身心分离的人总会不自觉地退回舒适区，而身心合一的人则更容易跳出舒适区，直面困难。 让感受回归行动。 身体感受永远是进入当下状态的最好媒介，而感受事物消失的过程更是一种很好的专注力训练。它提示我们，身心合一的要领不仅是专注于当下，更是享受当下，而这种享受必将使我们更从容，不慌张。 第二节　学习专注：深度沉浸是进化双刃剑的安全剑柄 审视自己的注意力——是被动吸引还是主动选择？ 审视自己的沉浸度——是分心走神还是极度专注？ 审视自己的练习量——是浅尝辄止还是大量投入？ ","date":"2022-01-28","objectID":"/posts/daily_life/reading/1.html/:2:1","tags":["note","心理学","日常生活"],"title":"《认知觉醒》阅读","uri":"/posts/daily_life/reading/1.html/"},{"categories":["日常生活"],"content":"第五章　学习力——学习不是一味地努力 第一节　匹配：舒适区边缘，适用于万物的方法论 将事物努力的方向匹配到舒适区边缘。 因为距离我们太远的事物，我们通常无法把握，无论它们是令人痛苦的还是令人享受的。 做选择是一件极为耗能的事情，如果没有与之匹配的清醒和定力，绝大多数人最终都会被强大的天性支配，去选择娱乐消遣。在有约束的环境下我们反而效率更高，生活更充实。 能解释的现象越多，这个概念就越底层。 不管做什么，不管当前做得怎么样，只要让自己处在舒适区的边缘持续练习，你的舒适区就会不断扩大，拉伸区也就会不断扩展，原先的困难区也会慢慢变成拉伸区，甚至是舒适区，所以成长是必然的。同时，我们也可以肯定：速成是不可能的。 从舒适区到拉伸区的策略：提炼目标。 在拉伸区练习的一大特点就是要有关注点。 第二节　深度：深度学习，人生为数不多的好出路 高效地获取知识，实际上抑制了深度思考的意识。 只满足于输入的过程，这类学习的知识留存率很低。 深度学习会要求你必须动用已有的知识去解释新知识。 “缝接”是深度学习的关键，而大多数人只完成了“获取知识”，却忽略了“缝接知识”这一步，因此，他们的学习过程是不完整的。 浅层学习满足输入，深度学习注重输出。 逼迫自己获取高质量的知识以及深度缝接新知识，再用自己的语言或文字教授他人，是为深度学习之道。 正确的行动往往是反天性的。 逐步改进深度学习的方法： 一是尽可能获取并亲自钻研一手知识。 二是尽可能用自己的话把所学的知识写出来。 三是反思生活。 深度学习除了能让我们不再浮躁，能磨炼理智，还能带来诸多好处，比如跨界能力的提升。 深度学习还能让人产生更多灵感。 与此同时，深度学习还能让我们看到不同事物之间更多的关联，产生洞见。 深度之下的广度才是有效的。 专注于深度学习，同时对浅学习保持开放。 第三节　关联：高手的“暗箱” 读书的三个步骤： 用自己的语言重述信息，即找到触动自己的信息点； 描述自己的相关经验，即关联生活中的其他知识； 我的应用，即转化为行动，让自己切实改变。 深度学习的三个层次： 知道信息点 关联信息点 行动和改变 绝大多数人习惯以孤立的思维看待事物，喜欢花大量时间收集和占有信息；而另一批先行者则更喜欢拨弄信息之间的关联，从而在不知不觉间变得聪明了起来。 我们在关联时，需要牢牢聚焦自身最迫切的需求，换句话说，就是让一切与自己有关。 知识的获取不在于多少，而在于是否与自己有关联，以及这种关联有多充分。 更重要的隐蔽条件不能忽视：你需要明确的目标或强烈的需求。 第四节　体系：建立个人认知体系其实很简单 因为个人成长的目的已经不是“知道和理解”了，而是“判断与选择”。 所以在个人成长领域，没有最优、最确定、最权威的认知体系，只有最适合我们当前状态的认知体系。 认知也是一种能力，同样遵循这个原理：处于认知圈边缘的知识与我们的实际需求贴合得最紧密，因此也更容易让我们产生触动，进而与现有的知识进行关联。而他人认知体系中的很多知识，纵使再有道理，如果距离我们的认知或需求太远，就相当于处在了学习的困难区。 除非对方的认知体系刚好和自己的认知圈比较匹配，否则痴迷于全盘接受，学习效果有限，还很浪费 体系的本质就是用独特的视角将一些零散的、独立的知识、概念或观点整合为应对这个世界的方法和技巧。 打碎各家的认知体系，只取其中最触动自己的点或块，然后将其拼接成自己的认知网络。 我们潜意识的感性能力完全可以作为学习的筛选器——通过情绪触动，识别与自身需求结合最紧密的内容。 产生“触动”不意味着“连接”紧密。 有效关联新知识的三个方面： 第五节　打卡：莫迷恋打卡，打卡打不出未来 打卡不仅会转移行动的动机，还会降低行动的效能，学习的偶尔中断是正常的。 所谓认知闭合需求，就是指当人们面对一个模糊的问题时，就有给问题找出一个明确的答案的欲望。 打卡心态的特性：学不到，假装一下；学到了，立即停止。 所谓“大美不自知”，破除任务心态的方法就是集中心力做眼前的事就好。 用记录代替打卡。 在任务设置时要使用新策略：设下限，不设上限。 保持动机，创造动机。 第六节　反馈：是时候告诉你什么是真正的学习了 是否有及时、持续的正向反馈，正是产生学习效果差异的关键。 正向反馈就是对本能脑和情绪脑的鼓励，使之驱动作为动力，然后也带动理智脑的驱动。 拥有清晰而强烈的作品意识，会更加重视输出和运用，会倾心打磨作品，主动换取外界的反馈。 “锁定价值—打磨作品—换取反馈”。 所有痛苦都是上天给我们的成长提示，这也是给自己的反馈。 无反馈式努力正是你被动、落后的根源。 真正的学习成长不是“努力，努力再努力”，而是“反馈，反馈再反馈”。 第七节　休息：你没成功，可能是因为太刻苦了 “刻意练习四要素”：定义明确的目标、极度的专注、有效的反馈、在拉伸区练习。 不过度消耗自己，当精力不足学会主动休息，使精力桶的水位得到快速回升。 高效学习的关键在于保持极度专注，依靠意志力苦撑是容易产生负收益的，保持更长时间的专注才是成功的关键因素。 ","date":"2022-01-28","objectID":"/posts/daily_life/reading/1.html/:2:2","tags":["note","心理学","日常生活"],"title":"《认知觉醒》阅读","uri":"/posts/daily_life/reading/1.html/"},{"categories":null,"content":"0 There is 0 begining. ","date":"2021-02-17","objectID":"/posts/0.html/:1:0","tags":null,"title":"0","uri":"/posts/0.html/"},{"categories":["machine learning"],"content":"基于梯度下降法的一元线性回归应用 ","date":"2020-03-06","objectID":"/posts/ai/machine_learning/5.html/:1:0","tags":["ai","machine_learning","note"],"title":"线性回归与非线性回归","uri":"/posts/ai/machine_learning/5.html/"},{"categories":["machine learning"],"content":"回归Regression 一元线性回归 回归分析(regression analysis)用来建立方程模拟两个或者多个变量之间如何关联 被预测的变量叫做：因变量(dependent variable), 输出(output) 被用来进行预测的变量叫做： 自变量(independent variable), 输入(input) 一元线性回归包含一个自变量和一个因变量 以上两个变量的关系用一条直线来模拟 如果包含两个以上的自变量，则称作多元回归分析(multiple regression) ℎ𝜃 𝑥 = 𝜃0 + 𝜃1𝑥 这个方程对应的图像是一条直线，称作回归线。其中，𝜃1为回归线的斜率， 𝜃0为回归线的截距。 线性回归的三种关系: 正相关 负相关 不相关 求解方程系数 ","date":"2020-03-06","objectID":"/posts/ai/machine_learning/5.html/:1:1","tags":["ai","machine_learning","note"],"title":"线性回归与非线性回归","uri":"/posts/ai/machine_learning/5.html/"},{"categories":["machine learning"],"content":"代价函数Cost Function 最小二乘法 真实值y，预测值ℎ𝜃 𝑥 ，则误差平方为(y − ℎ_𝜃(x))^2 找到合适的参数，使得误差平方和： 相关系数 使用相关系数去衡量线性相关性的强弱 决定系数 相关系数𝑅2(coefficient of determination)是用来描述两个变量之间的线性关系的，但决定系数的适用范围更广，可以用于描述非线性或者有两个及两个以上自变量的相关关系。它可以用来评价模型的效果 ","date":"2020-03-06","objectID":"/posts/ai/machine_learning/5.html/:1:2","tags":["ai","machine_learning","note"],"title":"线性回归与非线性回归","uri":"/posts/ai/machine_learning/5.html/"},{"categories":["machine learning"],"content":"梯度下降法Gradient Descent 学习率不能太小，也不能太大，可以多尝试一些值0.1,0.03,0.01,0.003,0.001,0.0003,0.0001…,否则有可能会陷入局部极小值 用梯度下降法来求解线性回归 非凸函数和凸函数 线性回归的代价函数是凸函数 优化过程 代码实现 import numpy as np import matplotlib.pyplot as plt #载入数据 data = np.genfromtxt(\"data.csv\",delimiter=\",\") x_data = data[:,0] y_data = data[:,1] plt.scatter(x_data,y_data) plt.show() #learning rate lr = 0.0001 #截距 b = 0 #斜率 k = 0 #最大迭代次数 epochs = 50 #最小二乘法 def compute_error(b, k, x_data, y_data): totalError = 0 for i in range(0,len(x_data)): #(真实值-预测值)^2 totalError += (y_data[i]-(k * x_data[i] + b)) ** 2 return totalError / float(len(x_data)) / 2 def gradient_descent_runner(x_data, y_data, b, k, lr, epochs): #计算总数据量 m = float(len(x_data)) for i in range (epochs): b_grad = 0 k_grad = 0 for j in range (0, len(x_data)): b_grad += - (1/m) * (y_data[j] - ((k * x_data[j]) + b)) k_grad += - (1/m) * x_data[j] * (y_data[j] - ((k * x_data[j]) + b)) b = b - (lr * b_grad) k = k - (lr * k_grad) #每迭代五次,输出图像 if i % 5 == 0: print(\"epochs :\",i) plt.plot(x_data, y_data, 'b.') plt.plot(x_data, k*x_data + b, 'r') plt.show() return b, k print(\"Starting b = {0}, k = {1}, error = {2}\".format(b, k, compute_error(b, k, x_data, y_data))) print(\"Running ...\") b, k = gradient_descent_runner(x_data, y_data, b, k, lr, epochs) print(\"After {0}iterations b = {1}, k = {2}, error = {3}\".format(epochs, b, k, compute_error(b, k, x_data, y_data))) # 画图 plt.plot(x_data, y_data, 'b.') plt.plot(x_data, k*x_data + b, 'r') plt.show() 运行结果 使用sklearn实现-一元线性回归 from sklearn. linear_model import LinearRegression import numpy as np import matplotlib.pyplot as plt #载入数据 data = np.genfromtxt(\"data.csv\",delimiter=\",\") x_data = data[:,0] y_data = data[:,1] plt.scatter(x_data,y_data) plt.show() print(x_data.shape) #加上纬度 x_data = data[:,0,np.newaxis] y_data = data[:,1,np.newaxis] print(x_data.shape) print(y_data.shape) #创建并拟合模型 model = LinearRegression() model.fit(x_data, y_data) #画图 plt.plot(x_data, y_data, 'b.') plt.plot(x_data, model.predict(x_data), 'r') plt.show() 运行结果 ","date":"2020-03-06","objectID":"/posts/ai/machine_learning/5.html/:1:3","tags":["ai","machine_learning","note"],"title":"线性回归与非线性回归","uri":"/posts/ai/machine_learning/5.html/"},{"categories":["machine learning"],"content":"基于梯度下降法的多元线性回归应用 ","date":"2020-03-06","objectID":"/posts/ai/machine_learning/5.html/:2:0","tags":["ai","machine_learning","note"],"title":"线性回归与非线性回归","uri":"/posts/ai/machine_learning/5.html/"},{"categories":["machine learning"],"content":"多元线性回归 单特征 多特征 ","date":"2020-03-06","objectID":"/posts/ai/machine_learning/5.html/:2:1","tags":["ai","machine_learning","note"],"title":"线性回归与非线性回归","uri":"/posts/ai/machine_learning/5.html/"},{"categories":["machine learning"],"content":"多元线性回归模型 当Y值的影响因素不是唯一时，采用多元线性回归 ","date":"2020-03-06","objectID":"/posts/ai/machine_learning/5.html/:2:2","tags":["ai","machine_learning","note"],"title":"线性回归与非线性回归","uri":"/posts/ai/machine_learning/5.html/"},{"categories":["machine learning"],"content":"梯度下降法-多元线性回归 代码实现 import numpy as np from numpy import genfromtxt import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D data = genfromtxt(r\"Delivery.csv\",delimiter=',') print(data) ''' [[100. 4. 9.3] [ 50. 3. 4.8] [100. 4. 8.9] [100. 2. 6.5] [ 50. 2. 4.2] [ 80. 2. 6.2] [ 75. 3. 7.4] [ 65. 4. 6. ] [ 90. 3. 7.6] [ 90. 2. 6.1]] ''' #切分数据 x_data = data[:,:-1] y_data = data[:,-1] print(x_data) print(y_data) ''' [[100. 4.] [ 50. 3.] [100. 4.] [100. 2.] [ 50. 2.] [ 80. 2.] [ 75. 3.] [ 65. 4.] [ 90. 3.] [ 90. 2.]] [9.3 4.8 8.9 6.5 4.2 6.2 7.4 6. 7.6 6.1] ''' #学习率 lr = 0.0001 #参数 theta0 = 0 theta1 = 0 theta2 = 0 #最大迭代次数 epochs = 1000 #最小二乘法 def compute_error(theta0, theta1, theta2, x_data, y_data): totalError = 0 for i in range(0, len(x_data)): totalError += (y_data[i] - (theta1 * x_data[i,0] + theta2 * x_data[i,1] +theta0)) ** 2 return totalError / float(len(x_data)) def gradient_descent_runner(x_data, y_data, theta0, theta1, theta2, lr, epochs): #计算总数据量 m = float(len(x_data)) for i in range(epochs): theta0_grad = 0 theta1_grad = 0 theta2_grad = 0 #计算梯度的总和再求平均 for j in range(0, len(x_data)): theta0_grad += -(1/m) * (y_data[j] - (theta1 * x_data[j,0] + theta2 * x_data[j,1] + theta0)) theta1_grad += -(1/m) * x_data[j,0] * (y_data[j] - (theta1 * x_data[j,0] + theta2 * x_data[j,1] + theta0)) theta2_grad += -(1/m) * x_data[j,1] * (y_data[j] - (theta1 * x_data[j,0] + theta2 * x_data[j,1] + theta0)) #更新 theta0 = theta0 - (lr * theta0_grad) theta1 = theta1 - (lr * theta1_grad) theta2 = theta2 - (lr * theta2_grad) return theta0, theta1, theta2 print(\"Starting theta0 = {0}, theta1 = {1}, theta2 = {2}, error = {3}\". format(theta0, theta1, theta2, compute_error(theta0, theta1, theta2, x_data, y_data))) print(\"Running....\") theta0, theta1, theta2 = gradient_descent_runner(x_data, y_data, theta0, theta1, theta2, lr, epochs) print(\"After {0}iterations theta0 = {1}, theta1 = {2}, theta2 = {3}, error = {4}\". format(epochs, theta0, theta1, theta2, compute_error(theta0, theta1, theta2, x_data, y_data))) ''' Starting theta0 = 0, theta1 = 0, theta2 = 0, error = 47.279999999999994 Running.... After 1000 iterations theta0 = 0.006971416196678632, theta1 = 0.08021042690771771, theta2 = 0.07611036240566814, error = 0.7731271432218118 ''' ax = plt.figure().add_subplot(111, projection = '3d') ax.scatter(x_data[:,0], x_data[:,1], y_data, c = 'r', marker = 'o', s = 100) x0 = x_data[:,0] x1 = x_data[:,1] #生成网格矩阵 x0, x1 = np.meshgrid(x0, x1) z = theta0 + x0 * theta1 + x1 * theta2 #画3D图 ax.plot_surface(x0, x1, z) #设置坐标轴 ax.set_xlabel(\"Miles\") ax.set_ylabel('Num of Deleveries') ax.set_zlabel('Time') #显示 plt.show() 运行结果 基于sklearn实现多元线性回归 from sklearn import linear_model import numpy as np from numpy import genfromtxt import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D data = genfromtxt(r\"Delivery.csv\",delimiter=',') print(data) #切分数据 x_data = data[:,:-1] y_data = data[:,-1] #创建模型 model = linear_model.LinearRegression() model.fit(x_data,y_data) #系数 print(\"coefficients:\",model.coef_) #截距 print(\"intercept:\",model.intercept_) #测试 x_test = [[102,4]] predict = model.predict(x_test) print(\"predict:\",predict) ax = plt.figure().add_subplot(111, projection = '3d') ax.scatter(x_data[:,0], x_data[:,1], y_data, c = 'r', marker = 'o', s = 100) x0 = x_data[:,0] x1 = x_data[:,1] #生成网格矩阵 x0, x1 = np.meshgrid(x0, x1) z = model.intercept_ + x0 * model.coef_[0] + x1 * model.coef_[1] #画3D图 ax.plot_surface(x0, x1, z) #设置坐标轴 ax.set_xlabel(\"Miles\") ax.set_ylabel('Num of Deleveries') ax.set_zlabel('Time') #显示 plt.show() 运行结果与上面类似,就不展示出来了 ","date":"2020-03-06","objectID":"/posts/ai/machine_learning/5.html/:2:3","tags":["ai","machine_learning","note"],"title":"线性回归与非线性回归","uri":"/posts/ai/machine_learning/5.html/"},{"categories":["machine learning"],"content":"多项式回归及应用 ","date":"2020-03-06","objectID":"/posts/ai/machine_learning/5.html/:3:0","tags":["ai","machine_learning","note"],"title":"线性回归与非线性回归","uri":"/posts/ai/machine_learning/5.html/"},{"categories":["machine learning"],"content":"代码实现 import numpy as np import matplotlib.pyplot as plt from sklearn.preprocessing import PolynomialFeatures from sklearn.lineaBr_model import LinearRegression #载入数据 data = np.genfromtxt(\"job.csv\",delimiter=\",\") x_data = data[1:,1] y_data = data[1:,2] plt.scatter(x_data, y_data) plt.show() x_data = x_data[:,np.newaxis] y_data = y_data[:,np.newaxis] #创建并拟合模型 #这里是多元线性回归 model = LinearRegression() model.fit(x_data, y_data) #画图 plt.plot(x_data, y_data, 'b.') plt.plot(x_data, model.predict(x_data), 'r') plt.show() 可以看出使用多元线性回归,效果并不是太好,所以我们换成多项式回归来对数据进行特征处理一下 #定义多项式回归,degree(偏置值)的值可以调节多项式的特征 poly_reg = PolynomialFeatures(degree = 5) #特征处理 x_poly = poly_reg.fit_transform(x_data) print(x_poly) #定义回归模型 lin_reg = LinearRegression() #训练模型 lin_reg.fit(x_poly, y_data) #画图 plt.plot(x_data, y_data, 'b.') plt.plot(x_data, lin_reg.predict(poly_reg.fit_transform(x_data)), c='r') plt.title(\"Truth of Bluff (Polynomial Regression)\") plt.xlabel(\"Position level\") plt.ylabel(\"Salary\") plt.show() 这里看到通过进行多项式特征处理后,拟合结果就比较好 ","date":"2020-03-06","objectID":"/posts/ai/machine_learning/5.html/:3:1","tags":["ai","machine_learning","note"],"title":"线性回归与非线性回归","uri":"/posts/ai/machine_learning/5.html/"},{"categories":["machine learning"],"content":"标准方程法(Normal Equation) 分子布局(Numerator-layout): 分子为列向量或者分母为行向量 分母布局(Denominator-layout):分子为行向量或者分母为列向量 求导公式:https://en.wikipedia.org/wiki/Matrix_calculus#Scalar-by-vector_identiti ","date":"2020-03-06","objectID":"/posts/ai/machine_learning/5.html/:4:0","tags":["ai","machine_learning","note"],"title":"线性回归与非线性回归","uri":"/posts/ai/machine_learning/5.html/"},{"categories":["machine learning"],"content":"矩阵不可逆的情况 线性相关的特征(多重共线性) 特征数据太多(样本数m\u003c=特征数量n) ","date":"2020-03-06","objectID":"/posts/ai/machine_learning/5.html/:4:1","tags":["ai","machine_learning","note"],"title":"线性回归与非线性回归","uri":"/posts/ai/machine_learning/5.html/"},{"categories":["machine learning"],"content":"梯度下降法vs标准方程法 ","date":"2020-03-06","objectID":"/posts/ai/machine_learning/5.html/:4:2","tags":["ai","machine_learning","note"],"title":"线性回归与非线性回归","uri":"/posts/ai/machine_learning/5.html/"},{"categories":["machine learning"],"content":"代码实现 import numpy as np from numpy import genfromtxt import matplotlib.pyplot as plt #标准方程法求解回归参数 def weights(xArr, yArr): xMat = np.mat(xArr) yMat = np.mat(yArr) xTx = xMat.T * xMat #矩阵乘法 #计算矩阵的值,如果值为0,说明该矩阵没有逆矩阵 if np.linalg.det(xTx) == 0.0 : print(\"This matrix cannot do inverse\") return #xTx.T 为 xTx 的逆矩阵 ws = xTx.I * xMat.T * yMat return ws #载入数据 data = np.genfromtxt(\"data.csv\",delimiter=\",\") x_data = data[:,0, np.newaxis] y_data = data[:,1, np.newaxis] plt.scatter(x_data, y_data) plt.show() #给样本添加偏置项 X_data = np.concatenate((np.ones((100,1)), x_data), axis=1) print(X_data.shape) ​``` (100, 2) ​``` ws = weights(X_data, y_data) print(ws) ​``` [[7.99102098] [1.32243102]] ​``` #画图 x_test = np.array(([20],[80])) y_test = ws[0] + x_test*ws[1] y_data_predict = ws[0] + x_data*ws[1] plt.plot(x_data, y_data, 'b.') plt.plot(x_data, y_data_predict, 'r') plt.show() 运行结果 ","date":"2020-03-06","objectID":"/posts/ai/machine_learning/5.html/:4:3","tags":["ai","machine_learning","note"],"title":"线性回归与非线性回归","uri":"/posts/ai/machine_learning/5.html/"},{"categories":["machine learning"],"content":"特征缩放交叉验证法 ","date":"2020-03-06","objectID":"/posts/ai/machine_learning/5.html/:5:0","tags":["ai","machine_learning","note"],"title":"线性回归与非线性回归","uri":"/posts/ai/machine_learning/5.html/"},{"categories":["machine learning"],"content":"特征缩放 数据归一化: 数据归一化就是把数据的取值范围处理为0-1或者-1-1之间。 任意数据转化为0-1之间：newValue = (oldValue-min)/(max-min) 任意数据转化为-1-1之间：newValue = ((oldValue-min)/(max-min)-0.5)*2 均值标准化: x为特征数据，u为数据的平均值，s为数据的方差 newValue = (oldValue-u)/s ","date":"2020-03-06","objectID":"/posts/ai/machine_learning/5.html/:5:1","tags":["ai","machine_learning","note"],"title":"线性回归与非线性回归","uri":"/posts/ai/machine_learning/5.html/"},{"categories":["machine learning"],"content":"交叉验证法 通过切分数据,每个部分分成训练集和测试集,且将每个部分的误差加起来计算平均值即为最终误差 ","date":"2020-03-06","objectID":"/posts/ai/machine_learning/5.html/:5:2","tags":["ai","machine_learning","note"],"title":"线性回归与非线性回归","uri":"/posts/ai/machine_learning/5.html/"},{"categories":["machine learning"],"content":"过拟合(Overfitting)\u0026正则化(Regularized) ","date":"2020-03-06","objectID":"/posts/ai/machine_learning/5.html/:6:0","tags":["ai","machine_learning","note"],"title":"线性回归与非线性回归","uri":"/posts/ai/machine_learning/5.html/"},{"categories":["machine learning"],"content":"过拟合 拟合情况: 防止过拟合: 减少特征 增加数据量 正则化 ","date":"2020-03-06","objectID":"/posts/ai/machine_learning/5.html/:6:1","tags":["ai","machine_learning","note"],"title":"线性回归与非线性回归","uri":"/posts/ai/machine_learning/5.html/"},{"categories":["machine learning"],"content":"正则化 ","date":"2020-03-06","objectID":"/posts/ai/machine_learning/5.html/:6:2","tags":["ai","machine_learning","note"],"title":"线性回归与非线性回归","uri":"/posts/ai/machine_learning/5.html/"},{"categories":["machine learning"],"content":"岭回归/LASSO回归/弹性网的应用 ","date":"2020-03-06","objectID":"/posts/ai/machine_learning/5.html/:7:0","tags":["ai","machine_learning","note"],"title":"线性回归与非线性回归","uri":"/posts/ai/machine_learning/5.html/"},{"categories":["machine learning"],"content":"岭回归(Ridge Regression) w = (𝑋^𝑇𝑋)^(−1)𝑋^𝑇y 如果数据的特征比样本点还多，数据特征n，样本个数m，如果n\u003em，则计算时会出错。因为(𝑋^𝑇𝑋)不是满秩矩阵，所以不可逆。 为了解决这个问题，统计学家引入了岭回归的概念。 岭回归最早是用来处理特征数多于样本的情况，现在也用于在估计中加入偏差，从而得到更好的估计。同时也可以解决多重共线性的问题。岭回归是一种有偏估计。 选择𝜆值，使到： 各回归系数的岭估计基本稳定。 残差平方和增大不太多 𝜆值为横坐标,所求参数为纵坐标 sklearn-代码实现 Longley数据集 Longley数据集来自J．W．Longley（1967）发表在JASA上的一篇论文，是强共线性的宏观经济数据,包含GNP deflator(GNP平减指数)、GNP(国民生产总值)、Unemployed(失业率)、ArmedForces(武装力量)、Population(人口)、year(年份)，Emlpoyed(就业率)。 LongLey数据集因存在严重的多重共线性问题，在早期经常用来检验各种算法或计算机的计算精度 import numpy as np from numpy import genfromtxt from sklearn import linear_model import matplotlib.pyplot as plt #载入数据 data = genfromtxt(r\"longley.csv\", delimiter=',') print(data) ​``` [[ nan nan nan nan nan nan nan nan] [ nan 83. 234.289 235.6 159. 107.608 1947. 60.323] [ nan 88.5 259.426 232.5 145.6 108.632 1948. 61.122] [ nan 88.2 258.054 368.2 161.6 109.773 1949. 60.171] [ nan 89.5 284.599 335.1 165. 110.929 1950. 61.187] [ nan 96.2 328.975 209.9 309.9 112.075 1951. 63.221] [ nan 98.1 346.999 193.2 359.4 113.27 1952. 63.639] [ nan 99. 365.385 187. 354.7 115.094 1953. 64.989] [ nan 100. 363.112 357.8 335. 116.219 1954. 63.761] [ nan 101.2 397.469 290.4 304.8 117.388 1955. 66.019] [ nan 104.6 419.18 282.2 285.7 118.734 1956. 67.857] [ nan 108.4 442.769 293.6 279.8 120.445 1957. 68.169] [ nan 110.8 444.546 468.1 263.7 121.95 1958. 66.513] [ nan 112.6 482.704 381.3 255.2 123.366 1959. 68.655] [ nan 114.2 502.601 393.1 251.4 125.368 1960. 69.564] [ nan 115.7 518.173 480.6 257.2 127.852 1961. 69.331] [ nan 116.9 554.894 400.7 282.7 130.081 1962. 70.551]] ​``` #切分数据 x_data = data[1:,2:] y_data = data[1:,1] #创建模型 #生成50个值 alphas_to_test = np.linspace(0.001, 1) #创建模型,保存误差值,CV:交叉验证 model = linear_model.RidgeCV(alphas = alphas_to_test, store_cv_values= True) model.fit(x_data, y_data) #岭系数 print(model.alpha_) #loss值 print(model.cv_values_.shape) ​``` 0.40875510204081633 (16, 50) ​``` #画图 #岭系数跟Loss值的关系 plt.plot(alphas_to_test, model.cv_values_.mean(axis=0)) #选取的岭系数的位置 plt.plot(model.alpha_, min(model.cv_values_.mean(axis=0)), 'ro') plt.show() #简单测试 model.predict(x_data[2,np.newaxis]) y_data[2] ​``` out:array([88.11216213]) out:88.2 ​``` 运行结果 标准方程法-代码实现 import numpy as np from numpy import genfromtxt from sklearn import linear_model import matplotlib.pyplot as plt #载入数据 data = genfromtxt(r\"longley.csv\", delimiter=',') print(data) #切分数据 x_data = data[1:,2:] y_data = data[1:,1,np.newaxis] print(x_data.shape) print(y_data.shape) ​``` [[ nan nan nan nan nan nan nan nan] [ nan 83. 234.289 235.6 159. 107.608 1947. 60.323] [ nan 88.5 259.426 232.5 145.6 108.632 1948. 61.122] [ nan 88.2 258.054 368.2 161.6 109.773 1949. 60.171] [ nan 89.5 284.599 335.1 165. 110.929 1950. 61.187] [ nan 96.2 328.975 209.9 309.9 112.075 1951. 63.221] [ nan 98.1 346.999 193.2 359.4 113.27 1952. 63.639] [ nan 99. 365.385 187. 354.7 115.094 1953. 64.989] [ nan 100. 363.112 357.8 335. 116.219 1954. 63.761] [ nan 101.2 397.469 290.4 304.8 117.388 1955. 66.019] [ nan 104.6 419.18 282.2 285.7 118.734 1956. 67.857] [ nan 108.4 442.769 293.6 279.8 120.445 1957. 68.169] [ nan 110.8 444.546 468.1 263.7 121.95 1958. 66.513] [ nan 112.6 482.704 381.3 255.2 123.366 1959. 68.655] [ nan 114.2 502.601 393.1 251.4 125.368 1960. 69.564] [ nan 115.7 518.173 480.6 257.2 127.852 1961. 69.331] [ nan 116.9 554.894 400.7 282.7 130.081 1962. 70.551]] (16, 6) (16, 1) ​``` print(np.mat(x_data).shape) print(np.mat(y_data).shape) #给样本项加偏置值 X_data = np.concatenate((np.ones((16,1)), x_data),axis=1) print(X_data.shape) ​``` (16, 6) (16, 1) (16, 7) ​``` #岭回归标准方程法求解回归参数 def weights(xArr, yArr, lam=0.2): xMat = np.mat(xArr) yMat = np.mat(yArr) xTx = xMat.T * xMat #矩阵乘法 rxTx = xTx + np.eye(xMat.shape[1]) * lam if np.linalg.det(rxTx) == 0.0: print(\"This mattrix cannot do inverse\") return ws = rxTx.I * xMat.T * yMat return ws ws = weights(X_data, y_data) print(ws) ​``` [[ 7.38107882e-04] [ 2.07703836e-01] [ 2.10076376e-02] [ 5.05385441e-03] [-1.59173066e+00] [ 1.10442920e-01] [-2.42280461e-01]] ​``` #计算预测值 print(np.mat(X_dat","date":"2020-03-06","objectID":"/posts/ai/machine_learning/5.html/:7:1","tags":["ai","machine_learning","note"],"title":"线性回归与非线性回归","uri":"/posts/ai/machine_learning/5.html/"},{"categories":["machine learning"],"content":"LASSO Tibshirani(1996)提出了Lasso(The Least Absolute Shrinkage and Selectionator operator)算法 通过构造一个一阶惩罚函数获得一个精炼的模型；通过最终确定一些 指标（变量）的系数为零（岭回归估计系数等于0的机会微乎其微， 造成筛选变量困难），解释力很强。 擅长处理具有多重共线性的数据，与岭回归一样是有偏估计 。 LASSO与岭回归 图像比较 sklearn-代码实现Lasso import numpy as np from numpy import genfromtxt from sklearn import linear_model #载入数据 data = genfromtxt(r\"longley.csv\",delimiter = ',') print(data.shape) #切分数据 x_data = data[1:,2:] y_data = data[1:,1] print(x_data.shape) print(y_data.shape) ​``` (17, 8) (16, 6) (16,) ​``` #创建模型,得到一个合适的误差值,由于交叉验证其函数内部自动生成随机数进行测试 model = linear_model.LassoCV() model.fit(x_data, y_data) #lasso系数 print(model.alpha_) #相关系数 print(model.coef_) #其中三个特征有共线性即为0 ​``` 20.03464209711722 [0.10206856 0.00409161 0.00354815 0. 0. 0. ] ​``` #预测值 print(model.predict(x_data[-2,np.newaxis])) #真实值 print(y_data[-2]) ​``` [115.6461414] 115.7 ​``` ","date":"2020-03-06","objectID":"/posts/ai/machine_learning/5.html/:7:2","tags":["ai","machine_learning","note"],"title":"线性回归与非线性回归","uri":"/posts/ai/machine_learning/5.html/"},{"categories":["machine learning"],"content":"弹性网(Elastic Net) 很容易看出,q=2时就是为岭回归,q=1为Lasso回归 sklearn-代码实现ElasticNet 由于部分结果与上面类似,这里就不贴出来了 import numpy as np from numpy import genfromtxt from sklearn import linear_model #载入数据 data = genfromtxt(r\"longley.csv\", delimiter=',') print(data) print(data.shape) #切分数据 x_data = data[1:,2:] y_data = data[1:,1] #创建模型 model = linear_model.ElasticNetCV() model.fit(x_data, y_data) #l弹性网系数 print(model.alpha_) #相关系数 print(model.coef_) ​``` 42.96498005089394 [0.1016487 0.00416716 0.00349843 0. 0. 0. ] ​``` #预测值 print(model.predict(x_data[-2,np.newaxis])) #真实值 print(y_data[-2]) ​``` [115.6037171] 115.7 ​``` ","date":"2020-03-06","objectID":"/posts/ai/machine_learning/5.html/:7:3","tags":["ai","machine_learning","note"],"title":"线性回归与非线性回归","uri":"/posts/ai/machine_learning/5.html/"},{"categories":["machine learning"],"content":"Week 9 ","date":"2019-07-27","objectID":"/posts/ai/machine_learning/4.html/:0:0","tags":["ai","machine_learning","note"],"title":"MachineLearning(AndrewNg)Notes-Week9","uri":"/posts/ai/machine_learning/4.html/"},{"categories":["machine learning"],"content":"Density Estimation(异常检测) ","date":"2019-07-27","objectID":"/posts/ai/machine_learning/4.html/:1:0","tags":["ai","machine_learning","note"],"title":"MachineLearning(AndrewNg)Notes-Week9","uri":"/posts/ai/machine_learning/4.html/"},{"categories":["machine learning"],"content":"Problem Motivation Density Estimation Algotithm Anomaly detection example ","date":"2019-07-27","objectID":"/posts/ai/machine_learning/4.html/:1:1","tags":["ai","machine_learning","note"],"title":"MachineLearning(AndrewNg)Notes-Week9","uri":"/posts/ai/machine_learning/4.html/"},{"categories":["machine learning"],"content":"Gaussian Distribution(高斯分布或正态分布) The formula for the Gaussian density is: $$ p(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right) $$ Gaussian distribution example Parameter estimation ","date":"2019-07-27","objectID":"/posts/ai/machine_learning/4.html/:1:2","tags":["ai","machine_learning","note"],"title":"MachineLearning(AndrewNg)Notes-Week9","uri":"/posts/ai/machine_learning/4.html/"},{"categories":["machine learning"],"content":"Algorithm Anomaly detection example ","date":"2019-07-27","objectID":"/posts/ai/machine_learning/4.html/:1:3","tags":["ai","machine_learning","note"],"title":"MachineLearning(AndrewNg)Notes-Week9","uri":"/posts/ai/machine_learning/4.html/"},{"categories":["machine learning"],"content":"Building an Anomaly Detection System ","date":"2019-07-27","objectID":"/posts/ai/machine_learning/4.html/:2:0","tags":["ai","machine_learning","note"],"title":"MachineLearning(AndrewNg)Notes-Week9","uri":"/posts/ai/machine_learning/4.html/"},{"categories":["machine learning"],"content":"Developing and Evaluating an Anomaly Detection System the important of real-number evaluation Aircraft engines motivating example Alogorithm evaluation ","date":"2019-07-27","objectID":"/posts/ai/machine_learning/4.html/:2:1","tags":["ai","machine_learning","note"],"title":"MachineLearning(AndrewNg)Notes-Week9","uri":"/posts/ai/machine_learning/4.html/"},{"categories":["machine learning"],"content":"Anomaly Detection vs. Supervised Learning ","date":"2019-07-27","objectID":"/posts/ai/machine_learning/4.html/:2:2","tags":["ai","machine_learning","note"],"title":"MachineLearning(AndrewNg)Notes-Week9","uri":"/posts/ai/machine_learning/4.html/"},{"categories":["machine learning"],"content":"Choosing What Features to Use Non-gaussian features Error analysis for anomaly detection Monitoring computers in a data center ","date":"2019-07-27","objectID":"/posts/ai/machine_learning/4.html/:2:3","tags":["ai","machine_learning","note"],"title":"MachineLearning(AndrewNg)Notes-Week9","uri":"/posts/ai/machine_learning/4.html/"},{"categories":["machine learning"],"content":"Multivariate Gaussian Distribution(Optional) ","date":"2019-07-27","objectID":"/posts/ai/machine_learning/4.html/:3:0","tags":["ai","machine_learning","note"],"title":"MachineLearning(AndrewNg)Notes-Week9","uri":"/posts/ai/machine_learning/4.html/"},{"categories":["machine learning"],"content":"Multivariate Gaussian Distribution Motivating example:Monitoring machines in a data center Multivariate Gaussian(Normal)distribution Multivariate Gaussian(Normal) examples ","date":"2019-07-27","objectID":"/posts/ai/machine_learning/4.html/:3:1","tags":["ai","machine_learning","note"],"title":"MachineLearning(AndrewNg)Notes-Week9","uri":"/posts/ai/machine_learning/4.html/"},{"categories":["machine learning"],"content":"Anomaly Detection using the Multivariate Gaussian Distribution Multivariate Gaussian (Normal) distribution Anomaly detection with the multivariate Gaussian Relationship to original model Original model vs. Multivariate Gaussian ","date":"2019-07-27","objectID":"/posts/ai/machine_learning/4.html/:3:2","tags":["ai","machine_learning","note"],"title":"MachineLearning(AndrewNg)Notes-Week9","uri":"/posts/ai/machine_learning/4.html/"},{"categories":["machine learning"],"content":"Predicting Movie Ratings ","date":"2019-07-27","objectID":"/posts/ai/machine_learning/4.html/:4:0","tags":["ai","machine_learning","note"],"title":"MachineLearning(AndrewNg)Notes-Week9","uri":"/posts/ai/machine_learning/4.html/"},{"categories":["machine learning"],"content":"Problem Formulation Example : Predicting movie ratings ","date":"2019-07-27","objectID":"/posts/ai/machine_learning/4.html/:4:1","tags":["ai","machine_learning","note"],"title":"MachineLearning(AndrewNg)Notes-Week9","uri":"/posts/ai/machine_learning/4.html/"},{"categories":["machine learning"],"content":"Content Based Recommendations Content-based recommender systems Problem formulation Optimization objective Optimization algorithm ","date":"2019-07-27","objectID":"/posts/ai/machine_learning/4.html/:4:2","tags":["ai","machine_learning","note"],"title":"MachineLearning(AndrewNg)Notes-Week9","uri":"/posts/ai/machine_learning/4.html/"},{"categories":["machine learning"],"content":"Collaborative Filtering ","date":"2019-07-27","objectID":"/posts/ai/machine_learning/4.html/:5:0","tags":["ai","machine_learning","note"],"title":"MachineLearning(AndrewNg)Notes-Week9","uri":"/posts/ai/machine_learning/4.html/"},{"categories":["machine learning"],"content":"Collaborative Filtering Problem motivation Optimization algorithm Collaborative filtering ","date":"2019-07-27","objectID":"/posts/ai/machine_learning/4.html/:5:1","tags":["ai","machine_learning","note"],"title":"MachineLearning(AndrewNg)Notes-Week9","uri":"/posts/ai/machine_learning/4.html/"},{"categories":["machine learning"],"content":"Collaborative Filtering Algorithm Collaborative filtering optimization objective Collaborative filtering algorithm ","date":"2019-07-27","objectID":"/posts/ai/machine_learning/4.html/:5:2","tags":["ai","machine_learning","note"],"title":"MachineLearning(AndrewNg)Notes-Week9","uri":"/posts/ai/machine_learning/4.html/"},{"categories":["machine learning"],"content":"Low Rank Matrix Factorization(低秩矩阵分解) ","date":"2019-07-27","objectID":"/posts/ai/machine_learning/4.html/:6:0","tags":["ai","machine_learning","note"],"title":"MachineLearning(AndrewNg)Notes-Week9","uri":"/posts/ai/machine_learning/4.html/"},{"categories":["machine learning"],"content":"Vectorization: Low Rank Matrix Factorization Collaborative filtering Finding related movies ","date":"2019-07-27","objectID":"/posts/ai/machine_learning/4.html/:6:1","tags":["ai","machine_learning","note"],"title":"MachineLearning(AndrewNg)Notes-Week9","uri":"/posts/ai/machine_learning/4.html/"},{"categories":["machine learning"],"content":"Implementational Detail: Mean Normalization Users who have not reated any movies Mean Normalizatio ","date":"2019-07-27","objectID":"/posts/ai/machine_learning/4.html/:6:2","tags":["ai","machine_learning","note"],"title":"MachineLearning(AndrewNg)Notes-Week9","uri":"/posts/ai/machine_learning/4.html/"},{"categories":["machine learning"],"content":"Clustering ","date":"2019-07-05","objectID":"/posts/ai/machine_learning/3.html/:1:0","tags":["ai","machine_learning","note"],"title":"MachineLearning(AndrewNg)Notes-Week8","uri":"/posts/ai/machine_learning/3.html/"},{"categories":["machine learning"],"content":"K-Means Algorithm(K均值 (K-means) 算法) ","date":"2019-07-05","objectID":"/posts/ai/machine_learning/3.html/:1:1","tags":["ai","machine_learning","note"],"title":"MachineLearning(AndrewNg)Notes-Week8","uri":"/posts/ai/machine_learning/3.html/"},{"categories":["machine learning"],"content":"Optimization Objective ","date":"2019-07-05","objectID":"/posts/ai/machine_learning/3.html/:1:2","tags":["ai","machine_learning","note"],"title":"MachineLearning(AndrewNg)Notes-Week8","uri":"/posts/ai/machine_learning/3.html/"},{"categories":["machine learning"],"content":"Random Initialization ","date":"2019-07-05","objectID":"/posts/ai/machine_learning/3.html/:1:3","tags":["ai","machine_learning","note"],"title":"MachineLearning(AndrewNg)Notes-Week8","uri":"/posts/ai/machine_learning/3.html/"},{"categories":["machine learning"],"content":"Choosing the Number of Clusters ","date":"2019-07-05","objectID":"/posts/ai/machine_learning/3.html/:1:4","tags":["ai","machine_learning","note"],"title":"MachineLearning(AndrewNg)Notes-Week8","uri":"/posts/ai/machine_learning/3.html/"},{"categories":["machine learning"],"content":"Motivation ","date":"2019-07-05","objectID":"/posts/ai/machine_learning/3.html/:2:0","tags":["ai","machine_learning","note"],"title":"MachineLearning(AndrewNg)Notes-Week8","uri":"/posts/ai/machine_learning/3.html/"},{"categories":["machine learning"],"content":"Motivation I : Data Compression ","date":"2019-07-05","objectID":"/posts/ai/machine_learning/3.html/:2:1","tags":["ai","machine_learning","note"],"title":"MachineLearning(AndrewNg)Notes-Week8","uri":"/posts/ai/machine_learning/3.html/"},{"categories":["machine learning"],"content":"Motivation II:Visualization ","date":"2019-07-05","objectID":"/posts/ai/machine_learning/3.html/:2:2","tags":["ai","machine_learning","note"],"title":"MachineLearning(AndrewNg)Notes-Week8","uri":"/posts/ai/machine_learning/3.html/"},{"categories":["machine learning"],"content":"Principal Component Analysis(PCA) ","date":"2019-07-05","objectID":"/posts/ai/machine_learning/3.html/:3:0","tags":["ai","machine_learning","note"],"title":"MachineLearning(AndrewNg)Notes-Week8","uri":"/posts/ai/machine_learning/3.html/"},{"categories":["machine learning"],"content":"Principal Component Analysis(PCA)Problem Formulation ","date":"2019-07-05","objectID":"/posts/ai/machine_learning/3.html/:3:1","tags":["ai","machine_learning","note"],"title":"MachineLearning(AndrewNg)Notes-Week8","uri":"/posts/ai/machine_learning/3.html/"},{"categories":["machine learning"],"content":"Principal Component Analysisi Algorithm ","date":"2019-07-05","objectID":"/posts/ai/machine_learning/3.html/:3:2","tags":["ai","machine_learning","note"],"title":"MachineLearning(AndrewNg)Notes-Week8","uri":"/posts/ai/machine_learning/3.html/"},{"categories":["machine learning"],"content":"PCA Applying ","date":"2019-07-05","objectID":"/posts/ai/machine_learning/3.html/:4:0","tags":["ai","machine_learning","note"],"title":"MachineLearning(AndrewNg)Notes-Week8","uri":"/posts/ai/machine_learning/3.html/"},{"categories":["machine learning"],"content":"Reconstruction from Compressed Representation ","date":"2019-07-05","objectID":"/posts/ai/machine_learning/3.html/:4:1","tags":["ai","machine_learning","note"],"title":"MachineLearning(AndrewNg)Notes-Week8","uri":"/posts/ai/machine_learning/3.html/"},{"categories":["machine learning"],"content":"Choosing the Number of Principal Components ","date":"2019-07-05","objectID":"/posts/ai/machine_learning/3.html/:4:2","tags":["ai","machine_learning","note"],"title":"MachineLearning(AndrewNg)Notes-Week8","uri":"/posts/ai/machine_learning/3.html/"},{"categories":["machine learning"],"content":"Advice for Applying PCA ","date":"2019-07-05","objectID":"/posts/ai/machine_learning/3.html/:4:3","tags":["ai","machine_learning","note"],"title":"MachineLearning(AndrewNg)Notes-Week8","uri":"/posts/ai/machine_learning/3.html/"},{"categories":["machine learning"],"content":"Large Margin Classification Optimization Objective Large Margin Intuition Mathematics Behind Large Margin Classification ","date":"2019-06-27","objectID":"/posts/ai/machine_learning/2.html/:0:1","tags":["ai","machine_learning","note"],"title":"MachineLearning(AndrewNg)Notes-Week7","uri":"/posts/ai/machine_learning/2.html/"},{"categories":["machine learning"],"content":"Kernels Kernels I Kernels II ","date":"2019-06-27","objectID":"/posts/ai/machine_learning/2.html/:0:2","tags":["ai","machine_learning","note"],"title":"MachineLearning(AndrewNg)Notes-Week7","uri":"/posts/ai/machine_learning/2.html/"},{"categories":["machine learning"],"content":"Using An SVM ","date":"2019-06-27","objectID":"/posts/ai/machine_learning/2.html/:0:3","tags":["ai","machine_learning","note"],"title":"MachineLearning(AndrewNg)Notes-Week7","uri":"/posts/ai/machine_learning/2.html/"},{"categories":["machine learning"],"content":"Liner Regression Cost Function $h(x)=\\theta_0+\\theta_1x+….$ $h(x)=\\theta^Tx$ Linear Regression $J(\\theta) = \\frac{1}{2m}\\sum_{1}^{m}(h_\\theta(x^i)-y^i)$ $\\frac{\\partial{J(\\theta)}}{\\partial{\\theta_j}}=\\frac{1}{m}\\sum_{1}^{m}(h_\\theta(x^i)-y^i)$ Gradient descent algorithm repeat until convergence{ $\\theta_j := \\theta_j - \\frac{ \\alpha}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)})-y^{(i)}) x^{(i)}$ } Feature scaling and mean normalization $x_i=\\frac{x_i-\\mu_i}{s_i}$ $\\mu_i$: the average of all the values for feature (i) $s_i$ : standard deviation learning rate If α is too small: slow convergence. If α is too large: may not decrease on every iteration and thus may not converge. Polynomial Regression change the behavior or curve of our hypothesis function by making it a quadratic, cubic or square root function (or any other form). Normal Equation $\\theta = (X^TX)^{-1}X^Ty$ ","date":"2019-06-23","objectID":"/posts/ai/machine_learning/1.html/:0:1","tags":["ai","machine_learning","note"],"title":"MachineLearning(AndrewNg)Notes-Week1-Week5总结","uri":"/posts/ai/machine_learning/1.html/"},{"categories":["machine learning"],"content":"Logistic Regression Logistic Function or Sigmoid Function Decision Boundary $\\theta^Tx \\ge 0 \\Rightarrow y=1$ $\\theta^Tx \\le 0 \\Rightarrow y=0$ Cost Function Gradient Descent $h=g(X\\theta)$ $J(\\theta)=\\frac{1}{m}(-y’log(h)-(1-y)‘log(1-h))$ $\\theta:=\\theta-\\frac{\\alpha}{m}X^T(g(\\theta X) -y)$ Advanced Optimization function[jVal, gradient] =costFunction(theta)jVal = [...code to compute J(theta)...]; gradient = [...code to compute derivative of J(theta)...]; end options = optimset('GradObj', 'on', 'MaxIter', 100); initialTheta = zeros(2,1); [optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options); Multiclass Classification: One-vs-all Train a logistic regression classifier $h_\\theta(X)$ for each class to predict the probability that y = i . To make a prediction on a new x, pick the class that maximizes $h_\\theta(X)$ Overfitting Reduce the number of features Regularization Regularized Logistic Regression ","date":"2019-06-23","objectID":"/posts/ai/machine_learning/1.html/:0:2","tags":["ai","machine_learning","note"],"title":"MachineLearning(AndrewNg)Notes-Week1-Week5总结","uri":"/posts/ai/machine_learning/1.html/"},{"categories":["machine learning"],"content":"Neural Networks Model Representation Forward propagation:Vectorized implementation Multiclass Classification one-vs-all Neural Network(Classification) L = total number of layers in the network $s_l$= number of units (not counting bias unit) in layer l K = number of output units/classes Cost Function Backpropagation Algorithm Gradient Checking Random Initialization ","date":"2019-06-23","objectID":"/posts/ai/machine_learning/1.html/:0:3","tags":["ai","machine_learning","note"],"title":"MachineLearning(AndrewNg)Notes-Week1-Week5总结","uri":"/posts/ai/machine_learning/1.html/"}]
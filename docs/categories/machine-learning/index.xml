<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>machine learning - åˆ†ç±» - </title>
        <link>https://zhengyua.cn/categories/machine-learning/</link>
        <description>machine learning - åˆ†ç±» - </description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>yu949812478@gmail.com (ä¸€ç›´æ˜¯é˜µé›¨ğŸŒ¦ï¸)</managingEditor>
            <webMaster>yu949812478@gmail.com (ä¸€ç›´æ˜¯é˜µé›¨ğŸŒ¦ï¸)</webMaster><lastBuildDate>Sat, 26 Feb 2022 18:18:00 &#43;0800</lastBuildDate><atom:link href="https://zhengyua.cn/categories/machine-learning/" rel="self" type="application/rss+xml" /><item>
    <title>ã€Šç™½è¯æœºå™¨å­¦ä¹ çš„æ•°å­¦ã€‹é˜…è¯»ç¬”è®°</title>
    <link>https://zhengyua.cn/posts/ai/machine_learning/0.html/</link>
    <pubDate>Sat, 26 Feb 2022 18:18:00 &#43;0800</pubDate>
    <author>ä½œè€…</author>
    <guid>https://zhengyua.cn/posts/ai/machine_learning/0.html/</guid>
    <description><![CDATA[æ¦‚è¿° é€šè¿‡è¯»å–å¤§é‡çš„æ•°æ®ã€å­¦ä¹ æ•°æ®çš„ç‰¹å¾å¹¶ä»ä¸­æ‰¾å‡ºæ•°æ®çš„æ¨¡å¼ã€‚ è¿™æ ·çš„ä»»åŠ¡ä¹Ÿè¢«ç§°ä¸ºæœºå™¨å­¦ä¹ æˆ–è€…æ¨¡å¼è¯†åˆ«ã€‚ æœºå™¨å­¦ä¹ ä¸­æ¯”è¾ƒæ“…é•¿çš„ä»»åŠ¡ï¼š å›å½’ï¼ˆregr]]></description>
</item><item>
    <title>MachineLearning(AndrewNg)Notes-Week9</title>
    <link>https://zhengyua.cn/posts/ai/machine_learning/4.html/</link>
    <pubDate>Sat, 27 Jul 2019 21:51:34 &#43;0800</pubDate>
    <author>ä½œè€…</author>
    <guid>https://zhengyua.cn/posts/ai/machine_learning/4.html/</guid>
    <description><![CDATA[Week 9 Density Estimation(å¼‚å¸¸æ£€æµ‹) Problem Motivation Density Estimation Algotithm Anomaly detection example Gaussian Distribution(é«˜æ–¯åˆ†å¸ƒæˆ–æ­£æ€åˆ†å¸ƒ) The formula for the Gaussian density is: $$ p(x) = \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right) $$ Gaussian distribution example Parameter estimation Algorithm Anomaly]]></description>
</item><item>
    <title>MachineLearning(AndrewNg)Notes-Week8</title>
    <link>https://zhengyua.cn/posts/ai/machine_learning/3.html/</link>
    <pubDate>Fri, 05 Jul 2019 11:55:09 &#43;0800</pubDate>
    <author>ä½œè€…</author>
    <guid>https://zhengyua.cn/posts/ai/machine_learning/3.html/</guid>
    <description><![CDATA[Clustering K-Means Algorithm(Kå‡å€¼ (K-means) ç®—æ³•) Optimization Objective Random Initialization Choosing the Number of Clusters Motivation Motivation I : Data Compression Motivation II:Visualization Principal Component Analysis(PCA) Principal Component Analysis(PCA)Problem Formulation Principal Component Analysisi Algorithm PCA Applying Reconstruction from Compressed Representation Choosing the Number of Principal Components Advice for Applying PCA]]></description>
</item><item>
    <title>MachineLearning(AndrewNg)Notes-Week7</title>
    <link>https://zhengyua.cn/posts/ai/machine_learning/2.html/</link>
    <pubDate>Thu, 27 Jun 2019 23:29:53 &#43;0800</pubDate>
    <author>ä½œè€…</author>
    <guid>https://zhengyua.cn/posts/ai/machine_learning/2.html/</guid>
    <description><![CDATA[Large Margin Classification Optimization Objective Large Margin Intuition Mathematics Behind Large Margin Classification Kernels Kernels I Kernels II Using An SVM ]]></description>
</item><item>
    <title>MachineLearning(AndrewNg)Notes-Week1-Week5æ€»ç»“</title>
    <link>https://zhengyua.cn/posts/ai/machine_learning/1.html/</link>
    <pubDate>Sun, 23 Jun 2019 11:53:03 &#43;0800</pubDate>
    <author>ä½œè€…</author>
    <guid>https://zhengyua.cn/posts/ai/machine_learning/1.html/</guid>
    <description><![CDATA[Liner Regression Cost Function $h(x)=\theta_0+\theta_1x+&hellip;.$ $h(x)=\theta^Tx$ Linear Regression $J(\theta) = \frac{1}{2m}\sum_{1}^{m}(h_\theta(x^i)-y^i)$ $\frac{\partial{J(\theta)}}{\partial{\theta_j}}=\frac{1}{m}\sum_{1}^{m}(h_\theta(x^i)-y^i)$ Gradient descent algorithm repeat until convergence{ $\theta_j := \theta_j - \frac{ \alpha}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)}) x^{(i)}$ } Feature scaling and mean normalization $x_i=\frac{x_i-\mu_i}{s_i}$ $\mu_i$: the average of all the values for feature (i) $s_i$ : standard deviation learning rate If Î± is too small: slow convergence. If Î± is too large: may not decrease on every iteration and thus may not]]></description>
</item></channel>
</rss>

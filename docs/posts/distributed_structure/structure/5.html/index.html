<!DOCTYPE html>
<html lang="zh-CN">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
        <title>MIT6824LEC3笔记:GFS - </title><meta name="Description" content="一直是阵雨🌦️&#39;s blog"><meta property="og:title" content="MIT6824LEC3笔记:GFS" />
<meta property="og:description" content="GFS 课程地址 Before gfs why is distributed storage hard High performance -&gt; shard data over many servers Many servers -&gt; constant faults Fault tolerance -&gt; replication replication -&gt; potential inconsistencies Better consistency -&gt; low performance it can sum up the all thinking points like: performance fault tolerance replication consistency these points have connection and influence each other. what would we like for consistency? There are two conditions" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://zhengyua.cn/posts/distributed_structure/structure/5.html/" /><meta property="og:image" content="https://zhengyua.cn/logo.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-01-29T17:00:06+08:00" />
<meta property="article:modified_time" content="2022-01-29T17:00:06+08:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://zhengyua.cn/logo.png"/>

<meta name="twitter:title" content="MIT6824LEC3笔记:GFS"/>
<meta name="twitter:description" content="GFS 课程地址 Before gfs why is distributed storage hard High performance -&gt; shard data over many servers Many servers -&gt; constant faults Fault tolerance -&gt; replication replication -&gt; potential inconsistencies Better consistency -&gt; low performance it can sum up the all thinking points like: performance fault tolerance replication consistency these points have connection and influence each other. what would we like for consistency? There are two conditions"/>
<meta name="application-name" content="LoveIt">
<meta name="apple-mobile-web-app-title" content="LoveIt"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="canonical" href="https://zhengyua.cn/posts/distributed_structure/structure/5.html/" /><link rel="prev" href="https://zhengyua.cn/posts/distributed_structure/structure/1.html/" /><link rel="next" href="https://zhengyua.cn/posts/distributed_structure/structure/3.html/" /><link rel="stylesheet" href="/lib/normalize/normalize.min.css"><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"><link rel="stylesheet" href="/lib/animate/animate.min.css"><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "MIT6824LEC3笔记:GFS",
        "inLanguage": "zh-CN",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/zhengyua.cn\/posts\/distributed_structure\/structure\/5.html\/"
        },"genre": "posts","keywords": "分布式架构, MIT6824, note","wordcount":  1838 ,
        "url": "https:\/\/zhengyua.cn\/posts\/distributed_structure\/structure\/5.html\/","datePublished": "2022-01-29T17:00:06+08:00","dateModified": "2022-01-29T17:00:06+08:00","publisher": {
            "@type": "Organization",
            "name": "一直是阵雨🌦️"},"author": {
                "@type": "Person",
                "name": "一直是阵雨🌦️"
            },"description": ""
    }
    </script></head>
    <body header-desktop="fixed" header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper">
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title=""><span id="id-1" class="typeit"></span></a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> 文章 </a><a class="menu-item" href="/tags/"> 标签 </a><a class="menu-item" href="/categories/"> 分类 </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                    <i class="fas fa-adjust fa-fw"></i>
                </a>
            </div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title=""><span id="id-2" class="typeit"></span></a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        取消
                    </a>
                </div><a class="menu-item" href="/posts/" title="">文章</a><a class="menu-item" href="/tags/" title="">标签</a><a class="menu-item" href="/categories/" title="">分类</a><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                <i class="fas fa-adjust fa-fw"></i>
            </a></div>
    </div>
</header>
<div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
</div>
<div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
</div>
<main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">目录</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animated flipInX">MIT6824LEC3笔记:GFS</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="https://github.com/catwithtudou" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw"></i>一直是阵雨🌦️</a></span>&nbsp;<span class="post-category">收录于 <a href="/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84/"><i class="far fa-folder fa-fw"></i>分布式架构</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="2022-01-29">2022-01-29</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;约 1838 字&nbsp;
                <i class="far fa-clock fa-fw"></i>&nbsp;预计阅读 4 分钟&nbsp;</div>
        </div><div class="details toc" id="toc-static"  kept="">
                <div class="details-summary toc-title">
                    <span>目录</span>
                    <span><i class="details-icon fas fa-angle-right"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#gfs">GFS</a>
      <ul>
        <li><a href="#before-gfs">Before gfs</a>
          <ul>
            <li><a href="#why-is-distributed-storage-hard">why is distributed storage hard</a></li>
            <li><a href="#what-would-we-like-for-consistency">what would we like for consistency?</a></li>
          </ul>
        </li>
        <li><a href="#gfs-1">Gfs</a>
          <ul>
            <li><a href="#context">Context</a></li>
            <li><a href="#design">Design</a></li>
            <li><a href="#master-state">Master state</a></li>
            <li><a href="#steps-when-read-a-file">steps when read a file</a></li>
            <li><a href="#steps-when-write-a-file-append">steps when write a file: append</a></li>
            <li><a href="#question">question</a></li>
            <li><a href="#strict-consistency">strict consistency</a></li>
          </ul>
        </li>
        <li><a href="#summary">Summary</a></li>
      </ul>
    </li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><h1 id="gfs">GFS</h1>
<blockquote>
<p><a href="https://pdos.csail.mit.edu/6.824/schedule.html" target="_blank" rel="noopener noreffer">课程地址</a></p>
</blockquote>
<h2 id="before-gfs">Before gfs</h2>
<h3 id="why-is-distributed-storage-hard">why is distributed storage hard</h3>
<p>High performance -&gt; shard data over many servers</p>
<p>Many servers -&gt; constant faults</p>
<p>Fault tolerance -&gt; replication</p>
<p>replication -&gt; potential inconsistencies</p>
<p>Better consistency -&gt; low performance</p>
<p>it can sum up the all thinking points like:</p>
<ul>
<li>performance</li>
<li>fault tolerance</li>
<li>replication</li>
<li>consistency</li>
</ul>
<p>these points have connection and influence each other.</p>
<h3 id="what-would-we-like-for-consistency">what would we like for consistency?</h3>
<p>There are two conditions we can think:</p>
<ul>
<li>single server</li>
</ul>
<p>C1: wx1</p>
<p>C2: wx2</p>
<p>C3: 		 rx?</p>
<p>C4:					rx?</p>
<p>C3 or C4 can see either 1 or 2, but both have to see the same value.</p>
<p>It&rsquo;s a strong consistency model in the single server, but it&rsquo;s has poor fault-tolerance.</p>
<p>So we can think the replication for the fault-tolerance, but it will make strong consistency tricky.</p>
<ul>
<li>Replication servers:S1 S2</li>
</ul>
<p>C1: write S1 or S2</p>
<p>C2: write S1 or S2</p>
<p>C3: read S1
C4: read S2</p>
<p>C3 or C4 will see the different results. It can&rsquo;t promise the same result or special result, because there is no standard protocol.</p>
<p>So that&rsquo;s not strong consistency. For consistency, we need to use the special protocol to fix these problems.</p>
<p>The gfs is one case about the distributed system&rsquo;s protocol.</p>
<h2 id="gfs-1">Gfs</h2>
<h3 id="context">Context</h3>
<p>Many Google services needed a big fast unified storage system, like Mapreduce, crawler/indexer, log storage/analysis, Youtube&hellip;</p>
<ul>
<li>
<p>Big: large data set</p>
</li>
<li>
<p>Fast: automatic sharding</p>
<ul>
<li>For parallel performance</li>
<li>To increase space available</li>
</ul>
</li>
<li>
<p>Global(over a single data center)</p>
<ul>
<li>any client can read any file</li>
<li>Allows sharing of data among applications</li>
</ul>
</li>
<li>
<p>Fault-tolerance</p>
<ul>
<li>Automatic recovery from failures</li>
<li>Just one data center per deployment</li>
<li>Just Google applications/users</li>
</ul>
</li>
<li>
<p>Aimed at sequential access to huge files; read or append</p>
<ul>
<li>not a low-latency DB for small items</li>
</ul>
</li>
</ul>
<blockquote>
<p>What was new about this in 2003? How did they get an SOSP paper accepted?</p>
<ul>
<li>Not the basic ideas of distribution, sharding, fault-tolerance.</li>
<li>Huge scale.</li>
<li>Used in industry, real-world experience.</li>
<li>Successful use of weak consistency.</li>
<li>Successful use of single master.</li>
</ul>
</blockquote>
<h3 id="design">Design</h3>
<ul>
<li>clients (library, RPC &ndash; but not visible as a UNIX FS)</li>
<li>each file split into independent 64 MB chunks</li>
<li>chunk servers, each chunk replicated on 3</li>
<li>every file&rsquo;s chunks are spread over the chunk servers
<ul>
<li>for parallel read/write (e.g. MapReduce), and to allow huge files</li>
</ul>
</li>
<li>single master, and master replicas</li>
<li>division of work: master deals w/ naming, chunkservers w/ data</li>
</ul>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://img.zhengyua.cn/20210515112556.png"
        data-srcset="https://img.zhengyua.cn/20210515112556.png, https://img.zhengyua.cn/20210515112556.png 1.5x, https://img.zhengyua.cn/20210515112556.png 2x"
        data-sizes="auto"
        alt="https://img.zhengyua.cn/20210515112556.png"
        title="image-20210515112556187" /></p>
<h3 id="master-state">Master state</h3>
<ul>
<li>in RAM (for speed, must be smallish)</li>
</ul>
<p>file name / index -&gt; array of chunk handles</p>
<p>Chunk handle(stable)</p>
<p>-&gt; version(stable)</p>
<p>-&gt; list of chunk servers(volatile)</p>
<p>-&gt; primary, secondaries(volatile)</p>
<p>-&gt; lease time(volatile)</p>
<ul>
<li>on disk: stable storage</li>
</ul>
<p>log -&gt;</p>
<p>master will first write the log.</p>
<p>when master fails or crashes, the log can reconsturct or fix the master for the backup.</p>
<p>Checkpoint -&gt;</p>
<p>when master fails or crashes, it can replay the last part or state basically then can quickly reconsturct master from the log.</p>
<blockquote>
<p>what state does need to end up in stable storage for the mass detection function coreectly?</p>
</blockquote>
<h3 id="steps-when-read-a-file">steps when read a file</h3>
<ol>
<li>
<p>C sends filename and offset to master M (if not cached)</p>
</li>
<li>
<p>M finds chunk handle for that offset</p>
</li>
<li>
<p>M replies with list of chunkservers
only those with latest version</p>
</li>
<li>
<p>C caches handle + chunkserver list</p>
<p>-&gt; caches is to reduce the load on this single machine</p>
</li>
<li>
<p>C sends request to nearest chunkserver
chunk handle, offset</p>
<p>-&gt; nearest chunkserver is to minimize network traffific or maximize the throughput about the joint setup form the clients to the servers</p>
</li>
<li>
<p>chunk server check the reversion then reads from chunk file on disk, returns</p>
<p>-&gt; check the version is to avoid reading the stale data</p>
</li>
</ol>
<h3 id="steps-when-write-a-file-append">steps when write a file: append</h3>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://img.zhengyua.cn/20210515115327.png"
        data-srcset="https://img.zhengyua.cn/20210515115327.png, https://img.zhengyua.cn/20210515115327.png 1.5x, https://img.zhengyua.cn/20210515115327.png 2x"
        data-sizes="auto"
        alt="https://img.zhengyua.cn/20210515115327.png"
        title="image-20210515115327279" /></p>
<ol>
<li>C asks M about file&rsquo;s last chunk</li>
<li>if M sees chunk has no primary (or lease expired):
2a. if no chunkservers w/ latest version <strong># error</strong>
2b. pick primary P and secondaries from those w/ latest version #
2c. increment version #, write to log on disk
2d. tell P and secondaries who they are, and new version #
2e. replicas write new version <strong># to disk</strong>
3. M tells C <strong>the primary and secondaries + version</strong>
4. C sends data to all (just temporary&hellip;), waits
5. C tells P to append
6. P checks that <strong>lease hasn&rsquo;t expired</strong>, and chunk has space
7. P picks an offset (at end of chunk)
8. P writes chunk file (a Linux file)
9. P tells each secondary the offset, tells to append to chunk file
10. P waits for <strong>all secondaries to reply</strong>, or timeout
secondary can reply &ldquo;error&rdquo; e.g. out of disk space
11. P tells C &ldquo;ok&rdquo; or &ldquo;error&rdquo;
12. C retries from start if error</li>
</ol>
<blockquote>
<p>What consistency guarantees does GFS provide to clients?</p>
<p>Needs to be in a form that tells applications how to use GFS.</p>
<p>Here&rsquo;s a possibility:</p>
<ul>
<li>If the primary tells a client that a record append succeeded, then any reader that subsequently opens the file and scans it will see the appended record somewhere.</li>
</ul>
</blockquote>
<h3 id="question">question</h3>
<ul>
<li>
<p>What if an appending client fails at an awkward moment?
Is there an awkward moment?</p>
</li>
<li>
<p>What if the appending client has cached a stale (wrong) primary?</p>
</li>
<li>
<p>What if the reading client has cached a stale secondary list?</p>
</li>
<li>
<p>Could a master crash+reboot cause it to forget about the file?
Or forget what chunkservers hold the relevant chunk?</p>
</li>
<li>
<p>Two clients do record append at exactly the same time.
Will they overwrite each others&rsquo; records?</p>
</li>
<li>
<p>Suppose one secondary never hears the append command from the primary.
What if reading client reads from that secondary?</p>
</li>
<li>
<p>What if the primary crashes before sending append to all secondaries?
Could a secondary that <em>didn&rsquo;t</em> see the append be chosen as the new primary?</p>
</li>
<li>
<p>Chunkserver S4 with an old stale copy of chunk is offline.
Primary and all live secondaries crash.
S4 comes back to life (before primary and secondaries).
Will master choose S4 (with stale chunk) as primary?
Better to have primary with stale data, or no replicas at all?</p>
</li>
<li>
<p>What should a primary do if a secondary always fails writes?
e.g. dead, or out of disk space, or disk has broken.
Should the primary drop secondary from set of secondaries?
And then return success to client appends?
Or should the primary keep sending ops, and having them fail,
and thus fail every client write request?</p>
</li>
<li>
<p>What if primary S1 is alive and serving client requests,
but network between master and S1 fails?
&ldquo;network partition&rdquo;
Will the master pick a new primary?
Will there now be two primaries?
So that the append goes to one primary, and the read to the other?
Thus breaking the consistency guarantee?
&ldquo;split brain&rdquo;</p>
</li>
<li>
<p>If there&rsquo;s a partitioned primary serving client appends, and its
lease expires, and the master picks a new primary, will the new
primary have the latest data as updated by partitioned primary?</p>
</li>
<li>
<p>What if the master fails altogether.
Will the replacement know everything the dead master knew?
E.g. each chunk&rsquo;s version number? primary? lease expiry time?</p>
</li>
<li>
<p>Who/what decides the master is dead, and must be replaced?
Could the master replicas ping the master, take over if no response?</p>
</li>
<li>
<p>What happens if the entire building suffers a power failure?
And then power is restored, and all servers reboot.</p>
</li>
<li>
<p>Suppose the master wants to create a new chunk replica.
Maybe because too few replicas.
Suppose it&rsquo;s the last chunk in the file, and being appended to.
How does the new replica ensure it doesn&rsquo;t miss any appends?
After all it is not yet one of the secondaries.</p>
</li>
<li>
<p>Is there <em>any</em> circumstance in which GFS will break the guarantee?
i.e. append succeeds, but subsequent readers don&rsquo;t see the record.
All master replicas permanently lose state (permanent disk failure).
Could be worse: result will be &ldquo;no answer&rdquo;, not &ldquo;incorrect data&rdquo;.
&ldquo;fail-stop&rdquo;
All chunkservers holding the chunk permanently lose disk content.
again, fail-stop; not the worse possible outcome
CPU, RAM, network, or disk yields an incorrect value.
checksum catches some cases, but not all
Time is not properly synchronized, so leases don&rsquo;t work out.
So multiple primaries, maybe write goes to one, read to the other.</p>
</li>
</ul>
<p>What application-visible anomalous behavior does GFS allow?
Will all clients see the same file content?
Could one client see a record that another client doesn&rsquo;t see at all?
Will a client see the same content if it reads a file twice?
Will all clients see successfully appended records in the same order?</p>
<p>Will these anomalies cause trouble for applications?
How about MapReduce?</p>
<h3 id="strict-consistency">strict consistency</h3>
<p>I.e. all clients see the same file content.
Too hard to give a real answer, but here are some issues.</p>
<ul>
<li>Primary should detect duplicate client write requests.
Or client should not issue them.</li>
<li>All secondaries should complete each write, or none.
Perhaps tentative writes until all promise to complete it?
Don&rsquo;t expose writes until all have agreed to perform them!</li>
<li>If primary crashes, some replicas may be missing the last few ops.
New primary must talk to all replicas to find all recent ops,
and sync with secondaries.</li>
<li>To avoid client reading from stale ex-secondary, either all client
reads must go to primary, or secondaries must also have leases.</li>
</ul>
<blockquote>
<p>Performance (Figure 3)
large aggregate throughput for read (3 copies, striping)
94 MB/sec total for 16 chunkservers
or 6 MB/second per chunkserver
is that good?
one disk sequential throughput was about 30 MB/s
one NIC was about 10 MB/s
Close to saturating network (inter-switch link)
So: individual server performance is low
but scalability is good
which is more important?
Table 3 reports 500 MB/sec for production GFS, which is a lot
writes to different files lower than possible maximum
authors blame their network stack (but no detail)
concurrent appends to single file
limited by the server that stores last chunk
hard to interpret after 15 years, e.g. how fast were the disks?</p>
<p>Random issues worth considering
What would it take to support small files well?
What would it take to support billions of files?
Could GFS be used as wide-area file system?
With replicas in different cities?
All replicas in one datacenter is not very fault tolerant!
How long does GFS take to recover from a failure?
Of a primary/secondary?
Of the master?
How well does GFS cope with slow chunkservers?</p>
<p>Retrospective interview with GFS engineer:
<a href="http://queue.acm.org/detail.cfm?id=1594206" target="_blank" rel="noopener noreffer">http://queue.acm.org/detail.cfm?id=1594206</a>
file count was the biggest problem
eventual numbers grew to 1000x those in Table 2 !
hard to fit in master RAM
master scanning of all files/chunks for GC is slow
1000s of clients too much CPU load on master
applications had to be designed to cope with GFS semantics
and limitations
master fail-over initially entirely manual, 10s of minutes
BigTable is one answer to many-small-files problem
and Colossus apparently shards master data over many masters</p>
</blockquote>
<h2 id="summary">Summary</h2>
<ul>
<li>
<p>case study of performance, fault-tolerance, consistency</p>
<p>specialized for MapReduce applications</p>
</li>
<li>
<p>good ideas:</p>
<ul>
<li>global cluster file system as universal infrastructure</li>
<li>separation of naming (master) from storage (chunkserver)</li>
<li>sharding for parallel throughput</li>
<li>huge files/chunks to reduce overheads</li>
<li>primary to sequence writes</li>
<li>leases to prevent split-brain chunkserver primaries</li>
</ul>
</li>
<li>
<p>not so great:</p>
<ul>
<li>single master performance:ran out of RAM and CPU</li>
<li>chunkservers not very efficient for small files</li>
<li>lack of automatic fail-over to master replica</li>
<li>maybe consistency was too relaxed</li>
</ul>
</li>
</ul>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>更新于 2022-01-29</span>
            </div>
            <div class="post-info-license"></div>
        </div>
        <div class="post-info-line">
            <div class="post-info-md"><span>
                            <a class="link-to-markdown" href="/posts/distributed_structure/structure/5.html/index.md" target="_blank">阅读原始文档</a>
                        </span></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="分享到 Twitter" data-sharer="twitter" data-url="https://zhengyua.cn/posts/distributed_structure/structure/5.html/" data-title="MIT6824LEC3笔记:GFS" data-hashtags="分布式架构,MIT6824,note"><i class="fab fa-twitter fa-fw"></i></a><a href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="https://zhengyua.cn/posts/distributed_structure/structure/5.html/" data-hashtag="分布式架构"><i class="fab fa-facebook-square fa-fw"></i></a><a href="javascript:void(0);" title="分享到 WhatsApp" data-sharer="whatsapp" data-url="https://zhengyua.cn/posts/distributed_structure/structure/5.html/" data-title="MIT6824LEC3笔记:GFS" data-web><i class="fab fa-whatsapp fa-fw"></i></a><a href="javascript:void(0);" title="分享到 Line" data-sharer="line" data-url="https://zhengyua.cn/posts/distributed_structure/structure/5.html/" data-title="MIT6824LEC3笔记:GFS"><i data-svg-src="/lib/simple-icons/icons/line.min.svg"></i></a><a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="https://zhengyua.cn/posts/distributed_structure/structure/5.html/" data-title="MIT6824LEC3笔记:GFS"><i class="fab fa-weibo fa-fw"></i></a><a href="javascript:void(0);" title="分享到 Myspace" data-sharer="myspace" data-url="https://zhengyua.cn/posts/distributed_structure/structure/5.html/" data-title="MIT6824LEC3笔记:GFS" data-description=""><i data-svg-src="/lib/simple-icons/icons/myspace.min.svg"></i></a><a href="javascript:void(0);" title="分享到 Blogger" data-sharer="blogger" data-url="https://zhengyua.cn/posts/distributed_structure/structure/5.html/" data-title="MIT6824LEC3笔记:GFS" data-description=""><i class="fab fa-blogger fa-fw"></i></a><a href="javascript:void(0);" title="分享到 Evernote" data-sharer="evernote" data-url="https://zhengyua.cn/posts/distributed_structure/structure/5.html/" data-title="MIT6824LEC3笔记:GFS"><i class="fab fa-evernote fa-fw"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw"></i>&nbsp;<a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84/">分布式架构</a>,&nbsp;<a href="/tags/mit6824/">MIT6824</a>,&nbsp;<a href="/tags/note/">note</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/">主页</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/posts/distributed_structure/structure/1.html/" class="prev" rel="prev" title="MIT6824LEC4笔记:Primary-Backup Replication"><i class="fas fa-angle-left fa-fw"></i>MIT6824LEC4笔记:Primary-Backup Replication</a>
            <a href="/posts/distributed_structure/structure/3.html/" class="next" rel="next" title="MIT6824LEC1笔记:Map Reduce">MIT6824LEC1笔记:Map Reduce<i class="fas fa-angle-right fa-fw"></i></a></div>
</div>
</article></div>
            </main><footer class="footer">
        <div class="footer-container">
            <div class="footer-line">
                <span id="run-time"></span>
            </div>
            <span id="busuanzi_container_site_pv">
                本站访问量：<span id="busuanzi_value_site_pv"></span>次
            </span><div class="footer-line"><i class="far fa-copyright fa-fw"></i><span itemprop="copyrightYear">2022 - 2024</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="https://github.com/catwithtudou" target="_blank">一直是阵雨🌦️</a></span><span class="icp-splitter">&nbsp;|&nbsp;</span><br class="icp-br"/>
                    <span class="icp">渝ICP备19012006号-1</span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="回到顶部">
                <i class="fas fa-arrow-up fa-fw"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="查看评论">
                <i class="fas fa-comment fa-fw"></i>
            </a>
        </div><link rel="stylesheet" href="/lib/lightgallery/lightgallery.min.css"><link rel="stylesheet" href="/lib/katex/katex.min.css"><link rel="stylesheet" href="/lib/katex/copy-tex.min.css"><link rel="stylesheet" href="/lib/cookieconsent/cookieconsent.min.css"><script type="text/javascript" src="/lib/smooth-scroll/smooth-scroll.min.js"></script><script type="text/javascript" src="/lib/autocomplete/autocomplete.min.js"></script><script type="text/javascript" src="/lib/lunr/lunr.min.js"></script><script type="text/javascript" src="/lib/lunr/lunr.stemmer.support.min.js"></script><script type="text/javascript" src="/lib/lunr/lunr.zh.min.js"></script><script type="text/javascript" src="/lib/lazysizes/lazysizes.min.js"></script><script type="text/javascript" src="/lib/twemoji/twemoji.min.js"></script><script type="text/javascript" src="/lib/lightgallery/lightgallery.min.js"></script><script type="text/javascript" src="/lib/lightgallery/lg-thumbnail.min.js"></script><script type="text/javascript" src="/lib/lightgallery/lg-zoom.min.js"></script><script type="text/javascript" src="/lib/clipboard/clipboard.min.js"></script><script type="text/javascript" src="/lib/sharer/sharer.min.js"></script><script type="text/javascript" src="/lib/typeit/typeit.min.js"></script><script type="text/javascript" src="/lib/katex/katex.min.js"></script><script type="text/javascript" src="/lib/katex/auto-render.min.js"></script><script type="text/javascript" src="/lib/katex/copy-tex.min.js"></script><script type="text/javascript" src="/lib/katex/mhchem.min.js"></script><script type="text/javascript" src="/lib/cookieconsent/cookieconsent.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"复制到剪贴板","maxShownLines":10},"comment":{},"cookieconsent":{"content":{"dismiss":"同意","link":"了解更多","message":"本网站使用 Cookies 来改善您的浏览体验."},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"data":{"id-1":"一直是阵雨🌦️'s blog","id-2":"一直是阵雨🌦️'s blog"},"lightGallery":{"actualSize":false,"exThumbImage":"data-thumbnail","hideBarsDelay":2000,"selector":".lightgallery","speed":400,"thumbContHeight":80,"thumbWidth":80,"thumbnail":true},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"$$","right":"$$"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"},{"display":false,"left":"$","right":"$"}],"strict":false},"search":{"highlightTag":"em","lunrIndexURL":"/index.json","lunrLanguageCode":"zh","lunrSegmentitURL":"/lib/lunr/lunr.segmentit.js","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"lunr"},"twemoji":true,"typeit":{"cursorChar":"|","cursorSpeed":1000,"data":{"id-1":["id-1"],"id-2":["id-2"]},"duration":-1,"speed":100}};</script><script type="text/javascript" src="/js/theme.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/jquery@2.1.3/dist/jquery.min.js"></script><script type="text/javascript" src="/js/custom.js"></script><script type="text/javascript">
            window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());
            gtag('config', 'G-J45YPF1TLY', { 'anonymize_ip': true });
        </script><script type="text/javascript" src="https://www.googletagmanager.com/gtag/js?id=G-J45YPF1TLY" async></script><script>
    var _hmt = _hmt || [];
    (function() {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?a129553334cecc8d46914f193ead738b";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
    })();
</script>

<script charset="UTF-8" id="LA_COLLECT" src="//sdk.51.la/js-sdk-pro.min.js"></script>
<script>LA.init({id: "Ja18LNgLATVSfW2h",ck: "Ja18LNgLATVSfW2h"})</script></body>
</html>
